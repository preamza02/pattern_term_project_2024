{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import  random\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pwd = Path(os.getcwd())\n",
    "sys.path.append(str(pwd.parent.parent / \"gym-checkers-for-thai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkers.agents.baselines import play_a_game, RandomPlayer\n",
    "from checkers.game import Checkers\n",
    "from checkers.agents import Player\n",
    "from checkers.agents.alpha_beta import MinimaxPlayer, first_order_adv\n",
    "\n",
    "from player import DeepLearningPlayer\n",
    "# from model.small_model import GDQL as GDQL\n",
    "from model.medium_model import GDQL_m as GDQL\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMAX_SEARCH_DEPTH = 2\n",
    "WEIGHT_FOLDER = pwd / \"weights\" / f\"vs_depth_{MINIMAX_SEARCH_DEPTH}\"\n",
    "\n",
    "N_EPISODES = 100\n",
    "N_MATCHES_PER_EPS = 50\n",
    "\n",
    "REWARD_DISCOUNT_FACTOR = 0.95\n",
    "\n",
    "EPSILON = 0.9\n",
    "EPSILON_DECAY_FACTOR = 0.999\n",
    "EPSILON_MIN = 0.33\n",
    "\n",
    "WIN_REWARD = 100\n",
    "LOSE_REWARD = -40\n",
    "DRAW_REWARD = -20\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "TARGET_UPDATE = 3 # update target network every TARGET_UPDATE episodes\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/24 11:13:53 INFO mlflow.tracking.fluent: Experiment with name 'DQL with gredient descent (medium model)' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"DQL with gredient descent (medium model)\")\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"MINIMAX_SEARCH_DEPTH\", MINIMAX_SEARCH_DEPTH)\n",
    "mlflow.log_param(\"N_EPISODES\", N_EPISODES)\n",
    "mlflow.log_param(\"N_MATCHES_PER_EPS\", N_MATCHES_PER_EPS)\n",
    "mlflow.log_param(\"REWARD_DISCOUNT_FACTOR\", REWARD_DISCOUNT_FACTOR)\n",
    "mlflow.log_param(\"EPSILON\", EPSILON)\n",
    "mlflow.log_param(\"EPSILON_DECAY_FACTOR\", EPSILON_DECAY_FACTOR)\n",
    "mlflow.log_param(\"EPSILON_MIN\", EPSILON_MIN)\n",
    "mlflow.log_param(\"BATCH_SIZE\", BATCH_SIZE)\n",
    "mlflow.log_param(\"TARGET_UPDATE\", TARGET_UPDATE)\n",
    "mlflow.log_param(\"LEARNING_RATE\", LEARNING_RATE)\n",
    "mlflow.log_param(\"WIN_REWARD\", WIN_REWARD)\n",
    "mlflow.log_param(\"LOSE_REWARD\", LOSE_REWARD)\n",
    "mlflow.log_param(\"DRAW_REWARD\", DRAW_REWARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder if it doesn't exist\n",
    "WEIGHT_FOLDER.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are corrupted, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1:  30%|███       | 15/50 [00:32<01:15,  2.14s/matches, loss=182, win_rate=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m available_action \u001b[38;5;129;01min\u001b[39;00m available_actions:\n\u001b[0;32m     65\u001b[0m     model_input \u001b[38;5;241m=\u001b[39m target_model\u001b[38;5;241m.\u001b[39mboard2input(next_state[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m, available_action)\n\u001b[1;32m---> 66\u001b[0m     next_state_value \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     max_next_state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_next_state_value, next_state_value)\n\u001b[0;32m     68\u001b[0m target_q \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m max_next_state_value \u001b[38;5;241m*\u001b[39m REWARD_DISCOUNT_FACTOR\n",
      "File \u001b[1;32mc:\\Users\\jiray\\.conda\\envs\\pattern_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jiray\\.conda\\envs\\pattern_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jiray\\Desktop\\pattern_term_project_2024\\src\\dql_with_gradient\\model\\medium_model.py:36\u001b[0m, in \u001b[0;36mGDQL_m.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m---> 36\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(x))\n\u001b[0;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc5(x)\n",
      "File \u001b[1;32mc:\\Users\\jiray\\.conda\\envs\\pattern_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jiray\\.conda\\envs\\pattern_project\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jiray\\.conda\\envs\\pattern_project\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "online_model = GDQL(lr=LEARNING_RATE)\n",
    "target_model = GDQL(lr=LEARNING_RATE)\n",
    "try:\n",
    "    online_model.load_state_dict(torch.load(WEIGHT_FOLDER / \"online_model.pth\"))\n",
    "    target_model.load_state_dict(torch.load(WEIGHT_FOLDER / \"target_model.pth\"))\n",
    "except FileNotFoundError:\n",
    "    print(\"No weights found, starting from scratch\")\n",
    "except RuntimeError:\n",
    "    print(\"Weights are corrupted, starting from scratch\")\n",
    "\n",
    "max_win_rate = 0\n",
    "\n",
    "for episode in range(N_EPISODES):\n",
    "    stime = time.time()\n",
    "    n_wins, n_losses, n_draws = 0, 0, 0\n",
    "    mean_loss = 0\n",
    "    DeepLearningPlayer.experience.clear()\n",
    "\n",
    "\n",
    "    looper = tqdm(range(N_MATCHES_PER_EPS), unit=\"matches\", leave=True, desc=f\"Episode {episode+1}\")\n",
    "    for i in looper:\n",
    "        ch = Checkers()\n",
    "\n",
    "        black_player = DeepLearningPlayer('black',\n",
    "                                model=online_model,\n",
    "                                epsilon=EPSILON,\n",
    "                                epsilon_decay=EPSILON_DECAY_FACTOR,\n",
    "                                epsilon_min=EPSILON_MIN,\n",
    "                                win_reward=WIN_REWARD,\n",
    "                                lose_reward=LOSE_REWARD,\n",
    "                                draw_reward=DRAW_REWARD,)\n",
    "\n",
    "        if MINIMAX_SEARCH_DEPTH == 0:\n",
    "            # Random player function\n",
    "            white_player = RandomPlayer('white', seed=i)\n",
    "        else:\n",
    "            # Minimax player function\n",
    "            white_player = MinimaxPlayer('white', \n",
    "                                        partial(first_order_adv, 'white', 86, 54.5, 87, 26),\n",
    "                                        search_depth=MINIMAX_SEARCH_DEPTH)\n",
    "        \n",
    "        # push into environment\n",
    "        winner = play_a_game(ch, black_player.next_move, white_player.next_move, 100, is_show_detail=False)\n",
    "        if winner == 'black':\n",
    "            n_wins += 1\n",
    "            black_player.set_win()\n",
    "        elif winner == 'white':\n",
    "            n_losses += 1\n",
    "            black_player.set_lose()\n",
    "        else:\n",
    "            n_draws += 1\n",
    "\n",
    "        if len(DeepLearningPlayer.experience) > BATCH_SIZE:\n",
    "            batch_states = random.sample(DeepLearningPlayer.experience, BATCH_SIZE)\n",
    "            \n",
    "            # find target, online Q values and compute loss\n",
    "            loss = 0\n",
    "            for batch_idx, (state, action, reward, next_state) in enumerate(batch_states):\n",
    "                online_model.train()\n",
    "                target_model.eval()\n",
    "\n",
    "                # find target Q\n",
    "                if next_state is not None:\n",
    "                    max_next_state_value = -np.inf\n",
    "                    ch.restore_state(state)\n",
    "                    available_actions = ch.legal_moves()\n",
    "                    for available_action in available_actions:\n",
    "                        model_input = target_model.board2input(next_state[0], 'black', available_action)\n",
    "                        next_state_value = target_model(model_input)\n",
    "                        max_next_state_value = max(max_next_state_value, next_state_value)\n",
    "                    target_q = reward + max_next_state_value * REWARD_DISCOUNT_FACTOR\n",
    "                else:\n",
    "                    target_q = reward\n",
    "\n",
    "                # find online Q\n",
    "                model_input = online_model.board2input(state[0], 'black', action)\n",
    "                online_q = online_model(model_input)\n",
    "\n",
    "                loss += (online_q - target_q) ** 2\n",
    "            loss /= BATCH_SIZE\n",
    "            mean_loss += loss.item()\n",
    "            looper.set_postfix(loss=loss.item(),\n",
    "                               win_rate=n_wins / (i+1),)\n",
    "\n",
    "            # compute loss\n",
    "            online_model.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            online_model.optimizer.step()\n",
    "\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_model.load_state_dict(online_model.state_dict())\n",
    "        print(\"\\tTarget model updated\")\n",
    "    print(f\"\\tWins: {n_wins}, Losses: {n_losses}, Draws: {n_draws}\")\n",
    "\n",
    "    mlflow.log_metric(\"runl time\", time.time() - stime, step=episode)\n",
    "    mlflow.log_metric(\"win rate\", n_wins / N_MATCHES_PER_EPS, step=episode)\n",
    "    mlflow.log_metric(\"draw rate\", n_draws / N_MATCHES_PER_EPS, step=episode)\n",
    "    mlflow.log_metric(\"mean of mse loss\", mean_loss / N_MATCHES_PER_EPS, step=episode)\n",
    "    if n_wins / N_MATCHES_PER_EPS > max_win_rate:\n",
    "        max_win_rate = n_wins / N_MATCHES_PER_EPS\n",
    "        torch.save(online_model.state_dict(), WEIGHT_FOLDER / \"online_model.pth\")\n",
    "        torch.save(target_model.state_dict(), WEIGHT_FOLDER / \"target_model.pth\")\n",
    "        print(f\"\\tNew max win rate: {max_win_rate}\")\n",
    "        mlflow.pytorch.log_model(online_model, \"models\")\n",
    "        mlflow.log_artifact(WEIGHT_FOLDER / \"online_model.pth\")\n",
    "        mlflow.log_artifact(WEIGHT_FOLDER / \"target_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_b_b_b_b\n",
      "b_b_b_b_\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_w_w\n",
      "w_w_w_w_\n",
      "0 turn: black last_moved_piece: None\n",
      "7 legal moves [(4, 8), (5, 8), (5, 9), (6, 9), (6, 10), (7, 10), (7, 11)]\n",
      "black moved 6, 10\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._b_\n",
      "_._._b_.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_w_w\n",
      "w_w_w_w_\n",
      "1 turn: white last_moved_piece: None\n",
      "7 legal moves [(24, 21), (24, 20), (25, 22), (25, 21), (26, 23), (26, 22), (27, 23)]\n",
      "white moved 25, 22\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._b_\n",
      "_._._b_.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_w_._w_w\n",
      "w_w_w_w_\n",
      "2 turn: black last_moved_piece: None\n",
      "8 legal moves [(1, 6), (2, 6), (4, 8), (5, 8), (5, 9), (7, 11), (10, 14), (10, 15)]\n",
      "black moved 10, 15\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "._._._b_\n",
      "_._._._.\n",
      "._._w_._\n",
      "_w_._w_w\n",
      "w_w_w_w_\n",
      "3 turn: white last_moved_piece: None\n",
      "8 legal moves [(22, 18), (22, 17), (24, 21), (24, 20), (26, 23), (27, 23), (29, 25), (30, 25)]\n",
      "white moved 27, 23\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "._._._b_\n",
      "_._._._.\n",
      "._._w_w_\n",
      "_w_._w_.\n",
      "w_w_w_w_\n",
      "4 turn: black last_moved_piece: None\n",
      "9 legal moves [(1, 6), (2, 6), (4, 8), (5, 8), (5, 9), (7, 10), (7, 11), (15, 18), (15, 19)]\n",
      "black moved 7, 11\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._._\n",
      "_._._._b\n",
      "._._._b_\n",
      "_._._._.\n",
      "._._w_w_\n",
      "_w_._w_.\n",
      "w_w_w_w_\n",
      "5 turn: white last_moved_piece: None\n",
      "9 legal moves [(22, 18), (22, 17), (23, 19), (23, 18), (24, 21), (24, 20), (29, 25), (30, 25), (31, 27)]\n",
      "white moved 31, 27\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._._\n",
      "_._._._b\n",
      "._._._b_\n",
      "_._._._.\n",
      "._._w_w_\n",
      "_w_._w_w\n",
      "w_w_w_._\n",
      "6 turn: black last_moved_piece: None\n",
      "9 legal moves [(1, 6), (2, 6), (2, 7), (3, 7), (4, 8), (5, 8), (5, 9), (15, 18), (15, 19)]\n",
      "black moved 1, 6\n",
      "\n",
      "_b_._b_b\n",
      "b_b_b_._\n",
      "_._._._b\n",
      "._._._b_\n",
      "_._._._.\n",
      "._._w_w_\n",
      "_w_._w_w\n",
      "w_w_w_._\n",
      "7 turn: white last_moved_piece: None\n",
      "8 legal moves [(22, 18), (22, 17), (23, 19), (23, 18), (24, 21), (24, 20), (29, 25), (30, 25)]\n",
      "white moved 24, 20\n",
      "\n",
      "_b_._b_b\n",
      "b_b_b_._\n",
      "_._._._b\n",
      "._._._b_\n",
      "_._._._.\n",
      "w_._w_w_\n",
      "_._._w_w\n",
      "w_w_w_._\n",
      "8 turn: black last_moved_piece: None\n",
      "9 legal moves [(2, 7), (3, 7), (4, 8), (5, 8), (5, 9), (6, 9), (6, 10), (15, 18), (15, 19)]\n",
      "black moved 15, 18\n",
      "\n",
      "_b_._b_b\n",
      "b_b_b_._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._b_.\n",
      "w_._w_w_\n",
      "_._._w_w\n",
      "w_w_w_._\n",
      "9 turn: white last_moved_piece: None\n",
      "2 legal moves [(22, 15), (23, 14)]\n",
      "white moved 22, 15\n",
      "\n",
      "_b_._b_b\n",
      "b_b_b_._\n",
      "_._._._b\n",
      "._._._w_\n",
      "_._._._.\n",
      "w_._._w_\n",
      "_._._w_w\n",
      "w_w_w_._\n",
      "10 turn: black last_moved_piece: None\n",
      "1 legal moves [(11, 18)]\n",
      "black moved 11, 18\n",
      "\n",
      "_b_._b_b\n",
      "b_b_b_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._b_.\n",
      "w_._._w_\n",
      "_._._w_w\n",
      "w_w_w_._\n",
      "11 turn: white last_moved_piece: None\n",
      "1 legal moves [(23, 14)]\n",
      "white moved 23, 14\n",
      "\n",
      "_b_._b_b\n",
      "b_b_b_._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_._._._.\n",
      "w_._._._\n",
      "_._._w_w\n",
      "w_w_w_._\n",
      "12 turn: black last_moved_piece: None\n",
      "7 legal moves [(2, 7), (3, 7), (4, 8), (5, 8), (5, 9), (6, 9), (6, 10)]\n",
      "black moved 6, 9\n",
      "\n",
      "_b_._b_b\n",
      "b_b_._._\n",
      "_._b_._.\n",
      "._._w_._\n",
      "_._._._.\n",
      "w_._._._\n",
      "_._._w_w\n",
      "w_w_w_._\n",
      "13 turn: white last_moved_piece: None\n",
      "9 legal moves [(27, 23), (14, 10), (20, 16), (26, 23), (26, 22), (28, 24), (29, 25), (29, 24), (30, 25)]\n",
      "white moved 27, 23\n",
      "\n",
      "_b_._b_b\n",
      "b_b_._._\n",
      "_._b_._.\n",
      "._._w_._\n",
      "_._._._.\n",
      "w_._._w_\n",
      "_._._w_.\n",
      "w_w_w_._\n",
      "14 turn: black last_moved_piece: None\n",
      "1 legal moves [(9, 18)]\n",
      "black moved 9, 18\n",
      "\n",
      "_b_._b_b\n",
      "b_b_._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._b_.\n",
      "w_._._w_\n",
      "_._._w_.\n",
      "w_w_w_._\n",
      "15 turn: black last_moved_piece: 18\n",
      "1 legal moves [(18, 27)]\n",
      "black moved 18, 27\n",
      "\n",
      "_b_._b_b\n",
      "b_b_._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._._._\n",
      "_._._w_b\n",
      "w_w_w_._\n",
      "16 turn: white last_moved_piece: None\n",
      "7 legal moves [(20, 16), (26, 23), (26, 22), (28, 24), (29, 25), (29, 24), (30, 25)]\n",
      "white moved 26, 22\n",
      "\n",
      "_b_._b_b\n",
      "b_b_._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_._._._b\n",
      "w_w_w_._\n",
      "17 turn: black last_moved_piece: None\n",
      "7 legal moves [(2, 6), (2, 7), (3, 7), (4, 8), (5, 8), (5, 9), (27, 31)]\n",
      "black moved 4, 8\n",
      "\n",
      "_b_._b_b\n",
      "._b_._._\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_._._._b\n",
      "w_w_w_._\n",
      "18 turn: white last_moved_piece: None\n",
      "8 legal moves [(22, 18), (22, 17), (20, 16), (28, 24), (29, 25), (29, 24), (30, 26), (30, 25)]\n",
      "white moved 29, 25\n",
      "\n",
      "_b_._b_b\n",
      "._b_._._\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_._w_._b\n",
      "w_._w_._\n",
      "19 turn: black last_moved_piece: None\n",
      "8 legal moves [(0, 4), (2, 6), (2, 7), (3, 7), (5, 9), (8, 12), (8, 13), (27, 31)]\n",
      "black moved 27, 31\n",
      "\n",
      "_b_._b_b\n",
      "._b_._._\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_._w_._.\n",
      "w_._w_B_\n",
      "20 turn: white last_moved_piece: None\n",
      "6 legal moves [(25, 21), (22, 18), (22, 17), (20, 16), (28, 24), (30, 26)]\n",
      "white moved 30, 26\n",
      "\n",
      "_b_._b_b\n",
      "._b_._._\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_._w_w_.\n",
      "w_._._B_\n",
      "21 turn: black last_moved_piece: None\n",
      "8 legal moves [(0, 4), (2, 6), (2, 7), (3, 7), (5, 9), (8, 12), (8, 13), (31, 27)]\n",
      "black moved 2, 6\n",
      "\n",
      "_b_._._b\n",
      "._b_b_._\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_._w_w_.\n",
      "w_._._B_\n",
      "22 turn: white last_moved_piece: None\n",
      "6 legal moves [(26, 23), (25, 21), (22, 18), (22, 17), (20, 16), (28, 24)]\n",
      "white moved 28, 24\n",
      "\n",
      "_b_._._b\n",
      "._b_b_._\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_w_w_w_.\n",
      "._._._B_\n",
      "23 turn: black last_moved_piece: None\n",
      "8 legal moves [(0, 4), (3, 7), (5, 9), (6, 9), (6, 10), (8, 12), (8, 13), (31, 27)]\n",
      "black moved 6, 9\n",
      "\n",
      "_b_._._b\n",
      "._b_._._\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_w_w_w_.\n",
      "._._._B_\n",
      "24 turn: white last_moved_piece: None\n",
      "6 legal moves [(20, 16), (22, 18), (22, 17), (24, 21), (25, 21), (26, 23)]\n",
      "white moved 25, 21\n",
      "\n",
      "_b_._._b\n",
      "._b_._._\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_w_w_._\n",
      "_w_._w_.\n",
      "._._._B_\n",
      "25 turn: black last_moved_piece: None\n",
      "7 legal moves [(0, 4), (3, 7), (8, 12), (8, 13), (9, 13), (9, 14), (31, 27)]\n",
      "black moved 3, 7\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_w_w_._\n",
      "_w_._w_.\n",
      "._._._B_\n",
      "26 turn: white last_moved_piece: None\n",
      "6 legal moves [(20, 16), (21, 17), (21, 16), (22, 18), (22, 17), (26, 23)]\n",
      "white moved 21, 17\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._w_._.\n",
      "w_._w_._\n",
      "_w_._w_.\n",
      "._._._B_\n",
      "27 turn: black last_moved_piece: None\n",
      "8 legal moves [(0, 4), (7, 10), (7, 11), (8, 12), (8, 13), (9, 13), (9, 14), (31, 27)]\n",
      "black moved 8, 12\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_._w_._.\n",
      "w_._w_._\n",
      "_w_._w_.\n",
      "._._._B_\n",
      "28 turn: white last_moved_piece: None\n",
      "6 legal moves [(17, 14), (17, 13), (20, 16), (22, 18), (24, 21), (26, 23)]\n",
      "white moved 20, 16\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_w_w_._.\n",
      "._._w_._\n",
      "_w_._w_.\n",
      "._._._B_\n",
      "29 turn: black last_moved_piece: None\n",
      "1 legal moves [(12, 21)]\n",
      "black moved 12, 21\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_._.\n",
      "._._._._\n",
      "_._w_._.\n",
      "._b_w_._\n",
      "_w_._w_.\n",
      "._._._B_\n",
      "30 turn: black last_moved_piece: 21\n",
      "1 legal moves [(21, 28)]\n",
      "black moved 21, 28\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_._.\n",
      "._._._._\n",
      "_._w_._.\n",
      "._._w_._\n",
      "_._._w_.\n",
      "B_._._B_\n",
      "31 turn: white last_moved_piece: None\n",
      "4 legal moves [(17, 14), (17, 13), (22, 18), (26, 23)]\n",
      "white moved 17, 14\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_._.\n",
      "._._w_._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_._._w_.\n",
      "B_._._B_\n",
      "32 turn: black last_moved_piece: None\n",
      "2 legal moves [(9, 18), (28, 10)]\n",
      "black moved 28, 10\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_B_.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_._._w_.\n",
      "._._._B_\n",
      "33 turn: white last_moved_piece: None\n",
      "3 legal moves [(22, 18), (22, 17), (26, 23)]\n",
      "white moved 26, 23\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_B_.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._w_w_\n",
      "_._._._.\n",
      "._._._B_\n",
      "34 turn: black last_moved_piece: None\n",
      "1 legal moves [(31, 17)]\n",
      "black moved 31, 17\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_B_.\n",
      "._._._._\n",
      "_._B_._.\n",
      "._._._w_\n",
      "_._._._.\n",
      "._._._._\n",
      "35 turn: white last_moved_piece: None\n",
      "2 legal moves [(23, 19), (23, 18)]\n",
      "white moved 23, 19\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_B_.\n",
      "._._._._\n",
      "_._B_._w\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "36 turn: black last_moved_piece: None\n",
      "19 legal moves [(0, 4), (5, 8), (7, 11), (9, 13), (9, 14), (17, 21), (17, 24), (17, 28), (17, 22), (17, 26), (17, 31), (17, 14), (17, 13), (17, 8), (17, 4), (10, 14), (10, 15), (10, 6), (10, 1)]\n",
      "black moved 17, 31\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_B_.\n",
      "._._._._\n",
      "_._._._w\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._B_\n",
      "37 turn: white last_moved_piece: None\n",
      "1 legal moves [(19, 15)]\n",
      "white moved 19, 15\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_B_.\n",
      "._._._w_\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._B_\n",
      "38 turn: black last_moved_piece: None\n",
      "1 legal moves [(10, 19)]\n",
      "black moved 10, 19\n",
      "\n",
      "black player wins\n",
      "total legal moves 222 avg branching factor 5.6923076923076925\n"
     ]
    }
   ],
   "source": [
    "ch = Checkers()\n",
    "\n",
    "black_player = DeepLearningPlayer('black',\n",
    "                                model=online_model,\n",
    "                                epsilon=EPSILON,\n",
    "                                epsilon_decay=EPSILON_DECAY_FACTOR,\n",
    "                                epsilon_min=EPSILON_MIN,)\n",
    "# Random player function\n",
    "white_player = RandomPlayer('white', seed=i)\n",
    "        \n",
    "# push into environment\n",
    "winner = play_a_game(ch, black_player.next_move, white_player.next_move, 100, is_show_detail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
