{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import  random\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pwd = Path(os.getcwd())\n",
    "sys.path.append(str(pwd.parent.parent / \"gym-checkers-for-thai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkers.agents.baselines import play_a_game, RandomPlayer\n",
    "from checkers.game import Checkers\n",
    "from checkers.agents import Player\n",
    "from checkers.agents.alpha_beta import MinimaxPlayer, first_order_adv\n",
    "\n",
    "from player import DeepLearningPlayer\n",
    "# from model.small_model import GDQL as GDQL\n",
    "from model.medium_model import GDQL_m as GDQL\n",
    "\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIMAX_SEARCH_DEPTH = 2\n",
    "WEIGHT_FOLDER = pwd / \"weights\" / f\"vs_depth_{MINIMAX_SEARCH_DEPTH}\"\n",
    "\n",
    "N_EPISODES = 100\n",
    "N_MATCHES_PER_EPS = 50\n",
    "\n",
    "REWARD_DISCOUNT_FACTOR = 0.95\n",
    "\n",
    "EPSILON = 0.9\n",
    "EPSILON_DECAY_FACTOR = 0.999\n",
    "EPSILON_MIN = 0.33\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "TARGET_UPDATE = 3 # update target network every TARGET_UPDATE episodes\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"DQL with gredient descent\")\n",
    "mlflow.start_run()\n",
    "mlflow.log_param(\"MINIMAX_SEARCH_DEPTH\", MINIMAX_SEARCH_DEPTH)\n",
    "mlflow.log_param(\"N_EPISODES\", N_EPISODES)\n",
    "mlflow.log_param(\"N_MATCHES_PER_EPS\", N_MATCHES_PER_EPS)\n",
    "mlflow.log_param(\"REWARD_DISCOUNT_FACTOR\", REWARD_DISCOUNT_FACTOR)\n",
    "mlflow.log_param(\"EPSILON\", EPSILON)\n",
    "mlflow.log_param(\"EPSILON_DECAY_FACTOR\", EPSILON_DECAY_FACTOR)\n",
    "mlflow.log_param(\"EPSILON_MIN\", EPSILON_MIN)\n",
    "mlflow.log_param(\"BATCH_SIZE\", BATCH_SIZE)\n",
    "mlflow.log_param(\"TARGET_UPDATE\", TARGET_UPDATE)\n",
    "mlflow.log_param(\"LEARNING_RATE\", LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the folder if it doesn't exist\n",
    "WEIGHT_FOLDER.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No weights found, starting from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1:  26%|██▌       | 13/50 [00:09<00:26,  1.39matches/s, loss=184, win_rate=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m\n\u001b[1;32m     35\u001b[0m     white_player \u001b[38;5;241m=\u001b[39m MinimaxPlayer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     36\u001b[0m                                 partial(first_order_adv, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m86\u001b[39m, \u001b[38;5;241m54.5\u001b[39m, \u001b[38;5;241m87\u001b[39m, \u001b[38;5;241m26\u001b[39m),\n\u001b[1;32m     37\u001b[0m                                 search_depth\u001b[38;5;241m=\u001b[39mMINIMAX_SEARCH_DEPTH)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# push into environment\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m winner \u001b[38;5;241m=\u001b[39m \u001b[43mplay_a_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblack_player\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_move\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhite_player\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_move\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_show_detail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m winner \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     42\u001b[0m     n_wins \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/pattern_term_project_2024/gym-checkers-for-thai/checkers/agents/baselines.py:58\u001b[0m, in \u001b[0;36mplay_a_game\u001b[0;34m(checkers, black_player_move, white_player_move, max_plies, is_show_detail)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m legal moves \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(moves), moves))\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Select a legal move for the current player\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m from_sq, to_sq \u001b[38;5;241m=\u001b[39m \u001b[43mplayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mturn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_moved_piece\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_show_detail:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(turn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoved \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (from_sq, to_sq))\n",
      "File \u001b[0;32m~/Desktop/pattern_term_project_2024/gym-checkers-for-thai/checkers/agents/alpha_beta.py:76\u001b[0m, in \u001b[0;36mMinimaxPlayer.next_move\u001b[0;34m(self, board, last_moved_piece)\u001b[0m\n\u001b[1;32m     73\u001b[0m     best_move \u001b[38;5;241m=\u001b[39m moves[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# More than one legal move\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     value, best_move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimax_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMinimaxPlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMinimaxPlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# print('move', move, 'value', value)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m dt \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[0;32m~/Desktop/pattern_term_project_2024/gym-checkers-for-thai/checkers/agents/alpha_beta.py:146\u001b[0m, in \u001b[0;36mMinimaxPlayer.minimax_search\u001b[0;34m(self, state, alpha, beta, depth, visited_states)\u001b[0m\n\u001b[1;32m    144\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39msave_state()\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Evaluate the next position\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m value, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimax_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextreme_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisited_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisited_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Update the max value\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extreme_value \u001b[38;5;241m<\u001b[39m value:\n",
      "File \u001b[0;32m~/Desktop/pattern_term_project_2024/gym-checkers-for-thai/checkers/agents/alpha_beta.py:171\u001b[0m, in \u001b[0;36mMinimaxPlayer.minimax_search\u001b[0;34m(self, state, alpha, beta, depth, visited_states)\u001b[0m\n\u001b[1;32m    167\u001b[0m next_board, next_turn, next_last_moved_piece, next_moves, winner \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimulator\u001b[38;5;241m.\u001b[39mmove(\u001b[38;5;241m*\u001b[39mmove, skip_check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    169\u001b[0m )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# print(self.color == turn, depth, move, next_board)\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Evaluate the next position\u001b[39;00m\n\u001b[1;32m    173\u001b[0m value, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminimax_search(\n\u001b[1;32m    174\u001b[0m     next_state,\n\u001b[1;32m    175\u001b[0m     alpha,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m     visited_states\u001b[38;5;241m=\u001b[39mvisited_states,\n\u001b[1;32m    179\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/pattern_term_project_2024/gym-checkers-for-thai/checkers/game.py:409\u001b[0m, in \u001b[0;36mCheckers.save_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboard\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mturn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_moved_piece\n",
      "File \u001b[0;32m~/anaconda3/envs/pattern_project/lib/python3.9/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pattern_project/lib/python3.9/copy.py:230\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 230\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/pattern_project/lib/python3.9/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pattern_project/lib/python3.9/copy.py:230\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 230\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/anaconda3/envs/pattern_project/lib/python3.9/copy.py:144\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(x)\n\u001b[0;32m--> 144\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[43m_deepcopy_dispatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     y \u001b[38;5;241m=\u001b[39m copier(x, memo)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "online_model = GDQL(lr=LEARNING_RATE)\n",
    "target_model = GDQL(lr=LEARNING_RATE)\n",
    "try:\n",
    "    online_model.load_state_dict(torch.load(WEIGHT_FOLDER / \"online_model.pth\"))\n",
    "    target_model.load_state_dict(torch.load(WEIGHT_FOLDER / \"target_model.pth\"))\n",
    "except FileNotFoundError:\n",
    "    print(\"No weights found, starting from scratch\")\n",
    "except RuntimeError:\n",
    "    print(\"Weights are corrupted, starting from scratch\")\n",
    "\n",
    "max_win_rate = 0\n",
    "\n",
    "for episode in range(N_EPISODES):\n",
    "    stime = time.time()\n",
    "    n_wins, n_losses, n_draws = 0, 0, 0\n",
    "    mean_loss = 0\n",
    "    DeepLearningPlayer.experience.clear()\n",
    "\n",
    "\n",
    "    looper = tqdm(range(N_MATCHES_PER_EPS), unit=\"matches\", leave=True, desc=f\"Episode {episode+1}\")\n",
    "    for i in looper:\n",
    "        ch = Checkers()\n",
    "\n",
    "        black_player = DeepLearningPlayer('black',\n",
    "                                model=online_model,\n",
    "                                epsilon=EPSILON,\n",
    "                                epsilon_decay=EPSILON_DECAY_FACTOR,\n",
    "                                epsilon_min=EPSILON_MIN,)\n",
    "\n",
    "        if MINIMAX_SEARCH_DEPTH == 0:\n",
    "            # Random player function\n",
    "            white_player = RandomPlayer('white', seed=i)\n",
    "        else:\n",
    "            # Minimax player function\n",
    "            white_player = MinimaxPlayer('white', \n",
    "                                        partial(first_order_adv, 'white', 86, 54.5, 87, 26),\n",
    "                                        search_depth=MINIMAX_SEARCH_DEPTH)\n",
    "        \n",
    "        # push into environment\n",
    "        winner = play_a_game(ch, black_player.next_move, white_player.next_move, 100, is_show_detail=False)\n",
    "        if winner == 'black':\n",
    "            n_wins += 1\n",
    "            black_player.set_win()\n",
    "        elif winner == 'white':\n",
    "            n_losses += 1\n",
    "            black_player.set_lose()\n",
    "        else:\n",
    "            n_draws += 1\n",
    "\n",
    "        if len(DeepLearningPlayer.experience) > BATCH_SIZE:\n",
    "            batch_states = random.sample(DeepLearningPlayer.experience, BATCH_SIZE)\n",
    "            \n",
    "            # find target, online Q values and compute loss\n",
    "            loss = 0\n",
    "            for batch_idx, (state, action, reward, next_state) in enumerate(batch_states):\n",
    "                online_model.train()\n",
    "                target_model.eval()\n",
    "\n",
    "                # find target Q\n",
    "                if next_state is not None:\n",
    "                    max_next_state_value = -np.inf\n",
    "                    ch.restore_state(state)\n",
    "                    available_actions = ch.legal_moves()\n",
    "                    for available_action in available_actions:\n",
    "                        model_input = target_model.board2input(next_state[0], 'black', available_action)\n",
    "                        next_state_value = target_model(model_input)\n",
    "                        max_next_state_value = max(max_next_state_value, next_state_value)\n",
    "                    target_q = reward + max_next_state_value * REWARD_DISCOUNT_FACTOR\n",
    "                else:\n",
    "                    target_q = reward\n",
    "\n",
    "                # find online Q\n",
    "                model_input = online_model.board2input(state[0], 'black', action)\n",
    "                online_q = online_model(model_input)\n",
    "\n",
    "                loss += (online_q - target_q) ** 2\n",
    "            loss /= BATCH_SIZE\n",
    "            mean_loss += loss.item()\n",
    "            looper.set_postfix(loss=loss.item(),\n",
    "                               win_rate=n_wins / (i+1),)\n",
    "\n",
    "            # compute loss\n",
    "            online_model.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            online_model.optimizer.step()\n",
    "\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_model.load_state_dict(online_model.state_dict())\n",
    "        print(\"\\tTarget model updated\")\n",
    "    print(f\"\\tWins: {n_wins}, Losses: {n_losses}, Draws: {n_draws}\")\n",
    "\n",
    "    mlflow.log_metric(\"runl time\", time.time() - stime, step=episode)\n",
    "    mlflow.log_metric(\"win rate\", n_wins / N_MATCHES_PER_EPS, step=episode)\n",
    "    mlflow.log_metric(\"draw rate\", n_draws / N_MATCHES_PER_EPS, step=episode)\n",
    "    mlflow.log_metric(\"mean of mse loss\", mean_loss / N_MATCHES_PER_EPS, step=episode)\n",
    "    if n_wins / N_MATCHES_PER_EPS > max_win_rate:\n",
    "        max_win_rate = n_wins / N_MATCHES_PER_EPS\n",
    "        torch.save(online_model.state_dict(), WEIGHT_FOLDER / \"online_model.pth\")\n",
    "        torch.save(target_model.state_dict(), WEIGHT_FOLDER / \"target_model.pth\")\n",
    "        print(f\"\\tNew max win rate: {max_win_rate}\")\n",
    "        mlflow.pytorch.log_model(online_model, \"models\")\n",
    "        mlflow.log_artifact(WEIGHT_FOLDER / \"online_model.pth\")\n",
    "        mlflow.log_artifact(WEIGHT_FOLDER / \"target_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_b_b_b_b\n",
      "b_b_b_b_\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_w_w\n",
      "w_w_w_w_\n",
      "0 turn: black last_moved_piece: None\n",
      "7 legal moves [(4, 8), (5, 8), (5, 9), (6, 9), (6, 10), (7, 10), (7, 11)]\n",
      "black moved 6, 10\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._b_\n",
      "_._._b_.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_w_w\n",
      "w_w_w_w_\n",
      "1 turn: white last_moved_piece: None\n",
      "7 legal moves [(24, 21), (24, 20), (25, 22), (25, 21), (26, 23), (26, 22), (27, 23)]\n",
      "white moved 25, 22\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._b_\n",
      "_._._b_.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_w_._w_w\n",
      "w_w_w_w_\n",
      "2 turn: black last_moved_piece: None\n",
      "8 legal moves [(1, 6), (2, 6), (4, 8), (5, 8), (5, 9), (7, 11), (10, 14), (10, 15)]\n",
      "black moved 10, 15\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "._._._b_\n",
      "_._._._.\n",
      "._._w_._\n",
      "_w_._w_w\n",
      "w_w_w_w_\n",
      "3 turn: white last_moved_piece: None\n",
      "8 legal moves [(22, 18), (22, 17), (24, 21), (24, 20), (26, 23), (27, 23), (29, 25), (30, 25)]\n",
      "white moved 27, 23\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "._._._b_\n",
      "_._._._.\n",
      "._._w_w_\n",
      "_w_._w_.\n",
      "w_w_w_w_\n",
      "4 turn: black last_moved_piece: None\n",
      "9 legal moves [(1, 6), (2, 6), (4, 8), (5, 8), (5, 9), (7, 10), (7, 11), (15, 18), (15, 19)]\n",
      "black moved 7, 11\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._._\n",
      "_._._._b\n",
      "._._._b_\n",
      "_._._._.\n",
      "._._w_w_\n",
      "_w_._w_.\n",
      "w_w_w_w_\n",
      "5 turn: white last_moved_piece: None\n",
      "9 legal moves [(22, 18), (22, 17), (23, 19), (23, 18), (24, 21), (24, 20), (29, 25), (30, 25), (31, 27)]\n",
      "white moved 31, 27\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._._\n",
      "_._._._b\n",
      "._._._b_\n",
      "_._._._.\n",
      "._._w_w_\n",
      "_w_._w_w\n",
      "w_w_w_._\n",
      "6 turn: black last_moved_piece: None\n",
      "9 legal moves [(1, 6), (2, 6), (2, 7), (3, 7), (4, 8), (5, 8), (5, 9), (15, 18), (15, 19)]\n",
      "black moved 1, 6\n",
      "\n",
      "_b_._b_b\n",
      "b_b_b_._\n",
      "_._._._b\n",
      "._._._b_\n",
      "_._._._.\n",
      "._._w_w_\n",
      "_w_._w_w\n",
      "w_w_w_._\n",
      "7 turn: white last_moved_piece: None\n",
      "8 legal moves [(22, 18), (22, 17), (23, 19), (23, 18), (24, 21), (24, 20), (29, 25), (30, 25)]\n",
      "white moved 24, 20\n",
      "\n",
      "_b_._b_b\n",
      "b_b_b_._\n",
      "_._._._b\n",
      "._._._b_\n",
      "_._._._.\n",
      "w_._w_w_\n",
      "_._._w_w\n",
      "w_w_w_._\n",
      "8 turn: black last_moved_piece: None\n",
      "9 legal moves [(2, 7), (3, 7), (4, 8), (5, 8), (5, 9), (6, 9), (6, 10), (15, 18), (15, 19)]\n",
      "black moved 15, 18\n",
      "\n",
      "_b_._b_b\n",
      "b_b_b_._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._b_.\n",
      "w_._w_w_\n",
      "_._._w_w\n",
      "w_w_w_._\n",
      "9 turn: white last_moved_piece: None\n",
      "2 legal moves [(22, 15), (23, 14)]\n",
      "white moved 22, 15\n",
      "\n",
      "_b_._b_b\n",
      "b_b_b_._\n",
      "_._._._b\n",
      "._._._w_\n",
      "_._._._.\n",
      "w_._._w_\n",
      "_._._w_w\n",
      "w_w_w_._\n",
      "10 turn: black last_moved_piece: None\n",
      "1 legal moves [(11, 18)]\n",
      "black moved 11, 18\n",
      "\n",
      "_b_._b_b\n",
      "b_b_b_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._b_.\n",
      "w_._._w_\n",
      "_._._w_w\n",
      "w_w_w_._\n",
      "11 turn: white last_moved_piece: None\n",
      "1 legal moves [(23, 14)]\n",
      "white moved 23, 14\n",
      "\n",
      "_b_._b_b\n",
      "b_b_b_._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_._._._.\n",
      "w_._._._\n",
      "_._._w_w\n",
      "w_w_w_._\n",
      "12 turn: black last_moved_piece: None\n",
      "7 legal moves [(2, 7), (3, 7), (4, 8), (5, 8), (5, 9), (6, 9), (6, 10)]\n",
      "black moved 6, 9\n",
      "\n",
      "_b_._b_b\n",
      "b_b_._._\n",
      "_._b_._.\n",
      "._._w_._\n",
      "_._._._.\n",
      "w_._._._\n",
      "_._._w_w\n",
      "w_w_w_._\n",
      "13 turn: white last_moved_piece: None\n",
      "9 legal moves [(27, 23), (14, 10), (20, 16), (26, 23), (26, 22), (28, 24), (29, 25), (29, 24), (30, 25)]\n",
      "white moved 27, 23\n",
      "\n",
      "_b_._b_b\n",
      "b_b_._._\n",
      "_._b_._.\n",
      "._._w_._\n",
      "_._._._.\n",
      "w_._._w_\n",
      "_._._w_.\n",
      "w_w_w_._\n",
      "14 turn: black last_moved_piece: None\n",
      "1 legal moves [(9, 18)]\n",
      "black moved 9, 18\n",
      "\n",
      "_b_._b_b\n",
      "b_b_._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._b_.\n",
      "w_._._w_\n",
      "_._._w_.\n",
      "w_w_w_._\n",
      "15 turn: black last_moved_piece: 18\n",
      "1 legal moves [(18, 27)]\n",
      "black moved 18, 27\n",
      "\n",
      "_b_._b_b\n",
      "b_b_._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._._._\n",
      "_._._w_b\n",
      "w_w_w_._\n",
      "16 turn: white last_moved_piece: None\n",
      "7 legal moves [(20, 16), (26, 23), (26, 22), (28, 24), (29, 25), (29, 24), (30, 25)]\n",
      "white moved 26, 22\n",
      "\n",
      "_b_._b_b\n",
      "b_b_._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_._._._b\n",
      "w_w_w_._\n",
      "17 turn: black last_moved_piece: None\n",
      "7 legal moves [(2, 6), (2, 7), (3, 7), (4, 8), (5, 8), (5, 9), (27, 31)]\n",
      "black moved 4, 8\n",
      "\n",
      "_b_._b_b\n",
      "._b_._._\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_._._._b\n",
      "w_w_w_._\n",
      "18 turn: white last_moved_piece: None\n",
      "8 legal moves [(22, 18), (22, 17), (20, 16), (28, 24), (29, 25), (29, 24), (30, 26), (30, 25)]\n",
      "white moved 29, 25\n",
      "\n",
      "_b_._b_b\n",
      "._b_._._\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_._w_._b\n",
      "w_._w_._\n",
      "19 turn: black last_moved_piece: None\n",
      "8 legal moves [(0, 4), (2, 6), (2, 7), (3, 7), (5, 9), (8, 12), (8, 13), (27, 31)]\n",
      "black moved 27, 31\n",
      "\n",
      "_b_._b_b\n",
      "._b_._._\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_._w_._.\n",
      "w_._w_B_\n",
      "20 turn: white last_moved_piece: None\n",
      "6 legal moves [(25, 21), (22, 18), (22, 17), (20, 16), (28, 24), (30, 26)]\n",
      "white moved 30, 26\n",
      "\n",
      "_b_._b_b\n",
      "._b_._._\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_._w_w_.\n",
      "w_._._B_\n",
      "21 turn: black last_moved_piece: None\n",
      "8 legal moves [(0, 4), (2, 6), (2, 7), (3, 7), (5, 9), (8, 12), (8, 13), (31, 27)]\n",
      "black moved 2, 6\n",
      "\n",
      "_b_._._b\n",
      "._b_b_._\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_._w_w_.\n",
      "w_._._B_\n",
      "22 turn: white last_moved_piece: None\n",
      "6 legal moves [(26, 23), (25, 21), (22, 18), (22, 17), (20, 16), (28, 24)]\n",
      "white moved 28, 24\n",
      "\n",
      "_b_._._b\n",
      "._b_b_._\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_w_w_w_.\n",
      "._._._B_\n",
      "23 turn: black last_moved_piece: None\n",
      "8 legal moves [(0, 4), (3, 7), (5, 9), (6, 9), (6, 10), (8, 12), (8, 13), (31, 27)]\n",
      "black moved 6, 9\n",
      "\n",
      "_b_._._b\n",
      "._b_._._\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._w_._\n",
      "_w_w_w_.\n",
      "._._._B_\n",
      "24 turn: white last_moved_piece: None\n",
      "6 legal moves [(20, 16), (22, 18), (22, 17), (24, 21), (25, 21), (26, 23)]\n",
      "white moved 25, 21\n",
      "\n",
      "_b_._._b\n",
      "._b_._._\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_w_w_._\n",
      "_w_._w_.\n",
      "._._._B_\n",
      "25 turn: black last_moved_piece: None\n",
      "7 legal moves [(0, 4), (3, 7), (8, 12), (8, 13), (9, 13), (9, 14), (31, 27)]\n",
      "black moved 3, 7\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._._._.\n",
      "w_w_w_._\n",
      "_w_._w_.\n",
      "._._._B_\n",
      "26 turn: white last_moved_piece: None\n",
      "6 legal moves [(20, 16), (21, 17), (21, 16), (22, 18), (22, 17), (26, 23)]\n",
      "white moved 21, 17\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._w_._.\n",
      "w_._w_._\n",
      "_w_._w_.\n",
      "._._._B_\n",
      "27 turn: black last_moved_piece: None\n",
      "8 legal moves [(0, 4), (7, 10), (7, 11), (8, 12), (8, 13), (9, 13), (9, 14), (31, 27)]\n",
      "black moved 8, 12\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_._w_._.\n",
      "w_._w_._\n",
      "_w_._w_.\n",
      "._._._B_\n",
      "28 turn: white last_moved_piece: None\n",
      "6 legal moves [(17, 14), (17, 13), (20, 16), (22, 18), (24, 21), (26, 23)]\n",
      "white moved 20, 16\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_w_w_._.\n",
      "._._w_._\n",
      "_w_._w_.\n",
      "._._._B_\n",
      "29 turn: black last_moved_piece: None\n",
      "1 legal moves [(12, 21)]\n",
      "black moved 12, 21\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_._.\n",
      "._._._._\n",
      "_._w_._.\n",
      "._b_w_._\n",
      "_w_._w_.\n",
      "._._._B_\n",
      "30 turn: black last_moved_piece: 21\n",
      "1 legal moves [(21, 28)]\n",
      "black moved 21, 28\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_._.\n",
      "._._._._\n",
      "_._w_._.\n",
      "._._w_._\n",
      "_._._w_.\n",
      "B_._._B_\n",
      "31 turn: white last_moved_piece: None\n",
      "4 legal moves [(17, 14), (17, 13), (22, 18), (26, 23)]\n",
      "white moved 17, 14\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_._.\n",
      "._._w_._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_._._w_.\n",
      "B_._._B_\n",
      "32 turn: black last_moved_piece: None\n",
      "2 legal moves [(9, 18), (28, 10)]\n",
      "black moved 28, 10\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_B_.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_._._w_.\n",
      "._._._B_\n",
      "33 turn: white last_moved_piece: None\n",
      "3 legal moves [(22, 18), (22, 17), (26, 23)]\n",
      "white moved 26, 23\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_B_.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._w_w_\n",
      "_._._._.\n",
      "._._._B_\n",
      "34 turn: black last_moved_piece: None\n",
      "1 legal moves [(31, 17)]\n",
      "black moved 31, 17\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_B_.\n",
      "._._._._\n",
      "_._B_._.\n",
      "._._._w_\n",
      "_._._._.\n",
      "._._._._\n",
      "35 turn: white last_moved_piece: None\n",
      "2 legal moves [(23, 19), (23, 18)]\n",
      "white moved 23, 19\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_B_.\n",
      "._._._._\n",
      "_._B_._w\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "36 turn: black last_moved_piece: None\n",
      "19 legal moves [(0, 4), (5, 8), (7, 11), (9, 13), (9, 14), (17, 21), (17, 24), (17, 28), (17, 22), (17, 26), (17, 31), (17, 14), (17, 13), (17, 8), (17, 4), (10, 14), (10, 15), (10, 6), (10, 1)]\n",
      "black moved 17, 31\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_B_.\n",
      "._._._._\n",
      "_._._._w\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._B_\n",
      "37 turn: white last_moved_piece: None\n",
      "1 legal moves [(19, 15)]\n",
      "white moved 19, 15\n",
      "\n",
      "_b_._._.\n",
      "._b_._b_\n",
      "_._b_B_.\n",
      "._._._w_\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._B_\n",
      "38 turn: black last_moved_piece: None\n",
      "1 legal moves [(10, 19)]\n",
      "black moved 10, 19\n",
      "\n",
      "black player wins\n",
      "total legal moves 222 avg branching factor 5.6923076923076925\n"
     ]
    }
   ],
   "source": [
    "ch = Checkers()\n",
    "\n",
    "black_player = DeepLearningPlayer('black',\n",
    "                                model=online_model,\n",
    "                                epsilon=EPSILON,\n",
    "                                epsilon_decay=EPSILON_DECAY_FACTOR,\n",
    "                                epsilon_min=EPSILON_MIN,)\n",
    "# Random player function\n",
    "white_player = RandomPlayer('white', seed=i)\n",
    "        \n",
    "# push into environment\n",
    "winner = play_a_game(ch, black_player.next_move, white_player.next_move, 100, is_show_detail=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
