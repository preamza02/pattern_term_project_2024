{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.utils import append_gym_checker\n",
    "\n",
    "# append_gym_checker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "pwd = Path(os.getcwd())\n",
    "if (pwd.name == \"notebooks\"):\n",
    "    sys.path.append(str(pwd.parent / \"gym-checkers-for-thai\"))\n",
    "else:\n",
    "    sys.path.append(str(pwd / \"gym-checkers-for-thai\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/boat/pattern/pattern_term_project_2024\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkers.game import Checkers\n",
    "from checkers.agents.baselines import play_a_game\n",
    "from checkers.game import Checkers\n",
    "from checkers.agents.alpha_beta import MinimaxPlayer, first_order_adv, material_value_adv\n",
    "from tqdm import tqdm\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/boat/pattern/pattern_term_project_2024\n"
     ]
    }
   ],
   "source": [
    "from src.prame_q_learn.agent.white_checker_agent import WhiteCheckerAgent\n",
    "from src.enum import RESULT_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# if os.path.exists(\"src/prame_q_learn/agent/weight/white_weight.json\"):\n",
    "#     os.remove(\"src/prame_q_learn/agent/weight/white_weight.json\")\n",
    "# open(\"src/prame_q_learn/agent/weight/white_weight.json\",\"w\").write(\"{}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                        | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game 0\n",
      "_b_b_b_b\n",
      "b_b_b_b_\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_w_w\n",
      "w_w_w_w_\n",
      "0 turn: black last_moved_piece: None\n",
      "7 legal moves [(4, 8), (5, 8), (5, 9), (6, 9), (6, 10), (7, 10), (7, 11)]\n",
      "black moved 5, 8\n",
      "_b_b_b_b\n",
      "b_._b_b_\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_w_w\n",
      "w_w_w_w_\n",
      "1 turn: white last_moved_piece: None\n",
      "7 legal moves [(24, 21), (24, 20), (25, 22), (25, 21), (26, 23), (26, 22), (27, 23)]\n",
      "white moved 26, 22\n",
      "_b_b_b_b\n",
      "b_._b_b_\n",
      "_b_._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_w_w_._w\n",
      "w_w_w_w_\n",
      "2 turn: black last_moved_piece: None\n",
      "8 legal moves [(0, 5), (1, 5), (6, 9), (6, 10), (7, 10), (7, 11), (8, 12), (8, 13)]\n",
      "black moved 7, 11\n",
      "_b_b_b_b\n",
      "b_._b_._\n",
      "_b_._._b\n",
      "._._._._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_w_w_._w\n",
      "w_w_w_w_\n",
      "3 turn: white last_moved_piece: None\n",
      "8 legal moves [(22, 18), (22, 17), (24, 21), (24, 20), (25, 21), (27, 23), (30, 26), (31, 26)]\n",
      "white moved 22, 18\n",
      "_b_b_b_b\n",
      "b_._b_._\n",
      "_b_._._b\n",
      "._._._._\n",
      "_._._w_.\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_w_w_\n",
      "4 turn: black last_moved_piece: None\n",
      "9 legal moves [(0, 5), (1, 5), (2, 7), (3, 7), (6, 9), (6, 10), (8, 12), (8, 13), (11, 15)]\n",
      "black moved 6, 9\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_b_b_._b\n",
      "._._._._\n",
      "_._._w_.\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_w_w_\n",
      "5 turn: white last_moved_piece: None\n",
      "9 legal moves [(18, 15), (18, 14), (24, 21), (24, 20), (25, 22), (25, 21), (27, 23), (30, 26), (31, 26)]\n",
      "white moved 18, 14\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_b_b_._b\n",
      "._._w_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_w_w_\n",
      "6 turn: black last_moved_piece: None\n",
      "1 legal moves [(9, 18)]\n",
      "black moved 9, 18\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_b_._._b\n",
      "._._._._\n",
      "_._._b_.\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_w_w_\n",
      "7 turn: white last_moved_piece: None\n",
      "7 legal moves [(24, 21), (24, 20), (25, 22), (25, 21), (27, 23), (30, 26), (31, 26)]\n",
      "white moved 30, 26\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_b_._._b\n",
      "._._._._\n",
      "_._._b_.\n",
      "._._._._\n",
      "_w_w_w_w\n",
      "w_w_._w_\n",
      "8 turn: black last_moved_piece: None\n",
      "11 legal moves [(0, 5), (1, 5), (1, 6), (2, 6), (2, 7), (3, 7), (8, 12), (8, 13), (11, 15), (18, 22), (18, 23)]\n",
      "black moved 8, 12\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_._._._b\n",
      "b_._._._\n",
      "_._._b_.\n",
      "._._._._\n",
      "_w_w_w_w\n",
      "w_w_._w_\n",
      "9 turn: white last_moved_piece: None\n",
      "7 legal moves [(24, 21), (24, 20), (25, 22), (25, 21), (26, 23), (26, 22), (27, 23)]\n",
      "white moved 26, 22\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_._._._b\n",
      "b_._._._\n",
      "_._._b_.\n",
      "._._w_._\n",
      "_w_w_._w\n",
      "w_w_._w_\n",
      "10 turn: black last_moved_piece: None\n",
      "10 legal moves [(0, 5), (1, 5), (1, 6), (2, 6), (2, 7), (3, 7), (4, 8), (11, 15), (12, 16), (18, 23)]\n",
      "black moved 11, 15\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_._._._.\n",
      "b_._._b_\n",
      "_._._b_.\n",
      "._._w_._\n",
      "_w_w_._w\n",
      "w_w_._w_\n",
      "11 turn: white last_moved_piece: None\n",
      "6 legal moves [(22, 17), (24, 21), (24, 20), (25, 21), (27, 23), (31, 26)]\n",
      "white moved 22, 17\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_._._._.\n",
      "b_._._b_\n",
      "_._w_b_.\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_._w_\n",
      "12 turn: black last_moved_piece: None\n",
      "11 legal moves [(0, 5), (1, 5), (1, 6), (2, 6), (2, 7), (3, 7), (4, 8), (12, 16), (15, 19), (18, 22), (18, 23)]\n",
      "black moved 15, 19\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_._._._.\n",
      "b_._._._\n",
      "_._w_b_b\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_._w_\n",
      "13 turn: white last_moved_piece: None\n",
      "8 legal moves [(17, 14), (17, 13), (24, 21), (24, 20), (25, 22), (25, 21), (27, 23), (31, 26)]\n",
      "white moved 17, 14\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_._._._.\n",
      "b_._w_._\n",
      "_._._b_b\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_._w_\n",
      "14 turn: black last_moved_piece: None\n",
      "11 legal moves [(0, 5), (1, 5), (1, 6), (2, 6), (2, 7), (3, 7), (4, 8), (12, 16), (18, 22), (18, 23), (19, 23)]\n",
      "black moved 2, 7\n",
      "_b_b_._b\n",
      "b_._._b_\n",
      "_._._._.\n",
      "b_._w_._\n",
      "_._._b_b\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_._w_\n",
      "15 turn: white last_moved_piece: None\n",
      "8 legal moves [(14, 10), (14, 9), (24, 21), (24, 20), (25, 22), (25, 21), (27, 23), (31, 26)]\n",
      "white moved 14, 10\n",
      "_b_b_._b\n",
      "b_._._b_\n",
      "_._._w_.\n",
      "b_._._._\n",
      "_._._b_b\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_._w_\n",
      "16 turn: black last_moved_piece: None\n",
      "1 legal moves [(7, 14)]\n",
      "black moved 7, 14\n",
      "_b_b_._b\n",
      "b_._._._\n",
      "_._._._.\n",
      "b_._b_._\n",
      "_._._b_b\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_._w_\n",
      "17 turn: white last_moved_piece: None\n",
      "6 legal moves [(24, 21), (24, 20), (25, 22), (25, 21), (27, 23), (31, 26)]\n",
      "white moved 24, 21\n",
      "_b_b_._b\n",
      "b_._._._\n",
      "_._._._.\n",
      "b_._b_._\n",
      "_._._b_b\n",
      "._w_._._\n",
      "_._w_._w\n",
      "w_w_._w_\n",
      "18 turn: black last_moved_piece: None\n",
      "10 legal moves [(0, 5), (1, 5), (1, 6), (3, 7), (4, 8), (12, 16), (14, 17), (18, 22), (18, 23), (19, 23)]\n",
      "black moved 3, 7\n",
      "_b_b_._.\n",
      "b_._._b_\n",
      "_._._._.\n",
      "b_._b_._\n",
      "_._._b_b\n",
      "._w_._._\n",
      "_._w_._w\n",
      "w_w_._w_\n",
      "19 turn: white last_moved_piece: None\n",
      "7 legal moves [(21, 17), (21, 16), (25, 22), (27, 23), (28, 24), (29, 24), (31, 26)]\n",
      "white moved 21, 17\n",
      "_b_b_._.\n",
      "b_._._b_\n",
      "_._._._.\n",
      "b_._b_._\n",
      "_._w_b_b\n",
      "._._._._\n",
      "_._w_._w\n",
      "w_w_._w_\n",
      "20 turn: black last_moved_piece: None\n",
      "1 legal moves [(14, 21)]\n",
      "black moved 14, 21\n",
      "_b_b_._.\n",
      "b_._._b_\n",
      "_._._._.\n",
      "b_._._._\n",
      "_._._b_b\n",
      "._b_._._\n",
      "_._w_._w\n",
      "w_w_._w_\n",
      "21 turn: black last_moved_piece: 21\n",
      "1 legal moves [(21, 30)]\n",
      "black moved 21, 30\n",
      "_b_b_._.\n",
      "b_._._b_\n",
      "_._._._.\n",
      "b_._._._\n",
      "_._._b_b\n",
      "._._._._\n",
      "_._._._w\n",
      "w_w_B_w_\n",
      "22 turn: white last_moved_piece: None\n",
      "5 legal moves [(27, 23), (28, 24), (29, 25), (29, 24), (31, 26)]\n",
      "white moved 27, 23\n",
      "_b_b_._.\n",
      "b_._._b_\n",
      "_._._._.\n",
      "b_._._._\n",
      "_._._b_b\n",
      "._._._w_\n",
      "_._._._.\n",
      "w_w_B_w_\n",
      "23 turn: black last_moved_piece: None\n",
      "2 legal moves [(18, 27), (19, 26)]\n",
      "black moved 18, 27\n",
      "_b_b_._.\n",
      "b_._._b_\n",
      "_._._._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._._b\n",
      "w_w_B_w_\n",
      "24 turn: white last_moved_piece: None\n",
      "4 legal moves [(28, 24), (29, 25), (29, 24), (31, 26)]\n",
      "white moved 28, 24\n",
      "_b_b_._.\n",
      "b_._._b_\n",
      "_._._._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._._._\n",
      "_w_._._b\n",
      "._w_B_w_\n",
      "25 turn: black last_moved_piece: None\n",
      "13 legal moves [(0, 5), (1, 5), (1, 6), (4, 8), (7, 10), (7, 11), (12, 16), (19, 23), (30, 26), (30, 23), (30, 25), (30, 21), (30, 16)]\n",
      "black moved 0, 5\n",
      "_._b_._.\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._._._\n",
      "_w_._._b\n",
      "._w_B_w_\n",
      "26 turn: white last_moved_piece: None\n",
      "4 legal moves [(24, 21), (24, 20), (29, 25), (31, 26)]\n",
      "white moved 24, 21\n",
      "_._b_._.\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._w_._._\n",
      "_._._._b\n",
      "._w_B_w_\n",
      "27 turn: black last_moved_piece: None\n",
      "1 legal moves [(30, 16)]\n",
      "black moved 30, 16\n",
      "_._b_._.\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "b_._._._\n",
      "_B_._._b\n",
      "._._._._\n",
      "_._._._b\n",
      "._w_._w_\n",
      "28 turn: white last_moved_piece: None\n",
      "3 legal moves [(29, 25), (29, 24), (31, 26)]\n",
      "white moved 31, 26\n",
      "_._b_._.\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "b_._._._\n",
      "_B_._._b\n",
      "._._._._\n",
      "_._._w_b\n",
      "._w_._._\n",
      "29 turn: black last_moved_piece: None\n",
      "16 legal moves [(1, 6), (4, 8), (5, 8), (5, 9), (7, 10), (7, 11), (19, 23), (27, 31), (16, 20), (16, 21), (16, 25), (16, 30), (16, 13), (16, 9), (16, 6), (16, 2)]\n",
      "black moved 16, 13\n",
      "_._b_._.\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "b_B_._._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._w_b\n",
      "._w_._._\n",
      "30 turn: white last_moved_piece: None\n",
      "4 legal moves [(26, 23), (26, 22), (29, 25), (29, 24)]\n",
      "white moved 26, 23\n",
      "_._b_._.\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "b_B_._._\n",
      "_._._._b\n",
      "._._._w_\n",
      "_._._._b\n",
      "._w_._._\n",
      "31 turn: black last_moved_piece: None\n",
      "1 legal moves [(19, 26)]\n",
      "black moved 19, 26\n",
      "_._b_._.\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "b_B_._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._b_b\n",
      "._w_._._\n",
      "32 turn: white last_moved_piece: None\n",
      "2 legal moves [(29, 25), (29, 24)]\n",
      "white moved 29, 25\n",
      "_._b_._.\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "b_B_._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._w_b_b\n",
      "._._._._\n",
      "33 turn: black last_moved_piece: None\n",
      "18 legal moves [(1, 6), (4, 8), (5, 8), (5, 9), (7, 10), (7, 11), (12, 16), (26, 30), (26, 31), (27, 31), (13, 16), (13, 20), (13, 17), (13, 22), (13, 9), (13, 6), (13, 2), (13, 8)]\n",
      "black moved 26, 30\n",
      "_._b_._.\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "b_B_._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._w_._b\n",
      "._._B_._\n",
      "34 turn: white last_moved_piece: None\n",
      "2 legal moves [(25, 22), (25, 21)]\n",
      "white moved 25, 22\n",
      "_._b_._.\n",
      "b_b_._b_\n",
      "_._._._.\n",
      "b_B_._._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_._._._b\n",
      "._._B_._\n",
      "35 turn: black last_moved_piece: None\n",
      "1 legal moves [(13, 26)]\n",
      "black moved 13, 26\n",
      "black player wins\n",
      "total legal moves 230 avg branching factor 6.388888888888889\n",
      "black player evaluated 3449 positions in 0.54s (avg 6347.22 positions/s) effective branching factor 5.66\n",
      "black player pruned dict_items([(('beta', 1), 355), (('alpha', 1), 5)])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 1/1 result lose, explore_rate 0.1\n",
      "black win 1 draw 0 loss 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# A few matches against a random player\n",
    "max_game_len = 100\n",
    "n_matches = 1\n",
    "n_wins, n_draws, n_losses = 0, 0, 0\n",
    "is_show_game = True\n",
    "explore_rate=0.1\n",
    "for i in tqdm(range(n_matches)):\n",
    "    if is_show_game:\n",
    "        print('game', i)\n",
    "    ch = Checkers()\n",
    "    black_player = MinimaxPlayer(\n",
    "        'black',\n",
    "        # value_func=partial(first_order_adv, 'black', 200, 100, 20, 0),\n",
    "        value_func=partial(first_order_adv, 'black', 86.0315, 54.568, 87.21072, 25.85066),        \n",
    "        # The provided legal moves might be ordered differently\n",
    "        rollout_order_gen=lambda x: sorted(x),\n",
    "        search_depth=3,\n",
    "        seed=i)\n",
    "\n",
    "    # white_player = MinimaxPlayer('white', value_func=partial(material_value_adv, 'white', 2, 1), search_depth=4, seed=i * 2)\n",
    "    # white_player = RandomPlayer('white', seed=i * 2)\n",
    "    white_player = WhiteCheckerAgent(weight_path = \"src/prame_q_learn/agent/weight/white_weight.json\",        \n",
    "                                    explore_rate=explore_rate,\n",
    "                                    explore_rate_decay_factor=1.0,\n",
    "                                    alpha= 0.9,\n",
    "                                    gamma= 0.5,\n",
    "                                    win_score= 100,\n",
    "                                    draw_score= -5,\n",
    "                                    lose_score= -50,\n",
    "                                    )\n",
    "\n",
    "    #modify this function to put our RL model as white\n",
    "    winner = play_a_game(ch, black_player.next_move, white_player.next_move, max_game_len,is_show_detail = is_show_game)\n",
    "\n",
    "    # Play with a minimax player\n",
    "    # play_a_game(ch, keyboard_player_move, white_player.next_move)\n",
    "    if is_show_game:\n",
    "        print('black player evaluated %i positions in %.2fs (avg %.2f positions/s) effective branching factor %.2f' % (black_player.n_evaluated_positions, black_player.evaluation_dt, black_player.n_evaluated_positions / black_player.evaluation_dt, (black_player.n_evaluated_positions / black_player.ply) ** (1 / black_player.search_depth)))\n",
    "        print('black player pruned', black_player.prunes.items())\n",
    "        print()\n",
    "    # Keep scores\n",
    "    # n_wins += 1 if winner == 'black' else 0\n",
    "    # n_draws += 1 if winner is None else 0\n",
    "    # n_losses += 1 if winner == 'white' else 0\n",
    "    result:RESULT_TYPE\n",
    "    if winner == 'black':\n",
    "        n_wins += 1\n",
    "        result = RESULT_TYPE.LOSE\n",
    "    elif winner is None:\n",
    "        n_draws += 1\n",
    "        result = RESULT_TYPE.DRAW\n",
    "    else:\n",
    "        n_losses += 1\n",
    "        result = RESULT_TYPE.WIN\n",
    "    white_player.get_result(result)\n",
    "    print(f\"round : {i+1}/{n_matches} result {result.value}, explore_rate {explore_rate}\")\n",
    "    \n",
    "    explore_rate = white_player._explore_rate\n",
    "\n",
    "print('black win', n_wins, 'draw', n_draws, 'loss', n_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth N supervised model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## depth 5 vs depth 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 1/1 result win, explore_rate 0.1\n",
      "black win 0 draw 0 loss 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# A few matches against a random player\n",
    "max_game_len = 100\n",
    "n_matches = 1\n",
    "n_wins, n_draws, n_losses = 0, 0, 0\n",
    "is_show_game = False\n",
    "explore_rate=0.1\n",
    "\n",
    "def rollout_order_gen_random(x):\n",
    "    random.shuffle(x)\n",
    "    return x\n",
    "    \n",
    "for i in tqdm(range(n_matches)):\n",
    "    if is_show_game:\n",
    "        print('game', i)\n",
    "    ch = Checkers()\n",
    "    black_player = MinimaxPlayer(\n",
    "        'black',\n",
    "        # value_func=partial(first_order_adv, 'black', 200, 100, 20, 0),\n",
    "        value_func=partial(first_order_adv, 'black', 86.0315, 54.568, 87.21072, 25.85066),        \n",
    "        # The provided legal moves might be ordered differently\n",
    "        rollout_order_gen=rollout_order_gen_random,\n",
    "        search_depth=3,\n",
    "        seed=i)\n",
    "\n",
    "    white_player = MinimaxPlayer(\n",
    "                    'white',\n",
    "                    # value_func=partial(first_order_adv, 'black', 200, 100, 20, 0),\n",
    "                    value_func=partial(first_order_adv, 'white', 86.0315, 54.568, 87.21072, 25.85066),        \n",
    "                    # The provided legal moves might be ordered differently\n",
    "                    rollout_order_gen=rollout_order_gen_random,\n",
    "                    search_depth=4,\n",
    "                    seed=i)\n",
    "\n",
    "    #modify this function to put our RL model as white\n",
    "    winner = play_a_game(ch, black_player.next_move, white_player.next_move, max_game_len,is_show_detail = is_show_game)\n",
    "\n",
    "    # Play with a minimax player\n",
    "    # play_a_game(ch, keyboard_player_move, white_player.next_move)\n",
    "    if is_show_game:\n",
    "        print('black player evaluated %i positions in %.2fs (avg %.2f positions/s) effective branching factor %.2f' % (black_player.n_evaluated_positions, black_player.evaluation_dt, black_player.n_evaluated_positions / black_player.evaluation_dt, (black_player.n_evaluated_positions / black_player.ply) ** (1 / black_player.search_depth)))\n",
    "        print('black player pruned', black_player.prunes.items())\n",
    "        print()\n",
    "    result:RESULT_TYPE\n",
    "    if winner == 'black':\n",
    "        n_wins += 1\n",
    "        result = RESULT_TYPE.LOSE\n",
    "    elif winner is None:\n",
    "        n_draws += 1\n",
    "        result = RESULT_TYPE.DRAW\n",
    "    else:\n",
    "        n_losses += 1\n",
    "        result = RESULT_TYPE.WIN\n",
    "\n",
    "    print(f\"round : {i+1}/{n_matches} result {result.value}, explore_rate {explore_rate}\")\n",
    "    # print(white_player.board_move_dict)\n",
    "\n",
    "print('black win', n_wins, 'draw', n_draws, 'loss', n_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'black': {'men': {0, 1, 2, 3, 4, 6, 7, 8}, 'kings': set()},\n",
       "  'white': {'men': {24, 25, 26, 27, 28, 29, 30, 31}, 'kings': set()}},\n",
       " (24, 20)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white_player.board_move_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def encode_board_to_feature_map(board):\n",
    "    output_maps = torch.zeros((4, 8, 4))\n",
    "    for k,(side, typ) in enumerate([[\"black\",\"men\"], [\"black\", \"kings\"], [\"white\",\"men\"],[\"white\",\"kings\"]]):\n",
    "        _indices = list(board[side][typ])\n",
    "        _2d_indices = [[int(i/4), i % 4] for i in _indices]\n",
    "        _maps = torch.zeros((1,8,4))\n",
    "        for i,j in _2d_indices:\n",
    "            _maps[:,i,j] = torch.tensor([1.0])\n",
    "        output_maps[k, :, :] = _maps\n",
    "    return output_maps\n",
    "\n",
    "def encode_action(action):\n",
    "    output_maps = torch.zeros((2, 8, 4))\n",
    "    a = [int(action[0] / 4), action[0] % 4]\n",
    "    b = [int(action[1] / 4), action[1] % 4]\n",
    "    output_maps[0, a[0],a[1]] = 1.0\n",
    "    output_maps[1, b[0],b[1]] = 1.0\n",
    "    return output_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 0., 1., 1.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_board_to_feature_map(white_player.board_move_dict[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_action(white_player.board_move_dict[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerDenseModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class MultiLayerDenseModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes):\n",
    "        super(MultiLayerDenseModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        \n",
    "        # Define layers\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_features, hidden_size))\n",
    "            layers.append(nn.ReLU())  # You can change the activation function if needed\n",
    "            in_features = hidden_size\n",
    "        layers.append(nn.Linear(in_features, output_size))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        # Create the sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Example usage:\n",
    "input_size = 128\n",
    "output_size = 64\n",
    "hidden_sizes = [256, 512, 256]  # Example hidden layer sizes\n",
    "model = MultiLayerDenseModel(input_size, output_size, hidden_sizes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in_data = encode_board_to_feature_map(white_player.board_move_dict[0][0])\n",
    "# in_data = in_data[None,:,:,:]\n",
    "# print(in_data.shape)\n",
    "# in_data = in_data.flatten(start_dim=1)\n",
    "# print(in_data.shape)\n",
    "# model.forward(in_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                                         | 1/1000 [00:03<1:02:49,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 1/1000 result win\n",
      "start training\n",
      "train loss: tensor(175.6419, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 174.19525146484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                                         | 2/1000 [00:07<1:02:59,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 2/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                                                         | 3/1000 [00:11<1:03:21,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 3/1000 result win\n",
      "start training\n",
      "train loss: tensor(252.9847, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(93.8799, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 170.19441986083984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▌                                                                                                                                         | 4/1000 [00:15<1:03:41,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 4/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▋                                                                                                                                         | 5/1000 [00:19<1:03:49,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 5/1000 result win\n",
      "start training\n",
      "train loss: tensor(247.4570, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(91.7366, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 165.20427322387695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                                                                         | 6/1000 [00:23<1:03:48,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 6/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                                                                         | 7/1000 [00:26<1:03:45,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 7/1000 result win\n",
      "start training\n",
      "train loss: tensor(239.8025, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(88.8952, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 157.88634490966797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                                                                                         | 8/1000 [00:30<1:03:38,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 8/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                                                                        | 9/1000 [00:34<1:03:49,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 9/1000 result win\n",
      "start training\n",
      "train loss: tensor(229.5883, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(83.5456, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 147.27031707763672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▎                                                                                                                                       | 10/1000 [00:38<1:03:53,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 10/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▌                                                                                                                                       | 11/1000 [00:42<1:03:57,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 11/1000 result win\n",
      "start training\n",
      "train loss: tensor(214.5673, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(76.1817, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 132.43588256835938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▋                                                                                                                                       | 12/1000 [00:46<1:03:53,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 12/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▊                                                                                                                                       | 13/1000 [00:50<1:03:58,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 13/1000 result win\n",
      "start training\n",
      "train loss: tensor(193.3159, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(66.3624, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 112.93603515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▉                                                                                                                                       | 14/1000 [00:54<1:03:56,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 14/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██                                                                                                                                       | 15/1000 [00:58<1:04:04,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 15/1000 result win\n",
      "start training\n",
      "train loss: tensor(164.8786, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(54.6486, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 89.54674530029297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▏                                                                                                                                      | 16/1000 [01:01<1:03:59,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 16/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▎                                                                                                                                      | 17/1000 [01:05<1:04:04,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 17/1000 result win\n",
      "start training\n",
      "train loss: tensor(129.4629, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(42.8556, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 65.13505554199219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▍                                                                                                                                      | 18/1000 [01:09<1:04:09,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 18/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▌                                                                                                                                      | 19/1000 [01:13<1:03:57,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 19/1000 result win\n",
      "start training\n",
      "train loss: tensor(97.8208, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(26.4186, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 44.255319595336914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▋                                                                                                                                      | 20/1000 [01:17<1:03:50,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 20/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▉                                                                                                                                      | 21/1000 [01:21<1:03:47,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 21/1000 result win\n",
      "start training\n",
      "train loss: tensor(66.0166, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(18.4269, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 30.598466873168945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███                                                                                                                                      | 22/1000 [01:25<1:03:42,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 22/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▏                                                                                                                                     | 23/1000 [01:29<1:03:31,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 23/1000 result win\n",
      "start training\n",
      "train loss: tensor(45.1731, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(14.0600, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 24.1253080368042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▎                                                                                                                                     | 24/1000 [01:33<1:03:12,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 24/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▍                                                                                                                                     | 25/1000 [01:37<1:03:11,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 25/1000 result win\n",
      "start training\n",
      "train loss: tensor(34.9884, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(12.4431, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.99210023880005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▌                                                                                                                                     | 26/1000 [01:40<1:03:11,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 26/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▋                                                                                                                                     | 27/1000 [01:44<1:03:06,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 27/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.8423, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9324, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.574571132659912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▊                                                                                                                                     | 28/1000 [01:48<1:03:01,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 28/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|███▉                                                                                                                                     | 29/1000 [01:52<1:03:03,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 29/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.1109, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9430, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.617719650268555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████                                                                                                                                     | 30/1000 [01:56<1:02:50,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 30/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████▏                                                                                                                                    | 31/1000 [02:00<1:02:42,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 31/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.4265, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.8486, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.724026203155518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████▍                                                                                                                                    | 32/1000 [02:04<1:02:35,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 32/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████▌                                                                                                                                    | 33/1000 [02:08<1:02:41,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 33/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.6203, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.8610, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.808034896850586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████▋                                                                                                                                    | 34/1000 [02:12<1:02:45,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 34/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▊                                                                                                                                    | 35/1000 [02:15<1:02:36,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 35/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.7023, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9293, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.864033699035645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████▉                                                                                                                                    | 36/1000 [02:19<1:02:16,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 36/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████                                                                                                                                    | 37/1000 [02:23<1:02:16,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 37/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.8236, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9168, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.900155067443848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████▏                                                                                                                                   | 38/1000 [02:27<1:02:14,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 38/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████▎                                                                                                                                   | 39/1000 [02:31<1:02:34,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 39/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.8443, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9622, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.923516273498535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████▍                                                                                                                                   | 40/1000 [02:35<1:02:31,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 40/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████▌                                                                                                                                   | 41/1000 [02:39<1:02:28,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 41/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.8885, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9635, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.938965797424316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████▊                                                                                                                                   | 42/1000 [02:43<1:02:27,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 42/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████▉                                                                                                                                   | 43/1000 [02:47<1:02:20,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 43/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9121, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9692, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.94943618774414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██████                                                                                                                                   | 44/1000 [02:51<1:02:15,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 44/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██████▏                                                                                                                                  | 45/1000 [02:54<1:02:20,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 45/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9224, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9788, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.956689834594727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▎                                                                                                                                  | 46/1000 [02:58<1:02:03,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 46/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▍                                                                                                                                  | 47/1000 [03:02<1:01:58,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 47/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9309, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9839, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.961822509765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▌                                                                                                                                  | 48/1000 [03:06<1:01:53,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 48/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▋                                                                                                                                  | 49/1000 [03:10<1:02:11,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 49/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9479, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9771, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.96552085876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▊                                                                                                                                  | 50/1000 [03:14<1:02:06,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 50/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▉                                                                                                                                  | 51/1000 [03:18<1:01:59,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 51/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9409, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9906, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.968201160430908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███████                                                                                                                                  | 52/1000 [03:22<1:01:44,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 52/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███████▎                                                                                                                                 | 53/1000 [03:26<1:01:52,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 53/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9530, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9840, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.970168590545654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███████▍                                                                                                                                 | 54/1000 [03:30<1:01:49,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 54/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███████▌                                                                                                                                 | 55/1000 [03:34<1:01:37,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 55/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9647, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9761, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.971611976623535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███████▋                                                                                                                                 | 56/1000 [03:38<1:01:36,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 56/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███████▊                                                                                                                                 | 57/1000 [03:42<1:01:47,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 57/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9562, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9873, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.972641944885254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███████▉                                                                                                                                 | 58/1000 [03:45<1:01:44,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 58/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████████                                                                                                                                 | 59/1000 [03:49<1:01:43,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 59/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9579, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9876, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.973365783691406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████████▏                                                                                                                                | 60/1000 [03:53<1:01:33,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 60/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████████▎                                                                                                                                | 61/1000 [03:57<1:01:28,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 61/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9753, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9716, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.973868370056152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████████▍                                                                                                                                | 62/1000 [04:01<1:01:17,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 62/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████████▋                                                                                                                                | 63/1000 [04:05<1:01:30,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 63/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9769, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9710, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.974177837371826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████████▊                                                                                                                                | 64/1000 [04:09<1:01:16,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 64/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████████▉                                                                                                                                | 65/1000 [04:13<1:01:08,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 65/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9547, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9937, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.974303722381592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████████                                                                                                                                | 66/1000 [04:17<1:01:01,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 66/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████████▏                                                                                                                               | 67/1000 [04:21<1:01:06,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 67/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9671, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9816, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.97431182861328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████████▎                                                                                                                               | 68/1000 [04:25<1:01:03,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 68/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████████▍                                                                                                                               | 69/1000 [04:29<1:01:04,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 69/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9550, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9937, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.974201679229736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████████▌                                                                                                                               | 70/1000 [04:33<1:00:56,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 70/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████████▋                                                                                                                               | 71/1000 [04:37<1:00:55,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 71/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9837, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9646, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.974027633666992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████████▊                                                                                                                               | 72/1000 [04:40<1:00:42,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 72/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████████                                                                                                                               | 73/1000 [04:44<1:00:33,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 73/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9723, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9756, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.973753452301025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██████████▏                                                                                                                              | 74/1000 [04:48<1:00:41,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 74/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████████▎                                                                                                                              | 75/1000 [04:52<1:00:34,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 75/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9784, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.97340679168701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████████▍                                                                                                                              | 76/1000 [04:56<1:00:22,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 76/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████████▌                                                                                                                              | 77/1000 [05:00<1:00:17,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 77/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9543, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9925, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.972952842712402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████████▋                                                                                                                              | 78/1000 [05:04<1:00:23,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 78/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████████▊                                                                                                                              | 79/1000 [05:08<1:00:24,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 79/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9504, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9954, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.972434520721436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████████▉                                                                                                                              | 80/1000 [05:12<1:00:26,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 80/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████████                                                                                                                              | 81/1000 [05:16<1:00:17,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 81/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9650, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9797, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.971867561340332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████████▏                                                                                                                             | 82/1000 [05:20<1:00:05,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 82/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████████▎                                                                                                                             | 83/1000 [05:24<1:00:05,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 83/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9495, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9942, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.971213817596436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████████▋                                                                                                                               | 84/1000 [05:28<59:50,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 84/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████████▊                                                                                                                               | 85/1000 [05:31<59:42,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 85/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9499, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9924, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.970499992370605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████████▉                                                                                                                               | 86/1000 [05:35<59:37,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 86/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████████                                                                                                                               | 87/1000 [05:39<59:37,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 87/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9557, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9851, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.96972417831421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████████▏                                                                                                                              | 88/1000 [05:43<59:28,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 88/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████████▎                                                                                                                              | 89/1000 [05:47<59:29,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 89/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9493, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9900, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.968864917755127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████████▌                                                                                                                              | 90/1000 [05:51<59:24,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 90/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████████▋                                                                                                                              | 91/1000 [05:55<59:21,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 91/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9534, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9841, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.967944145202637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████████▊                                                                                                                              | 92/1000 [05:59<59:09,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 92/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|████████████▉                                                                                                                              | 93/1000 [06:03<59:13,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 93/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9527, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9829, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.96693706512451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|█████████████                                                                                                                              | 94/1000 [06:07<58:58,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 94/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▏                                                                                                                             | 95/1000 [06:11<58:57,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 95/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9687, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9646, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.96587324142456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▎                                                                                                                             | 96/1000 [06:14<58:47,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 96/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▍                                                                                                                             | 97/1000 [06:18<58:46,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 97/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9403, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9913, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.964651107788086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▌                                                                                                                             | 98/1000 [06:22<58:43,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 98/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▊                                                                                                                             | 99/1000 [06:26<58:42,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 99/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9465, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9825, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.963340282440186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▊                                                                                                                            | 100/1000 [06:30<58:32,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 100/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████▉                                                                                                                            | 101/1000 [06:34<58:32,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 101/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9634, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9626, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.961952686309814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████                                                                                                                            | 102/1000 [06:38<58:23,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 102/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▏                                                                                                                           | 103/1000 [06:42<58:16,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 103/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9369, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9868, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.960362434387207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▎                                                                                                                           | 104/1000 [06:46<58:09,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 104/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|██████████████▍                                                                                                                           | 105/1000 [06:50<58:15,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 105/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9334, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9871, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.958635330200195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|██████████████▋                                                                                                                           | 106/1000 [06:53<58:01,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 106/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|██████████████▊                                                                                                                           | 107/1000 [06:57<58:02,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 107/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9275, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9895, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.95674228668213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|██████████████▉                                                                                                                           | 108/1000 [07:01<58:05,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 108/1000 result win\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|███████████████                                                                                                                           | 109/1000 [07:05<58:03,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round : 109/1000 result win\n",
      "start training\n",
      "train loss: tensor(31.9226, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "train loss: tensor(11.9908, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "val loss 21.954666137695312\n"
     ]
    }
   ],
   "source": [
    "# A few matches against a random player\n",
    "max_game_len = 100\n",
    "n_matches = 1000\n",
    "n_wins, n_draws, n_losses = 0, 0, 0\n",
    "is_show_game = False\n",
    "train_batch_size = 16\n",
    "device =  \"cuda\"\n",
    "offset_seed = 1000\n",
    "model = model.to(device)\n",
    "train_seqs = []\n",
    "\n",
    "for i in tqdm(range(n_matches)):\n",
    "    if is_show_game:\n",
    "        print('game', i)\n",
    "    ch = Checkers()\n",
    "    black_player = MinimaxPlayer(\n",
    "        'black',\n",
    "        # value_func=partial(first_order_adv, 'black', 200, 100, 20, 0),\n",
    "        value_func=partial(first_order_adv, 'black', 86.0315, 54.568, 87.21072, 25.85066),        \n",
    "        # The provided legal moves might be ordered differently\n",
    "        rollout_order_gen=lambda x: sorted(x),\n",
    "        search_depth=2,\n",
    "        seed=i+offset_seed)\n",
    "\n",
    "    white_player = MinimaxPlayer(\n",
    "                    'white',\n",
    "                    # value_func=partial(first_order_adv, 'black', 200, 100, 20, 0),\n",
    "                    value_func=partial(first_order_adv, 'white', 86.0315, 54.568, 87.21072, 25.85066),        \n",
    "                    # The provided legal moves might be ordered differently\n",
    "                    rollout_order_gen=lambda x: sorted(x),\n",
    "                    search_depth=4,\n",
    "                    seed=i+offset_seed)\n",
    "\n",
    "    #modify this function to put our RL model as white\n",
    "    winner = play_a_game(ch, black_player.next_move, white_player.next_move, max_game_len,is_show_detail = is_show_game)\n",
    "\n",
    "    # Play with a minimax player\n",
    "    # play_a_game(ch, keyboard_player_move, white_player.next_move)\n",
    "    if is_show_game:\n",
    "        print('black player evaluated %i positions in %.2fs (avg %.2f positions/s) effective branching factor %.2f' % (black_player.n_evaluated_positions, black_player.evaluation_dt, black_player.n_evaluated_positions / black_player.evaluation_dt, (black_player.n_evaluated_positions / black_player.ply) ** (1 / black_player.search_depth)))\n",
    "        print('black player pruned', black_player.prunes.items())\n",
    "        print()\n",
    "    result:RESULT_TYPE\n",
    "    if winner == 'black':\n",
    "        n_wins += 1\n",
    "        result = RESULT_TYPE.LOSE\n",
    "    elif winner is None:\n",
    "        n_draws += 1\n",
    "        result = RESULT_TYPE.DRAW\n",
    "    else:\n",
    "        n_losses += 1\n",
    "        result = RESULT_TYPE.WIN\n",
    "\n",
    "    print(f\"round : {i+1}/{n_matches} result {result.value}\")\n",
    "\n",
    "    # training \n",
    "    if result.value != \"win\": continue # dont learn if current episode is not winning\n",
    "\n",
    "    train_seqs.extend(white_player.board_move_dict)\n",
    "\n",
    "    if i % 2 != 0: continue\n",
    "        \n",
    "    print(\"start training\")\n",
    "    encoded_boards = []\n",
    "    encoded_actions = []\n",
    "    for board, action in train_seqs:\n",
    "        encoded_boards.append(encode_board_to_feature_map(board))\n",
    "        encoded_actions.append(encode_action(action))\n",
    "\n",
    "    encoded_boards = torch.stack(encoded_boards)\n",
    "    encoded_actions = torch.stack(encoded_actions)\n",
    "\n",
    "    dataset = TensorDataset(encoded_boards[:encoded_boards.shape[0]//2], encoded_actions[:encoded_boards.shape[0]//2])\n",
    "    val_dataset = TensorDataset(encoded_boards[encoded_boards.shape[0]//2:], encoded_actions[encoded_boards.shape[0]//2:])\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size = train_batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size = train_batch_size)\n",
    "\n",
    "    # train loop\n",
    "    for encoded_board, encoded_action in train_dataloader:\n",
    "        \n",
    "        encoded_board = encoded_board.to(device)\n",
    "        encoded_action = encoded_action.to(device)\n",
    "\n",
    "        encoded_board = encoded_board.flatten(start_dim = 1)\n",
    "        encoded_action = encoded_action.flatten(start_dim = 1)\n",
    "        \n",
    "        predicted_action = model(encoded_board)\n",
    "\n",
    "        loss = loss_fn(predicted_action, encoded_action)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"train loss:\", loss)\n",
    "\n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        count = 0\n",
    "        for encoded_board, encoded_action in val_dataloader:\n",
    "                \n",
    "                encoded_board = encoded_board.to(device)\n",
    "                encoded_action = encoded_action.to(device)\n",
    "        \n",
    "                encoded_board = encoded_board.flatten(start_dim = 1)\n",
    "                encoded_action = encoded_action.flatten(start_dim = 1)\n",
    "                \n",
    "                predicted_action = model(encoded_board)\n",
    "        \n",
    "                loss = loss_fn(predicted_action, encoded_action)\n",
    "            \n",
    "                val_loss += loss.item()\n",
    "                count += 1\n",
    "        print(\"val loss\", val_loss/count)\n",
    "\n",
    "    train_seqs = []\n",
    "\n",
    "print('black win', n_wins, 'draw', n_draws, 'loss', n_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), open(\"./modelDenseSomthing_latest_2.pt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dic = torch.load(\"/home/boat/pattern/pattern_term_project_2024/boat_weight/densenet/modelDenseSomthing_latest_msesum.pt\")\n",
    "dic = torch.load(\"/home/boat/pattern/pattern_term_project_2024/boat_weight/densenet/modelDenseSomthing_latest_msesum_depth4vs2_randRollout.pt\")\n",
    "model.load_state_dict(dic[\"model\"])\n",
    "device = \"cuda\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9990"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic[\"iteration\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkers.agents import Player\n",
    "\n",
    "# A random player\n",
    "class DenseNetPlayer(Player):\n",
    "    \"\"\"A player that makes random legal moves.\"\"\"\n",
    "    def __init__(self, color, model, device):\n",
    "        super().__init__(color=color)\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.color = color\n",
    "        \n",
    "    def next_move(self, board, last_moved_piece):\n",
    "        state = (board, self.color, last_moved_piece)\n",
    "        self.simulator.restore_state(state)\n",
    "        legal_moves = self.simulator.legal_moves()\n",
    "        # print(\"legal_moves\", legal_moves)\n",
    "\n",
    "        # inference here\n",
    "        encoded_board = encode_board_to_feature_map(board)\n",
    "        encoded_board = encoded_board.flatten(start_dim=0)\n",
    "        encoded_board = encoded_board[None, :].to(self.device)\n",
    "\n",
    "        predict = model(encoded_board)[0]\n",
    "\n",
    "        action_map = predict.reshape((2,8,4)).cpu().detach().numpy()\n",
    "        \n",
    "        max_pos_a = np.unravel_index(np.argmax(action_map[0,:,:]), (8,4))\n",
    "        max_pos_b = np.unravel_index(np.argmax(action_map[1,:,:]), (8,4))\n",
    "\n",
    "        pred_move = ((max_pos_a[0] * 4) + max_pos_a[1], (max_pos_b[0] * 4) + max_pos_b[1])\n",
    "        # print(action_map, max_pos_a, max_pos_b)\n",
    "        # print(\"pred_move\",pred_move)\n",
    "\n",
    "        # print(legal_moves)\n",
    "\n",
    "        # if prediction is legal\n",
    "        p = pred_move\n",
    "        for legal_move in legal_moves:\n",
    "            l = legal_move\n",
    "            if p[0] == l[0] and p[1] == l[1]:\n",
    "                print(\"choose predict move\", pred_move)\n",
    "                return pred_move\n",
    "\n",
    "        # the predict is not legal but predict target position is legal\n",
    "        for legal_move in legal_moves:\n",
    "            l = legal_move\n",
    "            if p[0] == l[0]:\n",
    "                print(\"choose legal move\", legal_move, \"from predict target\", pred_move)\n",
    "                return legal_move\n",
    "\n",
    "        for legal_move in legal_moves:\n",
    "            l = legal_move\n",
    "            if p[1] == l[1]:\n",
    "                print(\"choose legal move\", legal_move, \"from predict target\", pred_move)\n",
    "                return legal_move\n",
    "\n",
    "        # else: prediction is trash. random\n",
    "        move = self.random.choice(np.asarray(legal_moves, dtype=\"int,int\"))\n",
    "        print(\"random\", move)\n",
    "        \n",
    "        return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few matches against a random player\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "max_game_len = 100\n",
    "n_matches = 100\n",
    "n_wins, n_draws, n_losses = 0, 0, 0\n",
    "is_show_game = False\n",
    "explore_rate=0.1\n",
    "rand = 1203\n",
    "\n",
    "def rollout_order_gen_random(x):\n",
    "    random.shuffle(x)\n",
    "    return x\n",
    "    \n",
    "for i in tqdm(range(n_matches)):\n",
    "    if is_show_game:\n",
    "        print('game', i)\n",
    "    ch = Checkers()\n",
    "    black_player = MinimaxPlayer(\n",
    "        'black',\n",
    "        # value_func=partial(first_order_adv, 'black', 200, 100, 20, 0),\n",
    "        value_func=partial(first_order_adv, 'black', 86.0315, 54.568, 87.21072, 25.85066),        \n",
    "        # The provided legal moves might be ordered differently\n",
    "        rollout_order_gen=rollout_order_gen_random,\n",
    "        search_depth=2,\n",
    "        seed=i+rand)\n",
    "\n",
    "    white_player = DenseNetPlayer(\"white\",model, device)\n",
    "\n",
    "    #modify this function to put our RL model as white\n",
    "    winner = play_a_game(ch, black_player.next_move, white_player.next_move, max_game_len,is_show_detail = is_show_game)\n",
    "\n",
    "    # Play with a minimax player\n",
    "    # play_a_game(ch, keyboard_player_move, white_player.next_move)\n",
    "    if is_show_game:\n",
    "        print('black player evaluated %i positions in %.2fs (avg %.2f positions/s) effective branching factor %.2f' % (black_player.n_evaluated_positions, black_player.evaluation_dt, black_player.n_evaluated_positions / black_player.evaluation_dt, (black_player.n_evaluated_positions / black_player.ply) ** (1 / black_player.search_depth)))\n",
    "        print('black player pruned', black_player.prunes.items())\n",
    "        print()\n",
    "    result:RESULT_TYPE\n",
    "    if winner == 'black':\n",
    "        n_wins += 1\n",
    "        result = RESULT_TYPE.LOSE\n",
    "    elif winner is None:\n",
    "        n_draws += 1\n",
    "        result = RESULT_TYPE.DRAW\n",
    "    else:\n",
    "        n_losses += 1\n",
    "        result = RESULT_TYPE.WIN\n",
    "\n",
    "    print(f\"round : {i+1}/{n_matches} result {result.value}, explore_rate {explore_rate}\")\n",
    "\n",
    "print('black win', n_wins, 'draw', n_draws, 'loss', n_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepKILLme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# board configuration\n",
    "\n",
    "num_in_feat = 8*4*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white_possible_moves = []\n",
    "# for i in [\"men\",\"king\"]:\n",
    "#     for from_row in range(8):\n",
    "#         for from_col in range(4):\n",
    "#             if i == \"men\":\n",
    "#                 possible_moves.append([from_row, from_col, from_row - 1, from_col]) # เฉียงซ่าย\n",
    "#                 possible_moves.append([from_row, from_col, from_row - 1, from_col + 1]) #เฉียงขวา\n",
    "#             if i == \"king\":\n",
    "#                 # upper left\n",
    "#                 count = 1\n",
    "#                 while True:\n",
    "#                     current_row = from_row - count\n",
    "#                     current_col = from_col - count\n",
    "                    \n",
    "#                 diffX = 0\n",
    "#                 diffY = 0\n",
    "#                 while from_row + diffX\n",
    "possible_moves = {}\n",
    "inv_possible_moves = {}\n",
    "count = 0\n",
    "for from_row in range(8):\n",
    "    for from_col in range(4):\n",
    "        for to_row in range(8):\n",
    "            for to_col in range(4):\n",
    "                if from_row == to_row and from_col == to_col: continue\n",
    "                possible_moves[f\"{from_row},{from_col},{to_row},{to_col}\"] = count\n",
    "                inv_possible_moves[count] = [from_row, from_col, to_row, to_col]\n",
    "                count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepKILLme(\n",
      "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=512, bias=True)\n",
      "  (fc5): Linear(in_features=512, out_features=992, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepKILLme(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(DeepKILLme, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(in_feat, 256)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(256, 256)          # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(256, 256)          # Third fully connected layer\n",
    "        self.fc4 = nn.Linear(256, 512)           # Fourth fully connected layer\n",
    "        self.fc5 = nn.Linear(512, out_feat)  # Final output layer\n",
    "\n",
    "        # Define activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the deeper model\n",
    "model = DeepKILLme(num_in_feat, len(possible_moves))\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0,0,0,1': 0,\n",
       " '0,0,0,2': 1,\n",
       " '0,0,0,3': 2,\n",
       " '0,0,1,0': 3,\n",
       " '0,0,1,1': 4,\n",
       " '0,0,1,2': 5,\n",
       " '0,0,1,3': 6,\n",
       " '0,0,2,0': 7,\n",
       " '0,0,2,1': 8,\n",
       " '0,0,2,2': 9,\n",
       " '0,0,2,3': 10,\n",
       " '0,0,3,0': 11,\n",
       " '0,0,3,1': 12,\n",
       " '0,0,3,2': 13,\n",
       " '0,0,3,3': 14,\n",
       " '0,0,4,0': 15,\n",
       " '0,0,4,1': 16,\n",
       " '0,0,4,2': 17,\n",
       " '0,0,4,3': 18,\n",
       " '0,0,5,0': 19,\n",
       " '0,0,5,1': 20,\n",
       " '0,0,5,2': 21,\n",
       " '0,0,5,3': 22,\n",
       " '0,0,6,0': 23,\n",
       " '0,0,6,1': 24,\n",
       " '0,0,6,2': 25,\n",
       " '0,0,6,3': 26,\n",
       " '0,0,7,0': 27,\n",
       " '0,0,7,1': 28,\n",
       " '0,0,7,2': 29,\n",
       " '0,0,7,3': 30,\n",
       " '0,1,0,0': 31,\n",
       " '0,1,0,2': 32,\n",
       " '0,1,0,3': 33,\n",
       " '0,1,1,0': 34,\n",
       " '0,1,1,1': 35,\n",
       " '0,1,1,2': 36,\n",
       " '0,1,1,3': 37,\n",
       " '0,1,2,0': 38,\n",
       " '0,1,2,1': 39,\n",
       " '0,1,2,2': 40,\n",
       " '0,1,2,3': 41,\n",
       " '0,1,3,0': 42,\n",
       " '0,1,3,1': 43,\n",
       " '0,1,3,2': 44,\n",
       " '0,1,3,3': 45,\n",
       " '0,1,4,0': 46,\n",
       " '0,1,4,1': 47,\n",
       " '0,1,4,2': 48,\n",
       " '0,1,4,3': 49,\n",
       " '0,1,5,0': 50,\n",
       " '0,1,5,1': 51,\n",
       " '0,1,5,2': 52,\n",
       " '0,1,5,3': 53,\n",
       " '0,1,6,0': 54,\n",
       " '0,1,6,1': 55,\n",
       " '0,1,6,2': 56,\n",
       " '0,1,6,3': 57,\n",
       " '0,1,7,0': 58,\n",
       " '0,1,7,1': 59,\n",
       " '0,1,7,2': 60,\n",
       " '0,1,7,3': 61,\n",
       " '0,2,0,0': 62,\n",
       " '0,2,0,1': 63,\n",
       " '0,2,0,3': 64,\n",
       " '0,2,1,0': 65,\n",
       " '0,2,1,1': 66,\n",
       " '0,2,1,2': 67,\n",
       " '0,2,1,3': 68,\n",
       " '0,2,2,0': 69,\n",
       " '0,2,2,1': 70,\n",
       " '0,2,2,2': 71,\n",
       " '0,2,2,3': 72,\n",
       " '0,2,3,0': 73,\n",
       " '0,2,3,1': 74,\n",
       " '0,2,3,2': 75,\n",
       " '0,2,3,3': 76,\n",
       " '0,2,4,0': 77,\n",
       " '0,2,4,1': 78,\n",
       " '0,2,4,2': 79,\n",
       " '0,2,4,3': 80,\n",
       " '0,2,5,0': 81,\n",
       " '0,2,5,1': 82,\n",
       " '0,2,5,2': 83,\n",
       " '0,2,5,3': 84,\n",
       " '0,2,6,0': 85,\n",
       " '0,2,6,1': 86,\n",
       " '0,2,6,2': 87,\n",
       " '0,2,6,3': 88,\n",
       " '0,2,7,0': 89,\n",
       " '0,2,7,1': 90,\n",
       " '0,2,7,2': 91,\n",
       " '0,2,7,3': 92,\n",
       " '0,3,0,0': 93,\n",
       " '0,3,0,1': 94,\n",
       " '0,3,0,2': 95,\n",
       " '0,3,1,0': 96,\n",
       " '0,3,1,1': 97,\n",
       " '0,3,1,2': 98,\n",
       " '0,3,1,3': 99,\n",
       " '0,3,2,0': 100,\n",
       " '0,3,2,1': 101,\n",
       " '0,3,2,2': 102,\n",
       " '0,3,2,3': 103,\n",
       " '0,3,3,0': 104,\n",
       " '0,3,3,1': 105,\n",
       " '0,3,3,2': 106,\n",
       " '0,3,3,3': 107,\n",
       " '0,3,4,0': 108,\n",
       " '0,3,4,1': 109,\n",
       " '0,3,4,2': 110,\n",
       " '0,3,4,3': 111,\n",
       " '0,3,5,0': 112,\n",
       " '0,3,5,1': 113,\n",
       " '0,3,5,2': 114,\n",
       " '0,3,5,3': 115,\n",
       " '0,3,6,0': 116,\n",
       " '0,3,6,1': 117,\n",
       " '0,3,6,2': 118,\n",
       " '0,3,6,3': 119,\n",
       " '0,3,7,0': 120,\n",
       " '0,3,7,1': 121,\n",
       " '0,3,7,2': 122,\n",
       " '0,3,7,3': 123,\n",
       " '1,0,0,0': 124,\n",
       " '1,0,0,1': 125,\n",
       " '1,0,0,2': 126,\n",
       " '1,0,0,3': 127,\n",
       " '1,0,1,1': 128,\n",
       " '1,0,1,2': 129,\n",
       " '1,0,1,3': 130,\n",
       " '1,0,2,0': 131,\n",
       " '1,0,2,1': 132,\n",
       " '1,0,2,2': 133,\n",
       " '1,0,2,3': 134,\n",
       " '1,0,3,0': 135,\n",
       " '1,0,3,1': 136,\n",
       " '1,0,3,2': 137,\n",
       " '1,0,3,3': 138,\n",
       " '1,0,4,0': 139,\n",
       " '1,0,4,1': 140,\n",
       " '1,0,4,2': 141,\n",
       " '1,0,4,3': 142,\n",
       " '1,0,5,0': 143,\n",
       " '1,0,5,1': 144,\n",
       " '1,0,5,2': 145,\n",
       " '1,0,5,3': 146,\n",
       " '1,0,6,0': 147,\n",
       " '1,0,6,1': 148,\n",
       " '1,0,6,2': 149,\n",
       " '1,0,6,3': 150,\n",
       " '1,0,7,0': 151,\n",
       " '1,0,7,1': 152,\n",
       " '1,0,7,2': 153,\n",
       " '1,0,7,3': 154,\n",
       " '1,1,0,0': 155,\n",
       " '1,1,0,1': 156,\n",
       " '1,1,0,2': 157,\n",
       " '1,1,0,3': 158,\n",
       " '1,1,1,0': 159,\n",
       " '1,1,1,2': 160,\n",
       " '1,1,1,3': 161,\n",
       " '1,1,2,0': 162,\n",
       " '1,1,2,1': 163,\n",
       " '1,1,2,2': 164,\n",
       " '1,1,2,3': 165,\n",
       " '1,1,3,0': 166,\n",
       " '1,1,3,1': 167,\n",
       " '1,1,3,2': 168,\n",
       " '1,1,3,3': 169,\n",
       " '1,1,4,0': 170,\n",
       " '1,1,4,1': 171,\n",
       " '1,1,4,2': 172,\n",
       " '1,1,4,3': 173,\n",
       " '1,1,5,0': 174,\n",
       " '1,1,5,1': 175,\n",
       " '1,1,5,2': 176,\n",
       " '1,1,5,3': 177,\n",
       " '1,1,6,0': 178,\n",
       " '1,1,6,1': 179,\n",
       " '1,1,6,2': 180,\n",
       " '1,1,6,3': 181,\n",
       " '1,1,7,0': 182,\n",
       " '1,1,7,1': 183,\n",
       " '1,1,7,2': 184,\n",
       " '1,1,7,3': 185,\n",
       " '1,2,0,0': 186,\n",
       " '1,2,0,1': 187,\n",
       " '1,2,0,2': 188,\n",
       " '1,2,0,3': 189,\n",
       " '1,2,1,0': 190,\n",
       " '1,2,1,1': 191,\n",
       " '1,2,1,3': 192,\n",
       " '1,2,2,0': 193,\n",
       " '1,2,2,1': 194,\n",
       " '1,2,2,2': 195,\n",
       " '1,2,2,3': 196,\n",
       " '1,2,3,0': 197,\n",
       " '1,2,3,1': 198,\n",
       " '1,2,3,2': 199,\n",
       " '1,2,3,3': 200,\n",
       " '1,2,4,0': 201,\n",
       " '1,2,4,1': 202,\n",
       " '1,2,4,2': 203,\n",
       " '1,2,4,3': 204,\n",
       " '1,2,5,0': 205,\n",
       " '1,2,5,1': 206,\n",
       " '1,2,5,2': 207,\n",
       " '1,2,5,3': 208,\n",
       " '1,2,6,0': 209,\n",
       " '1,2,6,1': 210,\n",
       " '1,2,6,2': 211,\n",
       " '1,2,6,3': 212,\n",
       " '1,2,7,0': 213,\n",
       " '1,2,7,1': 214,\n",
       " '1,2,7,2': 215,\n",
       " '1,2,7,3': 216,\n",
       " '1,3,0,0': 217,\n",
       " '1,3,0,1': 218,\n",
       " '1,3,0,2': 219,\n",
       " '1,3,0,3': 220,\n",
       " '1,3,1,0': 221,\n",
       " '1,3,1,1': 222,\n",
       " '1,3,1,2': 223,\n",
       " '1,3,2,0': 224,\n",
       " '1,3,2,1': 225,\n",
       " '1,3,2,2': 226,\n",
       " '1,3,2,3': 227,\n",
       " '1,3,3,0': 228,\n",
       " '1,3,3,1': 229,\n",
       " '1,3,3,2': 230,\n",
       " '1,3,3,3': 231,\n",
       " '1,3,4,0': 232,\n",
       " '1,3,4,1': 233,\n",
       " '1,3,4,2': 234,\n",
       " '1,3,4,3': 235,\n",
       " '1,3,5,0': 236,\n",
       " '1,3,5,1': 237,\n",
       " '1,3,5,2': 238,\n",
       " '1,3,5,3': 239,\n",
       " '1,3,6,0': 240,\n",
       " '1,3,6,1': 241,\n",
       " '1,3,6,2': 242,\n",
       " '1,3,6,3': 243,\n",
       " '1,3,7,0': 244,\n",
       " '1,3,7,1': 245,\n",
       " '1,3,7,2': 246,\n",
       " '1,3,7,3': 247,\n",
       " '2,0,0,0': 248,\n",
       " '2,0,0,1': 249,\n",
       " '2,0,0,2': 250,\n",
       " '2,0,0,3': 251,\n",
       " '2,0,1,0': 252,\n",
       " '2,0,1,1': 253,\n",
       " '2,0,1,2': 254,\n",
       " '2,0,1,3': 255,\n",
       " '2,0,2,1': 256,\n",
       " '2,0,2,2': 257,\n",
       " '2,0,2,3': 258,\n",
       " '2,0,3,0': 259,\n",
       " '2,0,3,1': 260,\n",
       " '2,0,3,2': 261,\n",
       " '2,0,3,3': 262,\n",
       " '2,0,4,0': 263,\n",
       " '2,0,4,1': 264,\n",
       " '2,0,4,2': 265,\n",
       " '2,0,4,3': 266,\n",
       " '2,0,5,0': 267,\n",
       " '2,0,5,1': 268,\n",
       " '2,0,5,2': 269,\n",
       " '2,0,5,3': 270,\n",
       " '2,0,6,0': 271,\n",
       " '2,0,6,1': 272,\n",
       " '2,0,6,2': 273,\n",
       " '2,0,6,3': 274,\n",
       " '2,0,7,0': 275,\n",
       " '2,0,7,1': 276,\n",
       " '2,0,7,2': 277,\n",
       " '2,0,7,3': 278,\n",
       " '2,1,0,0': 279,\n",
       " '2,1,0,1': 280,\n",
       " '2,1,0,2': 281,\n",
       " '2,1,0,3': 282,\n",
       " '2,1,1,0': 283,\n",
       " '2,1,1,1': 284,\n",
       " '2,1,1,2': 285,\n",
       " '2,1,1,3': 286,\n",
       " '2,1,2,0': 287,\n",
       " '2,1,2,2': 288,\n",
       " '2,1,2,3': 289,\n",
       " '2,1,3,0': 290,\n",
       " '2,1,3,1': 291,\n",
       " '2,1,3,2': 292,\n",
       " '2,1,3,3': 293,\n",
       " '2,1,4,0': 294,\n",
       " '2,1,4,1': 295,\n",
       " '2,1,4,2': 296,\n",
       " '2,1,4,3': 297,\n",
       " '2,1,5,0': 298,\n",
       " '2,1,5,1': 299,\n",
       " '2,1,5,2': 300,\n",
       " '2,1,5,3': 301,\n",
       " '2,1,6,0': 302,\n",
       " '2,1,6,1': 303,\n",
       " '2,1,6,2': 304,\n",
       " '2,1,6,3': 305,\n",
       " '2,1,7,0': 306,\n",
       " '2,1,7,1': 307,\n",
       " '2,1,7,2': 308,\n",
       " '2,1,7,3': 309,\n",
       " '2,2,0,0': 310,\n",
       " '2,2,0,1': 311,\n",
       " '2,2,0,2': 312,\n",
       " '2,2,0,3': 313,\n",
       " '2,2,1,0': 314,\n",
       " '2,2,1,1': 315,\n",
       " '2,2,1,2': 316,\n",
       " '2,2,1,3': 317,\n",
       " '2,2,2,0': 318,\n",
       " '2,2,2,1': 319,\n",
       " '2,2,2,3': 320,\n",
       " '2,2,3,0': 321,\n",
       " '2,2,3,1': 322,\n",
       " '2,2,3,2': 323,\n",
       " '2,2,3,3': 324,\n",
       " '2,2,4,0': 325,\n",
       " '2,2,4,1': 326,\n",
       " '2,2,4,2': 327,\n",
       " '2,2,4,3': 328,\n",
       " '2,2,5,0': 329,\n",
       " '2,2,5,1': 330,\n",
       " '2,2,5,2': 331,\n",
       " '2,2,5,3': 332,\n",
       " '2,2,6,0': 333,\n",
       " '2,2,6,1': 334,\n",
       " '2,2,6,2': 335,\n",
       " '2,2,6,3': 336,\n",
       " '2,2,7,0': 337,\n",
       " '2,2,7,1': 338,\n",
       " '2,2,7,2': 339,\n",
       " '2,2,7,3': 340,\n",
       " '2,3,0,0': 341,\n",
       " '2,3,0,1': 342,\n",
       " '2,3,0,2': 343,\n",
       " '2,3,0,3': 344,\n",
       " '2,3,1,0': 345,\n",
       " '2,3,1,1': 346,\n",
       " '2,3,1,2': 347,\n",
       " '2,3,1,3': 348,\n",
       " '2,3,2,0': 349,\n",
       " '2,3,2,1': 350,\n",
       " '2,3,2,2': 351,\n",
       " '2,3,3,0': 352,\n",
       " '2,3,3,1': 353,\n",
       " '2,3,3,2': 354,\n",
       " '2,3,3,3': 355,\n",
       " '2,3,4,0': 356,\n",
       " '2,3,4,1': 357,\n",
       " '2,3,4,2': 358,\n",
       " '2,3,4,3': 359,\n",
       " '2,3,5,0': 360,\n",
       " '2,3,5,1': 361,\n",
       " '2,3,5,2': 362,\n",
       " '2,3,5,3': 363,\n",
       " '2,3,6,0': 364,\n",
       " '2,3,6,1': 365,\n",
       " '2,3,6,2': 366,\n",
       " '2,3,6,3': 367,\n",
       " '2,3,7,0': 368,\n",
       " '2,3,7,1': 369,\n",
       " '2,3,7,2': 370,\n",
       " '2,3,7,3': 371,\n",
       " '3,0,0,0': 372,\n",
       " '3,0,0,1': 373,\n",
       " '3,0,0,2': 374,\n",
       " '3,0,0,3': 375,\n",
       " '3,0,1,0': 376,\n",
       " '3,0,1,1': 377,\n",
       " '3,0,1,2': 378,\n",
       " '3,0,1,3': 379,\n",
       " '3,0,2,0': 380,\n",
       " '3,0,2,1': 381,\n",
       " '3,0,2,2': 382,\n",
       " '3,0,2,3': 383,\n",
       " '3,0,3,1': 384,\n",
       " '3,0,3,2': 385,\n",
       " '3,0,3,3': 386,\n",
       " '3,0,4,0': 387,\n",
       " '3,0,4,1': 388,\n",
       " '3,0,4,2': 389,\n",
       " '3,0,4,3': 390,\n",
       " '3,0,5,0': 391,\n",
       " '3,0,5,1': 392,\n",
       " '3,0,5,2': 393,\n",
       " '3,0,5,3': 394,\n",
       " '3,0,6,0': 395,\n",
       " '3,0,6,1': 396,\n",
       " '3,0,6,2': 397,\n",
       " '3,0,6,3': 398,\n",
       " '3,0,7,0': 399,\n",
       " '3,0,7,1': 400,\n",
       " '3,0,7,2': 401,\n",
       " '3,0,7,3': 402,\n",
       " '3,1,0,0': 403,\n",
       " '3,1,0,1': 404,\n",
       " '3,1,0,2': 405,\n",
       " '3,1,0,3': 406,\n",
       " '3,1,1,0': 407,\n",
       " '3,1,1,1': 408,\n",
       " '3,1,1,2': 409,\n",
       " '3,1,1,3': 410,\n",
       " '3,1,2,0': 411,\n",
       " '3,1,2,1': 412,\n",
       " '3,1,2,2': 413,\n",
       " '3,1,2,3': 414,\n",
       " '3,1,3,0': 415,\n",
       " '3,1,3,2': 416,\n",
       " '3,1,3,3': 417,\n",
       " '3,1,4,0': 418,\n",
       " '3,1,4,1': 419,\n",
       " '3,1,4,2': 420,\n",
       " '3,1,4,3': 421,\n",
       " '3,1,5,0': 422,\n",
       " '3,1,5,1': 423,\n",
       " '3,1,5,2': 424,\n",
       " '3,1,5,3': 425,\n",
       " '3,1,6,0': 426,\n",
       " '3,1,6,1': 427,\n",
       " '3,1,6,2': 428,\n",
       " '3,1,6,3': 429,\n",
       " '3,1,7,0': 430,\n",
       " '3,1,7,1': 431,\n",
       " '3,1,7,2': 432,\n",
       " '3,1,7,3': 433,\n",
       " '3,2,0,0': 434,\n",
       " '3,2,0,1': 435,\n",
       " '3,2,0,2': 436,\n",
       " '3,2,0,3': 437,\n",
       " '3,2,1,0': 438,\n",
       " '3,2,1,1': 439,\n",
       " '3,2,1,2': 440,\n",
       " '3,2,1,3': 441,\n",
       " '3,2,2,0': 442,\n",
       " '3,2,2,1': 443,\n",
       " '3,2,2,2': 444,\n",
       " '3,2,2,3': 445,\n",
       " '3,2,3,0': 446,\n",
       " '3,2,3,1': 447,\n",
       " '3,2,3,3': 448,\n",
       " '3,2,4,0': 449,\n",
       " '3,2,4,1': 450,\n",
       " '3,2,4,2': 451,\n",
       " '3,2,4,3': 452,\n",
       " '3,2,5,0': 453,\n",
       " '3,2,5,1': 454,\n",
       " '3,2,5,2': 455,\n",
       " '3,2,5,3': 456,\n",
       " '3,2,6,0': 457,\n",
       " '3,2,6,1': 458,\n",
       " '3,2,6,2': 459,\n",
       " '3,2,6,3': 460,\n",
       " '3,2,7,0': 461,\n",
       " '3,2,7,1': 462,\n",
       " '3,2,7,2': 463,\n",
       " '3,2,7,3': 464,\n",
       " '3,3,0,0': 465,\n",
       " '3,3,0,1': 466,\n",
       " '3,3,0,2': 467,\n",
       " '3,3,0,3': 468,\n",
       " '3,3,1,0': 469,\n",
       " '3,3,1,1': 470,\n",
       " '3,3,1,2': 471,\n",
       " '3,3,1,3': 472,\n",
       " '3,3,2,0': 473,\n",
       " '3,3,2,1': 474,\n",
       " '3,3,2,2': 475,\n",
       " '3,3,2,3': 476,\n",
       " '3,3,3,0': 477,\n",
       " '3,3,3,1': 478,\n",
       " '3,3,3,2': 479,\n",
       " '3,3,4,0': 480,\n",
       " '3,3,4,1': 481,\n",
       " '3,3,4,2': 482,\n",
       " '3,3,4,3': 483,\n",
       " '3,3,5,0': 484,\n",
       " '3,3,5,1': 485,\n",
       " '3,3,5,2': 486,\n",
       " '3,3,5,3': 487,\n",
       " '3,3,6,0': 488,\n",
       " '3,3,6,1': 489,\n",
       " '3,3,6,2': 490,\n",
       " '3,3,6,3': 491,\n",
       " '3,3,7,0': 492,\n",
       " '3,3,7,1': 493,\n",
       " '3,3,7,2': 494,\n",
       " '3,3,7,3': 495,\n",
       " '4,0,0,0': 496,\n",
       " '4,0,0,1': 497,\n",
       " '4,0,0,2': 498,\n",
       " '4,0,0,3': 499,\n",
       " '4,0,1,0': 500,\n",
       " '4,0,1,1': 501,\n",
       " '4,0,1,2': 502,\n",
       " '4,0,1,3': 503,\n",
       " '4,0,2,0': 504,\n",
       " '4,0,2,1': 505,\n",
       " '4,0,2,2': 506,\n",
       " '4,0,2,3': 507,\n",
       " '4,0,3,0': 508,\n",
       " '4,0,3,1': 509,\n",
       " '4,0,3,2': 510,\n",
       " '4,0,3,3': 511,\n",
       " '4,0,4,1': 512,\n",
       " '4,0,4,2': 513,\n",
       " '4,0,4,3': 514,\n",
       " '4,0,5,0': 515,\n",
       " '4,0,5,1': 516,\n",
       " '4,0,5,2': 517,\n",
       " '4,0,5,3': 518,\n",
       " '4,0,6,0': 519,\n",
       " '4,0,6,1': 520,\n",
       " '4,0,6,2': 521,\n",
       " '4,0,6,3': 522,\n",
       " '4,0,7,0': 523,\n",
       " '4,0,7,1': 524,\n",
       " '4,0,7,2': 525,\n",
       " '4,0,7,3': 526,\n",
       " '4,1,0,0': 527,\n",
       " '4,1,0,1': 528,\n",
       " '4,1,0,2': 529,\n",
       " '4,1,0,3': 530,\n",
       " '4,1,1,0': 531,\n",
       " '4,1,1,1': 532,\n",
       " '4,1,1,2': 533,\n",
       " '4,1,1,3': 534,\n",
       " '4,1,2,0': 535,\n",
       " '4,1,2,1': 536,\n",
       " '4,1,2,2': 537,\n",
       " '4,1,2,3': 538,\n",
       " '4,1,3,0': 539,\n",
       " '4,1,3,1': 540,\n",
       " '4,1,3,2': 541,\n",
       " '4,1,3,3': 542,\n",
       " '4,1,4,0': 543,\n",
       " '4,1,4,2': 544,\n",
       " '4,1,4,3': 545,\n",
       " '4,1,5,0': 546,\n",
       " '4,1,5,1': 547,\n",
       " '4,1,5,2': 548,\n",
       " '4,1,5,3': 549,\n",
       " '4,1,6,0': 550,\n",
       " '4,1,6,1': 551,\n",
       " '4,1,6,2': 552,\n",
       " '4,1,6,3': 553,\n",
       " '4,1,7,0': 554,\n",
       " '4,1,7,1': 555,\n",
       " '4,1,7,2': 556,\n",
       " '4,1,7,3': 557,\n",
       " '4,2,0,0': 558,\n",
       " '4,2,0,1': 559,\n",
       " '4,2,0,2': 560,\n",
       " '4,2,0,3': 561,\n",
       " '4,2,1,0': 562,\n",
       " '4,2,1,1': 563,\n",
       " '4,2,1,2': 564,\n",
       " '4,2,1,3': 565,\n",
       " '4,2,2,0': 566,\n",
       " '4,2,2,1': 567,\n",
       " '4,2,2,2': 568,\n",
       " '4,2,2,3': 569,\n",
       " '4,2,3,0': 570,\n",
       " '4,2,3,1': 571,\n",
       " '4,2,3,2': 572,\n",
       " '4,2,3,3': 573,\n",
       " '4,2,4,0': 574,\n",
       " '4,2,4,1': 575,\n",
       " '4,2,4,3': 576,\n",
       " '4,2,5,0': 577,\n",
       " '4,2,5,1': 578,\n",
       " '4,2,5,2': 579,\n",
       " '4,2,5,3': 580,\n",
       " '4,2,6,0': 581,\n",
       " '4,2,6,1': 582,\n",
       " '4,2,6,2': 583,\n",
       " '4,2,6,3': 584,\n",
       " '4,2,7,0': 585,\n",
       " '4,2,7,1': 586,\n",
       " '4,2,7,2': 587,\n",
       " '4,2,7,3': 588,\n",
       " '4,3,0,0': 589,\n",
       " '4,3,0,1': 590,\n",
       " '4,3,0,2': 591,\n",
       " '4,3,0,3': 592,\n",
       " '4,3,1,0': 593,\n",
       " '4,3,1,1': 594,\n",
       " '4,3,1,2': 595,\n",
       " '4,3,1,3': 596,\n",
       " '4,3,2,0': 597,\n",
       " '4,3,2,1': 598,\n",
       " '4,3,2,2': 599,\n",
       " '4,3,2,3': 600,\n",
       " '4,3,3,0': 601,\n",
       " '4,3,3,1': 602,\n",
       " '4,3,3,2': 603,\n",
       " '4,3,3,3': 604,\n",
       " '4,3,4,0': 605,\n",
       " '4,3,4,1': 606,\n",
       " '4,3,4,2': 607,\n",
       " '4,3,5,0': 608,\n",
       " '4,3,5,1': 609,\n",
       " '4,3,5,2': 610,\n",
       " '4,3,5,3': 611,\n",
       " '4,3,6,0': 612,\n",
       " '4,3,6,1': 613,\n",
       " '4,3,6,2': 614,\n",
       " '4,3,6,3': 615,\n",
       " '4,3,7,0': 616,\n",
       " '4,3,7,1': 617,\n",
       " '4,3,7,2': 618,\n",
       " '4,3,7,3': 619,\n",
       " '5,0,0,0': 620,\n",
       " '5,0,0,1': 621,\n",
       " '5,0,0,2': 622,\n",
       " '5,0,0,3': 623,\n",
       " '5,0,1,0': 624,\n",
       " '5,0,1,1': 625,\n",
       " '5,0,1,2': 626,\n",
       " '5,0,1,3': 627,\n",
       " '5,0,2,0': 628,\n",
       " '5,0,2,1': 629,\n",
       " '5,0,2,2': 630,\n",
       " '5,0,2,3': 631,\n",
       " '5,0,3,0': 632,\n",
       " '5,0,3,1': 633,\n",
       " '5,0,3,2': 634,\n",
       " '5,0,3,3': 635,\n",
       " '5,0,4,0': 636,\n",
       " '5,0,4,1': 637,\n",
       " '5,0,4,2': 638,\n",
       " '5,0,4,3': 639,\n",
       " '5,0,5,1': 640,\n",
       " '5,0,5,2': 641,\n",
       " '5,0,5,3': 642,\n",
       " '5,0,6,0': 643,\n",
       " '5,0,6,1': 644,\n",
       " '5,0,6,2': 645,\n",
       " '5,0,6,3': 646,\n",
       " '5,0,7,0': 647,\n",
       " '5,0,7,1': 648,\n",
       " '5,0,7,2': 649,\n",
       " '5,0,7,3': 650,\n",
       " '5,1,0,0': 651,\n",
       " '5,1,0,1': 652,\n",
       " '5,1,0,2': 653,\n",
       " '5,1,0,3': 654,\n",
       " '5,1,1,0': 655,\n",
       " '5,1,1,1': 656,\n",
       " '5,1,1,2': 657,\n",
       " '5,1,1,3': 658,\n",
       " '5,1,2,0': 659,\n",
       " '5,1,2,1': 660,\n",
       " '5,1,2,2': 661,\n",
       " '5,1,2,3': 662,\n",
       " '5,1,3,0': 663,\n",
       " '5,1,3,1': 664,\n",
       " '5,1,3,2': 665,\n",
       " '5,1,3,3': 666,\n",
       " '5,1,4,0': 667,\n",
       " '5,1,4,1': 668,\n",
       " '5,1,4,2': 669,\n",
       " '5,1,4,3': 670,\n",
       " '5,1,5,0': 671,\n",
       " '5,1,5,2': 672,\n",
       " '5,1,5,3': 673,\n",
       " '5,1,6,0': 674,\n",
       " '5,1,6,1': 675,\n",
       " '5,1,6,2': 676,\n",
       " '5,1,6,3': 677,\n",
       " '5,1,7,0': 678,\n",
       " '5,1,7,1': 679,\n",
       " '5,1,7,2': 680,\n",
       " '5,1,7,3': 681,\n",
       " '5,2,0,0': 682,\n",
       " '5,2,0,1': 683,\n",
       " '5,2,0,2': 684,\n",
       " '5,2,0,3': 685,\n",
       " '5,2,1,0': 686,\n",
       " '5,2,1,1': 687,\n",
       " '5,2,1,2': 688,\n",
       " '5,2,1,3': 689,\n",
       " '5,2,2,0': 690,\n",
       " '5,2,2,1': 691,\n",
       " '5,2,2,2': 692,\n",
       " '5,2,2,3': 693,\n",
       " '5,2,3,0': 694,\n",
       " '5,2,3,1': 695,\n",
       " '5,2,3,2': 696,\n",
       " '5,2,3,3': 697,\n",
       " '5,2,4,0': 698,\n",
       " '5,2,4,1': 699,\n",
       " '5,2,4,2': 700,\n",
       " '5,2,4,3': 701,\n",
       " '5,2,5,0': 702,\n",
       " '5,2,5,1': 703,\n",
       " '5,2,5,3': 704,\n",
       " '5,2,6,0': 705,\n",
       " '5,2,6,1': 706,\n",
       " '5,2,6,2': 707,\n",
       " '5,2,6,3': 708,\n",
       " '5,2,7,0': 709,\n",
       " '5,2,7,1': 710,\n",
       " '5,2,7,2': 711,\n",
       " '5,2,7,3': 712,\n",
       " '5,3,0,0': 713,\n",
       " '5,3,0,1': 714,\n",
       " '5,3,0,2': 715,\n",
       " '5,3,0,3': 716,\n",
       " '5,3,1,0': 717,\n",
       " '5,3,1,1': 718,\n",
       " '5,3,1,2': 719,\n",
       " '5,3,1,3': 720,\n",
       " '5,3,2,0': 721,\n",
       " '5,3,2,1': 722,\n",
       " '5,3,2,2': 723,\n",
       " '5,3,2,3': 724,\n",
       " '5,3,3,0': 725,\n",
       " '5,3,3,1': 726,\n",
       " '5,3,3,2': 727,\n",
       " '5,3,3,3': 728,\n",
       " '5,3,4,0': 729,\n",
       " '5,3,4,1': 730,\n",
       " '5,3,4,2': 731,\n",
       " '5,3,4,3': 732,\n",
       " '5,3,5,0': 733,\n",
       " '5,3,5,1': 734,\n",
       " '5,3,5,2': 735,\n",
       " '5,3,6,0': 736,\n",
       " '5,3,6,1': 737,\n",
       " '5,3,6,2': 738,\n",
       " '5,3,6,3': 739,\n",
       " '5,3,7,0': 740,\n",
       " '5,3,7,1': 741,\n",
       " '5,3,7,2': 742,\n",
       " '5,3,7,3': 743,\n",
       " '6,0,0,0': 744,\n",
       " '6,0,0,1': 745,\n",
       " '6,0,0,2': 746,\n",
       " '6,0,0,3': 747,\n",
       " '6,0,1,0': 748,\n",
       " '6,0,1,1': 749,\n",
       " '6,0,1,2': 750,\n",
       " '6,0,1,3': 751,\n",
       " '6,0,2,0': 752,\n",
       " '6,0,2,1': 753,\n",
       " '6,0,2,2': 754,\n",
       " '6,0,2,3': 755,\n",
       " '6,0,3,0': 756,\n",
       " '6,0,3,1': 757,\n",
       " '6,0,3,2': 758,\n",
       " '6,0,3,3': 759,\n",
       " '6,0,4,0': 760,\n",
       " '6,0,4,1': 761,\n",
       " '6,0,4,2': 762,\n",
       " '6,0,4,3': 763,\n",
       " '6,0,5,0': 764,\n",
       " '6,0,5,1': 765,\n",
       " '6,0,5,2': 766,\n",
       " '6,0,5,3': 767,\n",
       " '6,0,6,1': 768,\n",
       " '6,0,6,2': 769,\n",
       " '6,0,6,3': 770,\n",
       " '6,0,7,0': 771,\n",
       " '6,0,7,1': 772,\n",
       " '6,0,7,2': 773,\n",
       " '6,0,7,3': 774,\n",
       " '6,1,0,0': 775,\n",
       " '6,1,0,1': 776,\n",
       " '6,1,0,2': 777,\n",
       " '6,1,0,3': 778,\n",
       " '6,1,1,0': 779,\n",
       " '6,1,1,1': 780,\n",
       " '6,1,1,2': 781,\n",
       " '6,1,1,3': 782,\n",
       " '6,1,2,0': 783,\n",
       " '6,1,2,1': 784,\n",
       " '6,1,2,2': 785,\n",
       " '6,1,2,3': 786,\n",
       " '6,1,3,0': 787,\n",
       " '6,1,3,1': 788,\n",
       " '6,1,3,2': 789,\n",
       " '6,1,3,3': 790,\n",
       " '6,1,4,0': 791,\n",
       " '6,1,4,1': 792,\n",
       " '6,1,4,2': 793,\n",
       " '6,1,4,3': 794,\n",
       " '6,1,5,0': 795,\n",
       " '6,1,5,1': 796,\n",
       " '6,1,5,2': 797,\n",
       " '6,1,5,3': 798,\n",
       " '6,1,6,0': 799,\n",
       " '6,1,6,2': 800,\n",
       " '6,1,6,3': 801,\n",
       " '6,1,7,0': 802,\n",
       " '6,1,7,1': 803,\n",
       " '6,1,7,2': 804,\n",
       " '6,1,7,3': 805,\n",
       " '6,2,0,0': 806,\n",
       " '6,2,0,1': 807,\n",
       " '6,2,0,2': 808,\n",
       " '6,2,0,3': 809,\n",
       " '6,2,1,0': 810,\n",
       " '6,2,1,1': 811,\n",
       " '6,2,1,2': 812,\n",
       " '6,2,1,3': 813,\n",
       " '6,2,2,0': 814,\n",
       " '6,2,2,1': 815,\n",
       " '6,2,2,2': 816,\n",
       " '6,2,2,3': 817,\n",
       " '6,2,3,0': 818,\n",
       " '6,2,3,1': 819,\n",
       " '6,2,3,2': 820,\n",
       " '6,2,3,3': 821,\n",
       " '6,2,4,0': 822,\n",
       " '6,2,4,1': 823,\n",
       " '6,2,4,2': 824,\n",
       " '6,2,4,3': 825,\n",
       " '6,2,5,0': 826,\n",
       " '6,2,5,1': 827,\n",
       " '6,2,5,2': 828,\n",
       " '6,2,5,3': 829,\n",
       " '6,2,6,0': 830,\n",
       " '6,2,6,1': 831,\n",
       " '6,2,6,3': 832,\n",
       " '6,2,7,0': 833,\n",
       " '6,2,7,1': 834,\n",
       " '6,2,7,2': 835,\n",
       " '6,2,7,3': 836,\n",
       " '6,3,0,0': 837,\n",
       " '6,3,0,1': 838,\n",
       " '6,3,0,2': 839,\n",
       " '6,3,0,3': 840,\n",
       " '6,3,1,0': 841,\n",
       " '6,3,1,1': 842,\n",
       " '6,3,1,2': 843,\n",
       " '6,3,1,3': 844,\n",
       " '6,3,2,0': 845,\n",
       " '6,3,2,1': 846,\n",
       " '6,3,2,2': 847,\n",
       " '6,3,2,3': 848,\n",
       " '6,3,3,0': 849,\n",
       " '6,3,3,1': 850,\n",
       " '6,3,3,2': 851,\n",
       " '6,3,3,3': 852,\n",
       " '6,3,4,0': 853,\n",
       " '6,3,4,1': 854,\n",
       " '6,3,4,2': 855,\n",
       " '6,3,4,3': 856,\n",
       " '6,3,5,0': 857,\n",
       " '6,3,5,1': 858,\n",
       " '6,3,5,2': 859,\n",
       " '6,3,5,3': 860,\n",
       " '6,3,6,0': 861,\n",
       " '6,3,6,1': 862,\n",
       " '6,3,6,2': 863,\n",
       " '6,3,7,0': 864,\n",
       " '6,3,7,1': 865,\n",
       " '6,3,7,2': 866,\n",
       " '6,3,7,3': 867,\n",
       " '7,0,0,0': 868,\n",
       " '7,0,0,1': 869,\n",
       " '7,0,0,2': 870,\n",
       " '7,0,0,3': 871,\n",
       " '7,0,1,0': 872,\n",
       " '7,0,1,1': 873,\n",
       " '7,0,1,2': 874,\n",
       " '7,0,1,3': 875,\n",
       " '7,0,2,0': 876,\n",
       " '7,0,2,1': 877,\n",
       " '7,0,2,2': 878,\n",
       " '7,0,2,3': 879,\n",
       " '7,0,3,0': 880,\n",
       " '7,0,3,1': 881,\n",
       " '7,0,3,2': 882,\n",
       " '7,0,3,3': 883,\n",
       " '7,0,4,0': 884,\n",
       " '7,0,4,1': 885,\n",
       " '7,0,4,2': 886,\n",
       " '7,0,4,3': 887,\n",
       " '7,0,5,0': 888,\n",
       " '7,0,5,1': 889,\n",
       " '7,0,5,2': 890,\n",
       " '7,0,5,3': 891,\n",
       " '7,0,6,0': 892,\n",
       " '7,0,6,1': 893,\n",
       " '7,0,6,2': 894,\n",
       " '7,0,6,3': 895,\n",
       " '7,0,7,1': 896,\n",
       " '7,0,7,2': 897,\n",
       " '7,0,7,3': 898,\n",
       " '7,1,0,0': 899,\n",
       " '7,1,0,1': 900,\n",
       " '7,1,0,2': 901,\n",
       " '7,1,0,3': 902,\n",
       " '7,1,1,0': 903,\n",
       " '7,1,1,1': 904,\n",
       " '7,1,1,2': 905,\n",
       " '7,1,1,3': 906,\n",
       " '7,1,2,0': 907,\n",
       " '7,1,2,1': 908,\n",
       " '7,1,2,2': 909,\n",
       " '7,1,2,3': 910,\n",
       " '7,1,3,0': 911,\n",
       " '7,1,3,1': 912,\n",
       " '7,1,3,2': 913,\n",
       " '7,1,3,3': 914,\n",
       " '7,1,4,0': 915,\n",
       " '7,1,4,1': 916,\n",
       " '7,1,4,2': 917,\n",
       " '7,1,4,3': 918,\n",
       " '7,1,5,0': 919,\n",
       " '7,1,5,1': 920,\n",
       " '7,1,5,2': 921,\n",
       " '7,1,5,3': 922,\n",
       " '7,1,6,0': 923,\n",
       " '7,1,6,1': 924,\n",
       " '7,1,6,2': 925,\n",
       " '7,1,6,3': 926,\n",
       " '7,1,7,0': 927,\n",
       " '7,1,7,2': 928,\n",
       " '7,1,7,3': 929,\n",
       " '7,2,0,0': 930,\n",
       " '7,2,0,1': 931,\n",
       " '7,2,0,2': 932,\n",
       " '7,2,0,3': 933,\n",
       " '7,2,1,0': 934,\n",
       " '7,2,1,1': 935,\n",
       " '7,2,1,2': 936,\n",
       " '7,2,1,3': 937,\n",
       " '7,2,2,0': 938,\n",
       " '7,2,2,1': 939,\n",
       " '7,2,2,2': 940,\n",
       " '7,2,2,3': 941,\n",
       " '7,2,3,0': 942,\n",
       " '7,2,3,1': 943,\n",
       " '7,2,3,2': 944,\n",
       " '7,2,3,3': 945,\n",
       " '7,2,4,0': 946,\n",
       " '7,2,4,1': 947,\n",
       " '7,2,4,2': 948,\n",
       " '7,2,4,3': 949,\n",
       " '7,2,5,0': 950,\n",
       " '7,2,5,1': 951,\n",
       " '7,2,5,2': 952,\n",
       " '7,2,5,3': 953,\n",
       " '7,2,6,0': 954,\n",
       " '7,2,6,1': 955,\n",
       " '7,2,6,2': 956,\n",
       " '7,2,6,3': 957,\n",
       " '7,2,7,0': 958,\n",
       " '7,2,7,1': 959,\n",
       " '7,2,7,3': 960,\n",
       " '7,3,0,0': 961,\n",
       " '7,3,0,1': 962,\n",
       " '7,3,0,2': 963,\n",
       " '7,3,0,3': 964,\n",
       " '7,3,1,0': 965,\n",
       " '7,3,1,1': 966,\n",
       " '7,3,1,2': 967,\n",
       " '7,3,1,3': 968,\n",
       " '7,3,2,0': 969,\n",
       " '7,3,2,1': 970,\n",
       " '7,3,2,2': 971,\n",
       " '7,3,2,3': 972,\n",
       " '7,3,3,0': 973,\n",
       " '7,3,3,1': 974,\n",
       " '7,3,3,2': 975,\n",
       " '7,3,3,3': 976,\n",
       " '7,3,4,0': 977,\n",
       " '7,3,4,1': 978,\n",
       " '7,3,4,2': 979,\n",
       " '7,3,4,3': 980,\n",
       " '7,3,5,0': 981,\n",
       " '7,3,5,1': 982,\n",
       " '7,3,5,2': 983,\n",
       " '7,3,5,3': 984,\n",
       " '7,3,6,0': 985,\n",
       " '7,3,6,1': 986,\n",
       " '7,3,6,2': 987,\n",
       " '7,3,6,3': 988,\n",
       " '7,3,7,0': 989,\n",
       " '7,3,7,1': 990,\n",
       " '7,3,7,2': 991}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_board(board):\n",
    "    output_maps = torch.zeros((4, 8, 4))\n",
    "    for k,(side, typ) in enumerate([[\"black\",\"men\"], [\"black\", \"kings\"], [\"white\",\"men\"],[\"white\",\"kings\"]]):\n",
    "        _indices = list(board[side][typ])\n",
    "        _2d_indices = [[int(i/4), i % 4] for i in _indices]\n",
    "        _maps = torch.zeros((1,8,4))\n",
    "        for i,j in _2d_indices:\n",
    "            _maps[:,i,j] = torch.tensor([1.0])\n",
    "        output_maps[k, :, :] = _maps\n",
    "    output_maps = output_maps.flatten()\n",
    "    return output_maps\n",
    "\n",
    "def encode_action(action):\n",
    "    onehot = torch.zeros((len(possible_moves)))\n",
    "    a = [int(action[0] / 4), action[0] % 4]\n",
    "    b = [int(action[1] / 4), action[1] % 4]\n",
    "    index = possible_moves[f\"{a[0]},{a[1]},{b[0]},{b[1]}\"]\n",
    "    onehot[index] = torch.tensor([1.0])\n",
    "    return onehot\n",
    "\n",
    "def decode_action(encoded_action):\n",
    "    max_id = encoded_action.argmax(dim=0)\n",
    "\n",
    "    x = inv_possible_moves[max_id.item()]\n",
    "    \n",
    "    max_pos_a = (x[0],x[1])\n",
    "    max_pos_b = (x[2],x[3])\n",
    "\n",
    "    pred_move = ((max_pos_a[0] * 4) + max_pos_a[1], (max_pos_b[0] * 4) + max_pos_b[1])\n",
    "    return pred_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player (for inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/boat/pattern/pattern_term_project_2024\n"
     ]
    }
   ],
   "source": [
    "from src.utils import append_gym_checker\n",
    "\n",
    "append_gym_checker()\n",
    "\n",
    "import json\n",
    "import random\n",
    "from checkers.agents import Player\n",
    "import copy\n",
    "\n",
    "\n",
    "class DeepKILLmePlayer(Player):\n",
    "    def __init__(\n",
    "        self,\n",
    "        color,\n",
    "        memory,\n",
    "    ) -> None:\n",
    "        super().__init__(color=color)\n",
    "        self._color = color\n",
    "\n",
    "        self.policyModel = DeepKILLme(num_in_feat, len(possible_moves)).to(\"cuda\")\n",
    "        self.targetModel = DeepKILLme(num_in_feat, len(possible_moves)).to(\"cuda\")\n",
    "        self.targetModel.load_state_dict(self.policyModel.state_dict())\n",
    "        # extra\n",
    "        self.modelName = \"DeepKILLme\" #use to told the env to push reward and next state for us\n",
    "        self.memory = memory\n",
    "        self._epsilon = 0.1\n",
    "        self._illegal_move_penalty = 0\n",
    "\n",
    "    def next_move(self, board, last_moved_piece):\n",
    "        # fix\n",
    "        global global_epsilon\n",
    "        self._epsilon = global_epsilon\n",
    "        state = board, self.color, last_moved_piece\n",
    "        self.simulator.restore_state(state)\n",
    "        legal_moves = self.simulator.legal_moves()\n",
    "        # up to your model\n",
    "        move = None\n",
    "        reward = None\n",
    "        # print(board)\n",
    "        encoded_board = encode_board(board).to(\"cuda\")\n",
    "        encoded_pred_action = self.targetModel(encoded_board).to(\"cuda\")\n",
    "        if random.random() > self._epsilon:\n",
    "            print(\"do use q\")\n",
    "            pred_move = decode_action(encoded_pred_action)\n",
    "\n",
    "            if any([legal[0]==pred_move[0] and legal[1] == pred_move[1] for legal in legal_moves]):\n",
    "                move = pred_move\n",
    "            else:\n",
    "                move = random.choice(legal_moves)\n",
    "                reward = self._illegal_move_penalty\n",
    "        else:\n",
    "            move = random.choice(legal_moves)\n",
    "\n",
    "        self.memory.append([copy.deepcopy(board), copy.deepcopy(move), None, reward]) #board, action, next board, reward (next board will be fill by env later) (reward will fill by model if illegal move, else fill by model)\n",
    "        \n",
    "        return move\n",
    "    \n",
    "    def get_model_state(self):\n",
    "        model_states = {\n",
    "            \"policyModel_state_dict\":self.policyModel.state_dict(),\n",
    "            \"targetModel_state_dict\":self.targetModel.state_dict(),\n",
    "        }\n",
    "        return model_states\n",
    "    \n",
    "    def load_model_state(self, model_states):\n",
    "        self.policyModel.load_state_dict(model_states[\"policyModel_state_dict\"])\n",
    "        self.targetModel.load_state_dict(model_states[\"targetModel_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# BATCH_SIZE = 16\n",
    "# GAMMA = 0.99\n",
    "# EPS_START = 0.9\n",
    "# EPS_END = 0.05\n",
    "# EPS_DECAY = 1000\n",
    "# TAU = 0.005\n",
    "# LR = 1e-4\n",
    "\n",
    "memory = []\n",
    "\n",
    "global_epsilon = -1.0 #inference\n",
    "\n",
    "white_player = DeepKILLmePlayer(\"white\", memory)\n",
    "\n",
    "MAX_MEM_CAPACITY = 10000\n",
    "# optimizer = optim.AdamW(white_player.policyModel.parameters(), lr=LR, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # A few matches against a random player\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import math\n",
    "\n",
    "# max_game_len = 100\n",
    "# n_matches = 10000\n",
    "# n_wins, n_draws, n_losses = 0, 0, 0\n",
    "# is_show_game = False\n",
    "# explore_rate=0.1\n",
    "# rand = 1203\n",
    "\n",
    "# def rollout_order_gen_random(x):\n",
    "#     random.shuffle(x)\n",
    "#     return x\n",
    "    \n",
    "# for i in tqdm(range(n_matches)):\n",
    "#     if is_show_game:\n",
    "#         print('game', i)\n",
    "#     ch = Checkers()\n",
    "#     black_player = MinimaxPlayer(\n",
    "#         'black',\n",
    "#         # value_func=partial(first_order_adv, 'black', 200, 100, 20, 0),\n",
    "#         value_func=partial(first_order_adv, 'black', 86.0315, 54.568, 87.21072, 25.85066),        \n",
    "#         # The provided legal moves might be ordered differently\n",
    "#         rollout_order_gen=rollout_order_gen_random,\n",
    "#         search_depth=2,\n",
    "#         seed=i+rand)\n",
    "\n",
    "#     #modify this function to put our RL model as white\n",
    "#     winner = play_a_game(ch, black_player.next_move, white_player.next_move, max_game_len,is_show_detail = is_show_game, white_player=white_player)\n",
    "\n",
    "#     # impl train here\n",
    "#     if  white_player.memory.__len__() < BATCH_SIZE: continue\n",
    "#     # clip memory\n",
    "#     if white_player.memory.__len__() > MAX_MEM_CAPACITY:\n",
    "#         white_player.memory = white_player.memory[white_player.memory.__len__() - MAX_MEM_CAPACITY:]\n",
    "#     print(\"white_player.memory.__len__()\",white_player.memory.__len__())\n",
    "\n",
    "#     # filter none (maybe in case of multiple capture, the condition in baseline.py is not cover this case) TODO: impl multiple capture support\n",
    "#     white_player.memory = list(filter(lambda x: x[2] is not None, white_player.memory, ))\n",
    "\n",
    "#     transitions = random.sample(white_player.memory, BATCH_SIZE)\n",
    "\n",
    "#     state_batch = []\n",
    "#     next_state_batch = []\n",
    "#     action_batch = []\n",
    "#     reward_batch = []\n",
    "#     for board, action, next_board, reward in transitions:\n",
    "#         encoded_board = encode_board(board)\n",
    "#         encoded_next_board = encode_board(next_board)\n",
    "#         encoded_action = encode_action(action)\n",
    "#         reward = torch.tensor(reward)\n",
    "\n",
    "#         state_batch.append(encoded_board)\n",
    "#         next_state_batch.append(encoded_next_board)\n",
    "#         action_batch.append(encoded_action)\n",
    "#         reward_batch.append(reward)\n",
    "\n",
    "#     state_batch = torch.stack(state_batch).to(\"cuda\")\n",
    "#     next_state_batch = torch.stack(next_state_batch).to(\"cuda\")\n",
    "#     action_batch = torch.stack(action_batch).to(\"cuda\")\n",
    "#     reward_batch = torch.stack(reward_batch).to(\"cuda\")\n",
    "    \n",
    "#     print(\"len batch\", len(state_batch), len(next_state_batch), len(action_batch), len(reward_batch))\n",
    "\n",
    "#     # forward \n",
    "#     state_action_values = white_player.policyModel(state_batch)\n",
    "#     next_state_values = white_player.targetModel(next_state_batch).max(1).values\n",
    "\n",
    "#     # compute expected Q\n",
    "#     expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "#     criterion = nn.SmoothL1Loss()\n",
    "#     loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "    \n",
    "#     # In-place gradient clipping\n",
    "#     torch.nn.utils.clip_grad_value_(white_player.policyModel.parameters(), 100)\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # update target model using policy weight\n",
    "#     target_net_state_dict = white_player.targetModel.state_dict()\n",
    "#     policy_net_state_dict = white_player.policyModel.state_dict()\n",
    "#     for key in policy_net_state_dict:\n",
    "#         target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "#     white_player.targetModel.load_state_dict(target_net_state_dict)\n",
    "\n",
    "#     # decay eps\n",
    "#     global_epsilon = EPS_END + (EPS_START - EPS_END) * \\\n",
    "#         math.exp(-1. * i / EPS_DECAY)\n",
    "\n",
    "#     # Play with a minimax player\n",
    "#     # play_a_game(ch, keyboard_player_move, white_player.next_move)\n",
    "#     if is_show_game:\n",
    "#         print('black player evaluated %i positions in %.2fs (avg %.2f positions/s) effective branching factor %.2f' % (black_player.n_evaluated_positions, black_player.evaluation_dt, black_player.n_evaluated_positions / black_player.evaluation_dt, (black_player.n_evaluated_positions / black_player.ply) ** (1 / black_player.search_depth)))\n",
    "#         print('black player pruned', black_player.prunes.items())\n",
    "#         print()\n",
    "        \n",
    "#     result:RESULT_TYPE\n",
    "#     if winner == 'black':\n",
    "#         n_wins += 1\n",
    "#         result = RESULT_TYPE.LOSE\n",
    "#     elif winner is None:\n",
    "#         n_draws += 1\n",
    "#         result = RESULT_TYPE.DRAW\n",
    "#     else:\n",
    "#         n_losses += 1\n",
    "#         result = RESULT_TYPE.WIN\n",
    "\n",
    "#     print(f\"round : {i+1}/{n_matches} result {result.value}, explore_rate {explore_rate}\")\n",
    "#     print(\"loss\", loss)\n",
    "#     print(\"memory reward sum\", sum([i[3] for i in white_player.memory]))\n",
    "\n",
    "# print('black win', n_wins, 'draw', n_draws, 'loss', n_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'models': {'policyModel_state_dict': OrderedDict([('fc1.weight',\n",
       "                tensor([[-0.0643, -0.0060, -0.0968,  ...,  0.0024,  0.0515,  0.0332],\n",
       "                        [ 0.0447, -0.0436, -0.0284,  ...,  0.1375,  0.0381, -0.0696],\n",
       "                        [ 0.0180, -0.0077,  0.0272,  ...,  0.0707,  0.0670, -0.0595],\n",
       "                        ...,\n",
       "                        [-0.1327, -0.2020, -0.0895,  ...,  0.0465,  0.0284, -0.0021],\n",
       "                        [-0.0212, -0.0236, -0.0505,  ...,  0.0525,  0.0787,  0.1222],\n",
       "                        [ 0.0060, -0.0251,  0.0265,  ...,  0.0065, -0.1026, -0.1370]],\n",
       "                       device='cuda:0')),\n",
       "               ('fc1.bias',\n",
       "                tensor([-0.0034,  0.0150, -0.0583, -0.0317, -0.0089, -0.0281, -0.0411, -0.0656,\n",
       "                        -0.0332,  0.0464,  0.1508,  0.1326, -0.0333,  0.0468,  0.0047, -0.1089,\n",
       "                         0.1466,  0.0084,  0.1331,  0.0831,  0.1657, -0.0266,  0.1738, -0.0531,\n",
       "                         0.0339,  0.1062,  0.0911,  0.1590,  0.0981, -0.0713, -0.0957, -0.0262,\n",
       "                        -0.0148,  0.0597, -0.0004,  0.1098,  0.0798, -0.0264,  0.1218,  0.1639,\n",
       "                         0.0349, -0.0542,  0.0681,  0.0429,  0.0134, -0.0545,  0.1146, -0.0190,\n",
       "                         0.0776,  0.0023, -0.0876, -0.0770, -0.0621,  0.0749, -0.0580,  0.0891,\n",
       "                         0.1087, -0.0505, -0.0228,  0.0439, -0.0076,  0.1351,  0.0265, -0.0304,\n",
       "                         0.1836, -0.0954,  0.0930,  0.0885,  0.1360, -0.1183, -0.0200, -0.0156,\n",
       "                         0.0951, -0.1056,  0.0275, -0.0487,  0.1428,  0.1083,  0.0705, -0.0396,\n",
       "                         0.0063,  0.0678, -0.0078,  0.0593,  0.0585,  0.0387, -0.0585,  0.1397,\n",
       "                         0.0714,  0.1008,  0.0481,  0.0305,  0.0645, -0.0383,  0.1123,  0.1194,\n",
       "                        -0.0582, -0.0225,  0.1318, -0.0467,  0.0270, -0.0461, -0.0183,  0.0090,\n",
       "                         0.1209,  0.1561,  0.0861,  0.0449,  0.0128, -0.0278,  0.0533,  0.0221,\n",
       "                        -0.0091, -0.0850, -0.0146,  0.1546, -0.0409,  0.0295,  0.0551,  0.0280,\n",
       "                         0.0124,  0.0115, -0.0268,  0.0664,  0.0548,  0.1772,  0.1014,  0.0748,\n",
       "                        -0.0628, -0.0393, -0.0531,  0.0553, -0.0627,  0.1343,  0.1044,  0.0689,\n",
       "                        -0.0737, -0.0052,  0.0843, -0.0775,  0.0341, -0.0482,  0.1261, -0.0145,\n",
       "                        -0.1000,  0.0151,  0.1352,  0.1123,  0.0201,  0.0280,  0.1506,  0.0658,\n",
       "                         0.1220,  0.0246,  0.0385, -0.0188,  0.0309,  0.1457,  0.0415,  0.1361,\n",
       "                         0.0272,  0.0716,  0.0027,  0.0395, -0.0238,  0.0807, -0.0866,  0.0331,\n",
       "                        -0.0595,  0.1385, -0.0217, -0.0590,  0.0647, -0.0472,  0.1031,  0.1167,\n",
       "                         0.0961,  0.0280,  0.0161,  0.0680,  0.0469, -0.0532,  0.0018,  0.0329,\n",
       "                         0.0190, -0.0505, -0.0085,  0.0960,  0.0367,  0.1188, -0.0060,  0.0343,\n",
       "                         0.0705,  0.0595,  0.0091, -0.0549,  0.0281,  0.0020, -0.0040, -0.0037,\n",
       "                         0.0343,  0.1439, -0.0292,  0.0794,  0.1123, -0.0761, -0.0477,  0.0452,\n",
       "                         0.1416,  0.0832,  0.0217, -0.0460, -0.0447, -0.0326,  0.0799, -0.0970,\n",
       "                         0.1233,  0.2008,  0.0461,  0.1137,  0.0187,  0.1501, -0.0374,  0.0677,\n",
       "                         0.1470, -0.0798, -0.0063,  0.1071,  0.0798, -0.0931,  0.0756,  0.0726,\n",
       "                         0.0803, -0.0840, -0.0693,  0.0571,  0.0926,  0.0967,  0.1128,  0.0647,\n",
       "                         0.1112,  0.0529, -0.0387,  0.0413,  0.1187,  0.0240,  0.0714,  0.0713,\n",
       "                         0.0059,  0.0261,  0.1684,  0.1339,  0.0249,  0.0602, -0.1029,  0.0794],\n",
       "                       device='cuda:0')),\n",
       "               ('fc2.weight',\n",
       "                tensor([[ 0.1451, -0.0004, -0.0545,  ...,  0.1024,  0.2482,  0.0184],\n",
       "                        [ 0.1621, -0.0041, -0.0094,  ...,  0.0786,  0.1548, -0.0264],\n",
       "                        [ 0.1016, -0.0363,  0.0147,  ...,  0.1048,  0.2262,  0.0270],\n",
       "                        ...,\n",
       "                        [ 0.1621,  0.0180, -0.0513,  ...,  0.1245,  0.2557, -0.0013],\n",
       "                        [ 0.1411,  0.0179, -0.0245,  ...,  0.1059,  0.2368,  0.0125],\n",
       "                        [ 0.0968,  0.0775, -0.0678,  ...,  0.0567,  0.2562, -0.0537]],\n",
       "                       device='cuda:0')),\n",
       "               ('fc2.bias',\n",
       "                tensor([ 0.0599,  0.0363, -0.0145,  0.0417, -0.0444, -0.0236,  0.0094,  0.0160,\n",
       "                         0.0807,  0.0374, -0.0059,  0.0807,  0.0840,  0.0841,  0.0022,  0.0510,\n",
       "                         0.0576, -0.0542, -0.0239, -0.0172,  0.0037, -0.0020,  0.0790,  0.0606,\n",
       "                         0.0230,  0.0243, -0.1461,  0.0409, -0.0228,  0.0569,  0.0510,  0.0860,\n",
       "                        -0.0220,  0.0737, -0.0244,  0.0173,  0.0179,  0.0240, -0.0019,  0.0254,\n",
       "                         0.0328,  0.0084,  0.0586,  0.0514, -0.0134,  0.0780,  0.0023,  0.0534,\n",
       "                         0.0781, -0.0231,  0.0190, -0.0228,  0.0247,  0.0068,  0.0360, -0.0271,\n",
       "                         0.0735,  0.0395, -0.0332,  0.0337,  0.0294, -0.0034,  0.0268,  0.0412,\n",
       "                        -0.0086,  0.0640,  0.0886,  0.0557,  0.0408,  0.0355,  0.0952, -0.0352,\n",
       "                         0.0714, -0.0030, -0.0263,  0.0514,  0.0424,  0.0645,  0.0765, -0.0016,\n",
       "                         0.0842, -0.0011,  0.0826,  0.0233, -0.0102,  0.0457,  0.0239,  0.0685,\n",
       "                         0.0682,  0.0327,  0.0424,  0.0212,  0.0686, -0.0191,  0.0188,  0.0315,\n",
       "                        -0.0064,  0.0537, -0.0204,  0.0444, -0.1745,  0.0331,  0.0210, -0.0123,\n",
       "                         0.0483,  0.0232,  0.0331, -0.0029,  0.0765, -0.0143,  0.0292,  0.0027,\n",
       "                         0.0810,  0.0586, -0.0392,  0.0217,  0.0857, -0.0248, -0.0050, -0.0232,\n",
       "                        -0.0194, -0.0220,  0.0248,  0.0702, -0.0169, -0.0137,  0.0156,  0.0267,\n",
       "                         0.0785, -0.0012,  0.0321,  0.0887,  0.0341,  0.0320, -0.0036,  0.0451,\n",
       "                         0.0105, -0.0031, -0.0154, -0.0192,  0.0851, -0.0003,  0.0356,  0.0026,\n",
       "                        -0.0049,  0.0502,  0.0614, -0.0244, -0.0294,  0.0177, -0.0095, -0.0357,\n",
       "                        -0.1115,  0.0298,  0.0278, -0.0427,  0.0613,  0.0538, -0.0494, -0.0190,\n",
       "                        -0.0121,  0.0063,  0.0819,  0.0137,  0.0493,  0.0772,  0.0787,  0.0329,\n",
       "                         0.0535, -0.0238, -0.0249,  0.0771,  0.0821,  0.0196,  0.0592,  0.0470,\n",
       "                        -0.0072, -0.0116, -0.0212,  0.0621,  0.0282,  0.0167,  0.0124,  0.0888,\n",
       "                         0.0772,  0.0402,  0.0117,  0.0514,  0.0043,  0.0279,  0.0187,  0.0149,\n",
       "                         0.0345,  0.0820,  0.0919,  0.0523,  0.0153,  0.0793,  0.0373,  0.0427,\n",
       "                         0.0914,  0.0364, -0.0555,  0.0422, -0.0099,  0.0537,  0.0205,  0.0629,\n",
       "                         0.0283,  0.0211,  0.0194,  0.0936,  0.0046,  0.0777, -0.0213,  0.0956,\n",
       "                         0.0787, -0.0131, -0.0110, -0.0138, -0.0233,  0.0383,  0.0919,  0.0147,\n",
       "                         0.0643,  0.0068, -0.0146,  0.0673,  0.0278,  0.0508,  0.0550, -0.0395,\n",
       "                         0.0832,  0.0554,  0.0311, -0.0395,  0.0486,  0.0871,  0.0558,  0.0565,\n",
       "                         0.0628,  0.0509,  0.0720,  0.0705,  0.0257,  0.0702,  0.0019, -0.0102,\n",
       "                        -0.0166,  0.0163,  0.0512,  0.0362,  0.0775, -0.0128,  0.0803,  0.0664],\n",
       "                       device='cuda:0')),\n",
       "               ('fc3.weight',\n",
       "                tensor([[-0.0065,  0.1011,  0.0446,  ...,  0.0441,  0.0681,  0.0783],\n",
       "                        [ 0.0056, -0.0021,  0.0515,  ...,  0.0689,  0.0077,  0.0456],\n",
       "                        [ 0.0218,  0.0248,  0.0944,  ...,  0.1132, -0.0050,  0.0028],\n",
       "                        ...,\n",
       "                        [-0.0387,  0.0168, -0.0225,  ...,  0.0276,  0.0560, -0.0369],\n",
       "                        [ 0.0661,  0.0215,  0.0510,  ...,  0.0712,  0.0283,  0.0390],\n",
       "                        [ 0.0183,  0.0481,  0.0685,  ...,  0.1108,  0.0100, -0.0004]],\n",
       "                       device='cuda:0')),\n",
       "               ('fc3.bias',\n",
       "                tensor([ 6.7705e-02, -7.8271e-03,  9.1828e-02, -5.0688e-02,  5.0826e-02,\n",
       "                         3.5116e-02,  9.7673e-02, -1.8405e-04,  7.9428e-02, -3.5114e-02,\n",
       "                         7.8664e-03,  9.6581e-02,  3.9408e-02, -4.9654e-02,  3.8385e-02,\n",
       "                         6.4650e-02, -6.6670e-02,  7.9800e-02, -4.9197e-03,  2.1363e-02,\n",
       "                         4.5978e-02,  2.3419e-03,  3.0950e-04,  1.0559e-01,  4.4936e-02,\n",
       "                         5.4774e-02,  4.7226e-04,  9.6348e-02, -4.9697e-03,  4.4842e-02,\n",
       "                         4.4913e-02,  6.6349e-02, -2.2428e-02,  3.7031e-02,  4.1599e-02,\n",
       "                         2.4935e-03,  8.8754e-02, -5.3261e-02,  8.3122e-02, -5.0670e-02,\n",
       "                         8.3867e-02,  5.0991e-02, -1.2641e-02,  4.1992e-02,  5.9807e-02,\n",
       "                        -4.1193e-02,  2.2647e-02, -3.4714e-03,  8.1056e-02,  3.1432e-03,\n",
       "                         1.3793e-02, -3.2087e-02, -1.9500e-02, -4.1798e-02,  7.1161e-02,\n",
       "                         2.2532e-02,  7.3990e-03,  5.5943e-04,  1.0258e-01,  6.4331e-02,\n",
       "                         7.8370e-02,  1.0353e-01, -4.1222e-02,  5.0239e-03,  9.4295e-02,\n",
       "                        -3.1448e-02, -2.1055e-02,  6.0554e-02, -5.2680e-05, -1.6995e-02,\n",
       "                         6.5162e-02,  7.4660e-02, -4.4937e-02,  6.4808e-02, -3.0396e-02,\n",
       "                         2.0630e-02, -1.9053e-02,  8.2143e-02,  4.8126e-02,  8.6662e-02,\n",
       "                         2.6490e-02,  4.3103e-02, -6.9855e-03, -1.9933e-02,  9.8552e-03,\n",
       "                        -2.9201e-02,  5.7709e-02, -4.1749e-02,  8.2388e-02, -7.8159e-03,\n",
       "                         6.2147e-02,  9.9741e-02, -1.6340e-02,  9.7407e-02,  9.6378e-02,\n",
       "                        -5.6174e-02,  5.9441e-03,  1.7980e-02,  5.5334e-02,  8.3207e-02,\n",
       "                        -3.1619e-02,  2.0028e-02, -2.5443e-02, -2.1105e-02,  2.5669e-03,\n",
       "                         6.0031e-03,  8.4860e-03,  7.0199e-02,  8.5642e-02, -8.2769e-03,\n",
       "                         2.1481e-02,  6.0939e-02, -3.2030e-02,  1.1085e-02, -5.2639e-03,\n",
       "                         8.6092e-02,  8.3758e-02,  6.6857e-02, -2.2494e-02, -2.8956e-03,\n",
       "                         2.1291e-02,  5.9081e-02, -1.8426e-02,  6.1610e-02,  7.5241e-02,\n",
       "                         3.9773e-02,  8.5725e-02, -7.3203e-05,  2.2882e-02,  4.9318e-02,\n",
       "                         6.7593e-02,  1.6675e-03,  5.0411e-02,  9.9202e-02, -7.5235e-03,\n",
       "                         1.3156e-02,  4.2236e-02,  1.5647e-02,  6.6901e-02,  1.0597e-02,\n",
       "                         9.1926e-02, -5.3875e-02, -9.8965e-03,  5.5788e-02,  3.0823e-02,\n",
       "                         6.3902e-02,  3.3920e-02,  6.3564e-02,  2.2767e-02, -5.7573e-03,\n",
       "                         9.3601e-02, -5.8611e-02,  3.1256e-02,  8.8157e-02,  5.3269e-02,\n",
       "                        -9.3086e-03,  4.3062e-02,  6.6895e-02,  9.6279e-02,  5.7093e-03,\n",
       "                         3.8053e-02, -1.9180e-03,  1.3602e-02,  5.5835e-02,  7.6212e-03,\n",
       "                         2.0985e-02,  9.0285e-02, -4.5478e-03, -6.1018e-02, -2.3202e-02,\n",
       "                         4.4860e-02,  4.0708e-02,  7.4628e-02, -3.0232e-02,  1.4144e-02,\n",
       "                         1.3474e-02,  2.0843e-02,  1.3985e-02,  7.3776e-02,  7.7731e-02,\n",
       "                         8.7605e-02,  6.4183e-02, -2.1281e-02,  9.5029e-02,  5.6802e-02,\n",
       "                        -5.7019e-02,  1.0633e-01, -4.4200e-03,  7.5592e-02,  1.8050e-02,\n",
       "                         7.6417e-02,  4.6236e-02,  7.4131e-02,  7.6907e-02,  4.1906e-02,\n",
       "                         5.5375e-02,  5.1038e-03, -2.0680e-02,  4.3334e-02, -1.3529e-02,\n",
       "                         9.4398e-02, -3.7629e-02, -4.5577e-02,  2.3524e-02,  5.4507e-02,\n",
       "                         1.5821e-02,  3.0007e-03,  6.0992e-04,  3.4325e-02,  1.0129e-02,\n",
       "                        -2.7254e-03,  1.0468e-01, -1.5876e-02,  5.6492e-02, -5.0578e-02,\n",
       "                        -3.0805e-02, -5.1963e-02,  3.3250e-02,  7.8004e-02, -5.6316e-02,\n",
       "                         7.5644e-02, -1.8974e-03,  1.4290e-02, -1.8896e-02,  3.1192e-02,\n",
       "                         7.5429e-02,  7.1585e-02, -3.9482e-02,  3.0660e-02,  9.6238e-02,\n",
       "                        -1.3446e-03, -2.9857e-02,  7.7575e-02, -2.7355e-02,  6.8309e-02,\n",
       "                         7.9515e-03, -2.0452e-03, -3.9344e-02, -3.9624e-02,  8.2097e-02,\n",
       "                         6.2353e-02,  2.2776e-02,  5.9708e-02,  5.9127e-02, -1.2389e-03,\n",
       "                         1.0325e-01, -3.4171e-02, -3.4972e-02,  7.1586e-02,  8.0193e-02,\n",
       "                         7.1522e-02,  6.7850e-02,  3.4490e-02, -3.2543e-02, -1.8730e-02,\n",
       "                         6.9179e-02], device='cuda:0')),\n",
       "               ('fc4.weight',\n",
       "                tensor([[ 0.1502,  0.1310,  0.1433,  ..., -0.0426,  0.1228,  0.0929],\n",
       "                        [ 0.0944,  0.0735,  0.0705,  ...,  0.0390,  0.1371,  0.1363],\n",
       "                        [ 0.0581,  0.1465,  0.0519,  ..., -0.0511,  0.0434,  0.0948],\n",
       "                        ...,\n",
       "                        [ 0.0782,  0.0935,  0.1142,  ...,  0.0164,  0.1015,  0.1180],\n",
       "                        [ 0.0709,  0.1341,  0.0622,  ..., -0.0487,  0.1373,  0.0889],\n",
       "                        [ 0.0695,  0.1196,  0.0677,  ..., -0.0347,  0.0886,  0.0375]],\n",
       "                       device='cuda:0')),\n",
       "               ('fc4.bias',\n",
       "                tensor([ 9.7553e-02,  7.6601e-02,  1.3472e-02,  1.7982e-02,  1.4770e-01,\n",
       "                         2.4274e-01, -2.2728e-02,  4.3701e-02,  1.0090e-01,  1.0571e-01,\n",
       "                         9.0061e-02,  3.2470e-02,  7.8286e-02,  1.5594e-02,  1.0160e-01,\n",
       "                        -3.8960e-03,  9.5038e-02,  1.9801e-02,  4.2122e-02, -1.5946e-02,\n",
       "                         5.8534e-02,  3.8324e-02,  1.3719e-01, -6.9716e-03,  2.3334e-02,\n",
       "                        -4.6426e-02, -3.1099e-02, -4.2742e-02,  9.0892e-02,  1.8632e-01,\n",
       "                         5.3054e-02,  5.4202e-02,  7.6445e-02,  2.3582e-02,  9.5201e-02,\n",
       "                         2.3368e-02,  8.2415e-02,  3.2893e-02, -1.3682e-02,  6.6562e-02,\n",
       "                        -5.2270e-02,  5.9959e-02, -7.6388e-03, -9.7654e-03,  1.3170e-01,\n",
       "                        -6.0419e-02,  9.1717e-03, -4.2071e-03,  5.1760e-02,  6.2425e-02,\n",
       "                        -2.9432e-02,  1.6465e-01,  7.5948e-02,  4.7835e-02,  7.1324e-02,\n",
       "                         5.7066e-02, -4.9800e-03,  1.0852e-01,  1.0161e-01,  7.9005e-02,\n",
       "                        -1.6801e-02,  7.1831e-02,  6.1592e-03,  1.7825e-01,  4.0578e-02,\n",
       "                         4.9935e-02,  3.4070e-02,  7.4671e-02, -1.9086e-02, -5.4088e-02,\n",
       "                         9.6737e-02,  4.0193e-02,  5.0287e-02,  8.0949e-02,  6.1543e-02,\n",
       "                         2.6030e-02, -1.6226e-03,  8.4770e-02, -2.7151e-02,  9.6757e-02,\n",
       "                         8.0618e-02, -2.0518e-02, -5.4284e-02,  6.0656e-02,  1.7944e-02,\n",
       "                         4.8221e-02, -5.0831e-02,  1.9229e-01,  5.2907e-02, -2.9252e-02,\n",
       "                        -5.1512e-02, -4.7620e-02,  4.0680e-02, -7.9636e-03,  4.0598e-02,\n",
       "                         5.9393e-02,  4.6063e-02,  4.5291e-02,  8.2191e-02,  8.5280e-02,\n",
       "                         5.6807e-02,  5.8603e-02,  1.0273e-01,  2.4756e-02, -2.2505e-02,\n",
       "                         3.1370e-02,  3.8419e-02,  7.4358e-04,  9.9729e-02,  8.2035e-02,\n",
       "                         2.2334e-02, -6.8811e-03, -5.4084e-02, -5.2792e-02,  3.1844e-02,\n",
       "                         4.6030e-02,  4.4536e-02,  2.2820e-02,  9.1005e-02,  3.5548e-02,\n",
       "                         8.4975e-02, -8.9828e-03,  8.5283e-02,  2.7142e-01,  1.1608e-01,\n",
       "                         2.4126e-01,  1.0747e-01,  3.9820e-02, -5.5224e-02,  6.7079e-02,\n",
       "                         3.3250e-02,  7.9131e-02,  3.6118e-02,  7.1335e-02,  9.0913e-02,\n",
       "                         6.1424e-02,  4.5920e-02,  8.5122e-02,  6.2345e-02,  9.2077e-02,\n",
       "                         7.2057e-02,  4.5273e-02, -4.5772e-02,  1.7482e-02,  4.7422e-02,\n",
       "                         8.6548e-02,  5.6284e-02,  9.8011e-02,  1.6962e-02,  9.1542e-02,\n",
       "                         9.0881e-02,  1.0415e-02,  9.6958e-02, -4.8667e-02,  8.3721e-02,\n",
       "                         1.1515e-01,  1.0570e-01, -4.8010e-02,  4.5047e-02, -1.4281e-02,\n",
       "                        -2.3322e-02,  7.8518e-02, -1.3310e-02, -3.4048e-02,  4.6343e-02,\n",
       "                         6.7005e-02,  2.7111e-03,  4.8363e-02,  9.2318e-02,  8.9280e-02,\n",
       "                         2.5134e-01, -5.8237e-02, -2.5164e-02,  8.0974e-02,  7.8435e-02,\n",
       "                        -9.1144e-03,  1.0113e-01,  4.7270e-02,  3.8837e-02,  1.0077e-01,\n",
       "                         4.2794e-03,  7.8759e-03,  8.4284e-02, -1.2613e-02,  1.0613e-01,\n",
       "                         7.4266e-02,  5.7868e-02,  9.6713e-02,  4.6782e-02,  8.4058e-02,\n",
       "                         1.2297e-02, -2.8659e-02,  7.1890e-03,  6.1618e-02,  1.3515e-01,\n",
       "                         9.6991e-02,  8.5338e-02,  5.1661e-02,  2.3086e-01,  4.5328e-02,\n",
       "                         5.7012e-02,  1.7618e-01,  7.2051e-02,  2.0075e-02,  9.9328e-02,\n",
       "                         6.6560e-02,  9.8794e-02, -4.6163e-02,  6.6453e-02,  7.5828e-02,\n",
       "                         4.7508e-02, -3.6169e-02,  2.2966e-02,  4.5074e-02,  7.9162e-02,\n",
       "                         5.5888e-02, -3.0055e-02,  6.6313e-02, -2.3465e-03, -4.8908e-02,\n",
       "                         7.5645e-02,  1.5897e-01,  6.8189e-04,  2.4564e-02, -4.9644e-02,\n",
       "                        -7.7062e-03,  7.8621e-02,  5.0602e-02,  4.8174e-02,  4.5780e-02,\n",
       "                         7.3807e-02,  5.8838e-02,  3.3630e-02, -3.8788e-02,  1.0753e-01,\n",
       "                         3.3280e-02,  6.2785e-02,  9.8675e-02, -4.8311e-02,  2.3794e-02,\n",
       "                         1.1803e-02,  1.1940e-03,  9.6789e-02,  8.7650e-02,  2.3162e-01,\n",
       "                         7.2839e-02,  7.8364e-02,  6.6447e-02,  6.3047e-02,  5.8231e-02,\n",
       "                        -4.6908e-02,  1.9259e-01,  5.8270e-02,  7.5079e-02,  3.3640e-02,\n",
       "                         4.1575e-02,  5.4414e-02, -5.0670e-03,  6.7203e-02,  9.9807e-02,\n",
       "                        -1.3743e-05,  3.3413e-02, -3.1078e-02,  6.2862e-02,  9.6162e-02,\n",
       "                         2.9149e-03, -5.1309e-02, -4.1530e-02,  8.6841e-02,  7.7022e-02,\n",
       "                        -5.1663e-02,  7.2214e-02,  9.9231e-02,  1.4981e-01,  7.0731e-02,\n",
       "                         3.0695e-02,  6.2061e-02, -5.6716e-02,  6.5562e-02, -3.4274e-02,\n",
       "                         1.2480e-02,  9.3353e-02,  1.9328e-01,  2.0744e-02,  9.3229e-03,\n",
       "                         2.4442e-02, -3.8335e-02, -4.5468e-02, -5.1327e-02,  1.0085e-01,\n",
       "                         1.0732e-01,  5.2781e-02,  2.3885e-02,  1.0139e-02, -2.3128e-02,\n",
       "                         6.1918e-03,  4.2116e-02,  2.2728e-02, -4.9802e-02, -4.5001e-02,\n",
       "                         1.4657e-02,  6.0645e-02,  1.2747e-02,  8.0182e-02,  7.6132e-03,\n",
       "                         4.5194e-02,  7.8974e-02,  9.6706e-02, -4.5714e-02, -3.0489e-02,\n",
       "                         8.1682e-02,  1.0932e-01, -4.7715e-02,  3.8710e-02, -2.5511e-02,\n",
       "                         1.2067e-01, -6.4571e-03, -4.3763e-02,  2.3911e-02,  6.9322e-02,\n",
       "                        -4.4137e-02,  8.0932e-03,  9.2651e-02,  9.9573e-02,  4.3297e-02,\n",
       "                         7.7170e-02,  6.6795e-02,  1.0394e-01,  6.1457e-02,  5.5375e-03,\n",
       "                         1.0671e-02,  4.1702e-02,  9.3672e-03,  8.6722e-02,  2.7101e-02,\n",
       "                         8.5057e-02,  3.4416e-02,  9.5140e-02,  3.6246e-02,  4.4369e-02,\n",
       "                        -7.4034e-03,  4.9592e-02, -2.6074e-02,  9.2142e-02,  7.9404e-02,\n",
       "                         9.6846e-02, -3.7954e-02,  1.0263e-01,  9.5999e-02,  8.9792e-02,\n",
       "                         4.8311e-02,  6.8474e-02,  9.2606e-02,  8.7708e-02,  7.0951e-02,\n",
       "                         9.4374e-02,  2.7053e-02, -1.7560e-02,  7.1617e-02, -1.5062e-03,\n",
       "                        -4.7761e-02,  8.6449e-02,  1.0352e-01,  6.6706e-02,  7.1615e-02,\n",
       "                         8.5868e-02,  1.9025e-01,  2.7031e-01,  1.0121e-01,  7.4603e-02,\n",
       "                         2.6689e-02,  3.8686e-02, -1.4904e-03,  1.0365e-01, -8.1863e-03,\n",
       "                         6.4102e-02,  7.7583e-02,  1.0757e-01, -2.9191e-02, -2.7982e-02,\n",
       "                         7.6399e-02,  9.0354e-02,  2.3745e-01, -3.1546e-03,  9.7505e-02,\n",
       "                         3.7408e-03,  6.7881e-02,  3.1707e-02,  6.3505e-02,  4.0956e-02,\n",
       "                         1.0407e-01,  5.1578e-02,  8.9146e-02,  6.7909e-02,  7.2591e-02,\n",
       "                         5.0203e-02, -3.4259e-02,  7.2931e-02,  5.2575e-02, -2.5509e-03,\n",
       "                         5.0154e-02,  8.1541e-02, -4.2053e-02,  8.7114e-02,  1.1498e-02,\n",
       "                         1.0498e-01, -4.5066e-02,  5.9735e-02,  8.1768e-02,  1.6184e-02,\n",
       "                         7.6117e-02,  5.5117e-03,  1.8324e-02,  7.6733e-02,  1.4908e-01,\n",
       "                         8.0104e-02, -8.7619e-03,  1.0271e-01,  7.9172e-02,  7.3239e-02,\n",
       "                        -4.7130e-02,  7.0379e-03,  6.1235e-02,  1.2642e-02,  5.6394e-02,\n",
       "                         7.2760e-02, -4.0888e-02, -4.5150e-02,  1.1169e-01,  2.8645e-03,\n",
       "                        -3.5904e-03,  7.4077e-02, -1.4922e-02,  5.4082e-02, -2.2779e-02,\n",
       "                         7.7370e-02, -4.1333e-02, -2.8624e-02,  5.8071e-02,  9.5288e-02,\n",
       "                         4.6927e-02,  9.4129e-02,  1.2789e-01, -3.1097e-02,  8.0058e-02,\n",
       "                         8.3119e-02,  2.7419e-02,  7.2136e-02,  9.2670e-02,  8.6695e-02,\n",
       "                         7.6970e-02,  3.7841e-02,  6.2484e-02, -1.1730e-02, -1.6042e-02,\n",
       "                         1.0793e-01,  5.8573e-02,  6.5772e-02, -1.6297e-02,  1.4012e-01,\n",
       "                         7.7056e-02, -3.3294e-03,  5.2463e-02,  8.6329e-02,  4.5598e-02,\n",
       "                         5.5419e-02,  6.3293e-02,  1.0879e-01,  9.2090e-03,  9.3773e-02,\n",
       "                        -4.6591e-02, -3.8091e-02,  8.8768e-02,  4.0538e-02, -1.6652e-02,\n",
       "                         9.0537e-02,  1.4430e-01,  7.5777e-02,  5.8372e-02,  2.5584e-01,\n",
       "                         8.1387e-02,  4.0626e-02,  9.7474e-02,  7.6169e-04,  7.5639e-02,\n",
       "                        -4.4341e-02,  5.7889e-02,  7.0580e-02,  8.9947e-02,  2.1663e-02,\n",
       "                         8.2729e-02,  9.4202e-02,  7.1477e-02,  1.0515e-01,  1.3790e-02,\n",
       "                        -5.3920e-02,  9.1460e-02,  1.6628e-01,  6.3089e-02,  3.3896e-02,\n",
       "                         9.5542e-02,  9.7508e-02, -7.7477e-02, -7.7639e-03,  4.6172e-02,\n",
       "                        -4.3945e-02,  5.7296e-02, -5.2110e-02,  2.5892e-01,  6.2535e-02,\n",
       "                         3.4962e-02,  2.9578e-02], device='cuda:0')),\n",
       "               ('fc5.weight',\n",
       "                tensor([[0.0706, 0.1351, 0.1039,  ..., 0.1378, 0.0865, 0.0958],\n",
       "                        [0.1210, 0.0698, 0.1132,  ..., 0.0747, 0.0537, 0.0961],\n",
       "                        [0.0763, 0.0655, 0.1286,  ..., 0.0884, 0.0503, 0.1355],\n",
       "                        ...,\n",
       "                        [0.1288, 0.1326, 0.0763,  ..., 0.0724, 0.0746, 0.0864],\n",
       "                        [0.0689, 0.0567, 0.1152,  ..., 0.0754, 0.0521, 0.1230],\n",
       "                        [0.0701, 0.1213, 0.1335,  ..., 0.0780, 0.1105, 0.1083]],\n",
       "                       device='cuda:0')),\n",
       "               ('fc5.bias',\n",
       "                tensor([ 5.5964e-02,  5.7714e-02,  6.9460e-02,  3.5826e-02,  2.5021e-02,\n",
       "                        -5.6266e-03,  1.1225e-02,  7.0997e-02,  6.7016e-02,  4.0831e-02,\n",
       "                         4.8352e-02,  4.3072e-03,  1.5520e-02,  1.8824e-02,  8.2301e-02,\n",
       "                         2.8181e-02,  3.3876e-02,  4.5918e-02,  5.5353e-02,  3.6836e-02,\n",
       "                         5.8239e-02,  4.7303e-02,  3.8330e-02,  3.0714e-02,  6.8027e-02,\n",
       "                         5.1206e-02,  1.1197e-02,  5.0060e-02,  2.2199e-02,  3.3650e-02,\n",
       "                         6.7801e-02,  2.9059e-02,  2.5592e-02,  5.8378e-02,  2.8485e-03,\n",
       "                         4.1138e-02,  3.5451e-02,  5.6159e-02,  2.5670e-02,  6.7996e-02,\n",
       "                         3.0432e-02,  5.2035e-02,  1.9978e-02,  1.0741e-02,  3.9460e-02,\n",
       "                         5.8319e-03,  2.6897e-02,  3.7121e-02,  2.6295e-02,  3.4434e-02,\n",
       "                         3.2616e-02,  2.3259e-02,  3.0632e-03,  7.4687e-02,  3.0586e-02,\n",
       "                         4.6641e-02,  5.1360e-02,  3.2102e-02,  1.6519e-02,  3.6469e-02,\n",
       "                         4.1130e-02,  5.2228e-02,  2.7671e-02,  6.2952e-02, -2.3866e-03,\n",
       "                         5.2501e-02,  6.6695e-03,  7.3561e-02,  6.1228e-02,  7.0904e-02,\n",
       "                         1.4718e-02,  1.9975e-02,  6.3378e-02,  2.1474e-02,  7.2131e-02,\n",
       "                         7.1709e-02,  6.6591e-02,  2.4886e-02,  1.2938e-02,  7.9439e-03,\n",
       "                         3.1934e-02,  4.5226e-02,  7.0437e-02,  4.4487e-02,  5.3708e-02,\n",
       "                         3.2850e-02,  7.3796e-02,  1.1122e-02,  5.3305e-02,  6.4641e-02,\n",
       "                         6.0622e-02,  4.6062e-02,  2.6268e-02,  4.4033e-02, -2.2880e-03,\n",
       "                         1.0521e-02,  1.8722e-02,  7.7198e-02, -1.7430e-03,  7.0714e-02,\n",
       "                         6.7382e-02,  5.6957e-02,  4.8825e-02,  3.5602e-03,  3.4999e-02,\n",
       "                         1.8782e-02,  2.1563e-02,  1.1756e-02,  5.2336e-02, -1.2324e-03,\n",
       "                         3.5438e-02,  3.8705e-02,  8.8528e-03,  1.5055e-02,  4.3870e-02,\n",
       "                         5.3696e-02,  3.2199e-02,  6.7338e-02,  7.1268e-02,  7.8546e-02,\n",
       "                         3.9723e-02,  3.3295e-02,  5.6782e-02,  6.1809e-02,  4.3166e-02,\n",
       "                         6.8290e-02,  5.7991e-02,  5.8273e-02,  5.6745e-02,  4.0484e-02,\n",
       "                         4.1913e-02,  5.8098e-02,  3.7426e-02,  6.8795e-02,  5.5285e-02,\n",
       "                         6.2190e-02,  5.0762e-02,  2.5669e-02,  4.7072e-02,  3.7683e-02,\n",
       "                         3.3192e-03,  4.0232e-02,  7.9173e-02,  6.0424e-02,  4.7652e-02,\n",
       "                         6.7910e-02,  6.3143e-02,  3.2843e-02,  5.1391e-02,  3.0801e-02,\n",
       "                         1.7576e-02,  5.3683e-02,  1.5173e-02,  5.1588e-02,  2.4982e-02,\n",
       "                         3.2047e-02,  6.8584e-02,  1.5950e-02,  4.3919e-02,  1.0885e-02,\n",
       "                         3.0533e-02,  3.9781e-02,  4.3274e-02,  6.8215e-02,  5.4582e-02,\n",
       "                         3.6791e-02,  7.0441e-02,  1.1272e-02,  4.8272e-02,  3.6707e-02,\n",
       "                         5.6482e-02,  2.1983e-02,  2.3569e-02,  7.1630e-02,  6.5125e-02,\n",
       "                         8.3652e-02,  5.2886e-02,  6.1186e-02,  3.4791e-02,  3.7515e-02,\n",
       "                         7.7423e-02,  3.8625e-02,  4.7376e-02,  4.9762e-02,  5.6885e-02,\n",
       "                         2.2207e-02,  6.5058e-02,  3.4166e-02,  3.2890e-03,  2.9975e-03,\n",
       "                         4.8350e-02,  5.2808e-02,  3.9794e-02,  5.9502e-02,  2.0201e-02,\n",
       "                         5.8556e-02,  5.3217e-02,  2.6058e-02,  4.6718e-02,  1.0477e-02,\n",
       "                         2.1959e-02,  4.6659e-02,  3.6506e-02,  4.2561e-02,  4.9265e-02,\n",
       "                         4.8613e-02,  1.3413e-02,  2.0959e-02,  4.0583e-02,  6.5897e-02,\n",
       "                         5.6470e-02,  1.2217e-02,  2.4993e-02,  7.2960e-02,  9.9721e-03,\n",
       "                         6.6522e-02,  3.6646e-02,  6.8531e-02,  4.3811e-02,  2.1681e-02,\n",
       "                         1.2950e-02,  5.3444e-02,  5.9184e-02,  6.3427e-02,  1.6017e-02,\n",
       "                         2.3846e-02,  5.1764e-02,  3.0279e-02,  2.4702e-02,  8.1105e-03,\n",
       "                         2.5372e-02,  4.9509e-02,  3.3999e-02,  6.4314e-02,  4.3272e-02,\n",
       "                         7.3791e-02,  6.3606e-02,  3.2641e-03,  6.2404e-02,  3.5842e-02,\n",
       "                         5.6925e-03,  4.9324e-02,  3.2700e-02,  5.0359e-02,  1.8555e-02,\n",
       "                         2.8984e-02,  5.2713e-02,  2.1870e-02,  6.3170e-02,  4.4067e-02,\n",
       "                         4.7020e-02,  4.0375e-02,  8.3889e-03,  4.4203e-04,  3.0870e-02,\n",
       "                         5.0125e-02,  2.0838e-02,  4.8352e-02,  6.8440e-02,  5.1022e-02,\n",
       "                         9.0495e-03,  3.5674e-02,  5.3434e-03,  8.9219e-03,  6.9455e-02,\n",
       "                         2.0972e-02,  6.9786e-02,  6.1016e-03,  4.2292e-03,  1.6940e-02,\n",
       "                         4.8474e-02,  4.0964e-02,  4.2370e-02,  1.6830e-02,  2.4613e-02,\n",
       "                         6.0532e-02,  6.7282e-02,  6.1489e-02,  6.4270e-02,  3.7930e-02,\n",
       "                         5.0915e-02,  1.0338e-02,  8.5771e-03,  3.7206e-03,  6.5821e-02,\n",
       "                         8.2290e-03,  6.6442e-02, -4.1931e-03,  2.7152e-02,  5.0296e-02,\n",
       "                         5.8241e-02,  6.5061e-02,  7.0036e-02,  3.9362e-02,  4.2691e-02,\n",
       "                         1.9993e-02, -4.8794e-03,  4.0841e-02,  1.1340e-02,  3.7031e-02,\n",
       "                         4.4426e-02,  6.7266e-02,  4.5673e-02,  4.1564e-02,  4.1698e-02,\n",
       "                         3.4251e-02,  1.7811e-02,  2.3631e-03,  1.8440e-02,  4.7179e-02,\n",
       "                         1.7583e-02,  3.2557e-02,  5.3354e-02,  5.7313e-02,  1.6607e-02,\n",
       "                         4.7456e-02,  4.4339e-02,  1.4689e-02,  5.8940e-02,  7.1546e-02,\n",
       "                        -5.8600e-03,  6.7740e-02, -2.2195e-03,  4.5695e-02,  2.5728e-02,\n",
       "                         2.7165e-02,  1.2471e-02,  2.3468e-02,  8.2721e-03,  4.7795e-02,\n",
       "                         4.7589e-02,  2.6838e-02,  7.5146e-02,  1.5576e-03,  7.7662e-02,\n",
       "                         3.1868e-02,  6.3146e-02,  3.3956e-02,  4.1351e-02,  2.3498e-03,\n",
       "                         2.2349e-02,  1.0534e-02,  5.4240e-02,  3.7359e-02,  3.0305e-02,\n",
       "                         6.8444e-02,  1.5080e-02,  5.3708e-02,  3.7508e-02,  4.4151e-02,\n",
       "                         5.7466e-02,  7.4070e-02,  3.6721e-02,  2.7944e-02,  7.7630e-02,\n",
       "                         3.9192e-02,  9.4379e-03,  5.9667e-02,  2.6328e-02,  3.2080e-02,\n",
       "                         4.2128e-02,  1.9877e-02,  6.8832e-02,  2.4552e-02,  6.8843e-03,\n",
       "                         3.2229e-02,  7.6527e-02,  4.1490e-02,  4.5525e-02,  5.7216e-02,\n",
       "                         3.2112e-02,  5.5153e-02,  6.2152e-02,  6.8151e-02,  1.4437e-02,\n",
       "                         4.2754e-02,  4.5070e-02,  3.1589e-02,  4.5719e-02,  3.1471e-02,\n",
       "                         1.7597e-02,  5.8044e-02,  2.8991e-02,  6.8553e-02,  3.7681e-02,\n",
       "                        -8.9440e-04,  4.0124e-03,  1.0928e-02,  3.3416e-02,  4.8069e-02,\n",
       "                         6.8608e-02,  1.7942e-02,  3.9880e-02,  9.0672e-03,  5.6551e-02,\n",
       "                         1.1053e-02,  3.4534e-02,  4.5698e-02,  3.0563e-02,  2.6120e-02,\n",
       "                        -4.9003e-04,  7.1856e-02,  7.7879e-02,  5.8982e-02,  6.7376e-02,\n",
       "                         1.4608e-02,  4.3437e-02,  6.0555e-02,  5.3762e-03,  1.2003e-02,\n",
       "                         2.2141e-02,  3.6750e-02, -1.5311e-03,  3.8706e-02, -1.1227e-03,\n",
       "                         6.5274e-02,  2.3134e-02,  3.2669e-02,  3.5847e-03,  4.8788e-02,\n",
       "                         2.8525e-02,  6.7551e-02,  1.0037e-02,  4.3736e-02,  5.8278e-02,\n",
       "                         5.9608e-02,  3.6554e-02,  5.0838e-02,  6.9469e-02,  4.6980e-02,\n",
       "                         2.8742e-02,  2.6693e-02,  4.5051e-02,  1.2553e-02,  1.3857e-02,\n",
       "                         7.4897e-02,  5.7655e-03,  1.6859e-02,  5.8878e-02,  6.3127e-02,\n",
       "                         3.7925e-02,  4.2157e-02,  6.6524e-02,  3.1781e-02,  5.2921e-02,\n",
       "                         4.9813e-02,  2.6797e-03,  7.4680e-02,  1.6502e-02,  8.3106e-02,\n",
       "                         1.1685e-02,  1.4371e-02,  4.6309e-03,  2.9629e-02,  4.8589e-02,\n",
       "                         1.2615e-02,  7.3809e-02,  3.6036e-03,  2.1298e-02,  5.2874e-02,\n",
       "                         5.9320e-02,  1.1537e-02,  2.5273e-02,  4.0174e-02,  6.9898e-02,\n",
       "                         3.1919e-02,  2.9086e-02, -1.5622e-03,  6.4298e-03,  3.0002e-02,\n",
       "                         5.8775e-02,  2.1336e-02,  4.6651e-02,  5.6568e-02,  2.7939e-02,\n",
       "                         3.1303e-02,  3.7681e-02,  5.2301e-02,  7.7035e-03,  6.2409e-02,\n",
       "                         4.8532e-03,  7.6719e-02, -4.4810e-04,  7.1606e-02,  7.1010e-02,\n",
       "                         6.5139e-02,  5.2452e-02,  4.9996e-02,  4.5775e-03,  4.6239e-03,\n",
       "                         7.2140e-03,  4.4605e-02,  3.1069e-02,  1.4623e-02,  2.2778e-02,\n",
       "                         6.2927e-02,  8.4547e-03,  5.6753e-02,  3.8624e-02,  1.5409e-02,\n",
       "                         1.9014e-02,  2.6271e-02, -4.4908e-03,  1.3661e-02,  3.0860e-03,\n",
       "                         2.7717e-02,  6.7086e-02,  3.1293e-02,  2.4737e-02,  4.1141e-02,\n",
       "                         4.3319e-02,  2.2385e-02,  2.3144e-02,  5.2807e-02,  3.8041e-02,\n",
       "                         5.2514e-02,  2.4949e-02,  4.9267e-02,  2.2770e-02,  8.3398e-03,\n",
       "                         1.9407e-02,  1.0469e-02,  1.5385e-02,  6.3653e-02,  6.6311e-02,\n",
       "                         2.7835e-02,  5.0750e-02,  4.0080e-02,  5.3486e-03,  2.8413e-02,\n",
       "                         6.4209e-02,  3.6791e-02,  5.3177e-03,  1.8261e-02,  5.5284e-02,\n",
       "                         2.4864e-02,  4.7806e-02,  6.9205e-02,  5.9221e-02,  6.6764e-02,\n",
       "                         1.5294e-02,  3.6562e-02, -9.8489e-03,  7.4772e-02,  4.7796e-02,\n",
       "                         4.6418e-02,  2.4872e-02,  7.7914e-02,  2.6052e-02,  2.8509e-02,\n",
       "                         2.2841e-02,  6.9989e-02,  5.0292e-02,  3.7920e-02,  1.1777e-02,\n",
       "                         7.4676e-02, -5.6888e-03,  2.4984e-02,  2.3278e-02,  6.2786e-02,\n",
       "                         6.8759e-03,  4.9991e-02,  1.2397e-02,  3.8345e-02,  5.7645e-02,\n",
       "                         2.0753e-02,  4.5851e-02,  6.4974e-02,  2.0404e-02,  9.9946e-03,\n",
       "                         6.6271e-02,  2.9148e-02,  7.0356e-02,  7.5244e-03,  4.3925e-02,\n",
       "                         3.8012e-02,  7.6633e-02,  3.3452e-02,  6.9417e-02,  6.9101e-02,\n",
       "                         4.1640e-02, -7.4459e-04,  6.3535e-02,  3.3376e-02,  6.0088e-03,\n",
       "                         4.6898e-02,  2.6792e-02,  4.0404e-02,  7.7062e-02,  5.9073e-02,\n",
       "                         6.2460e-02,  1.8157e-02,  6.0816e-02,  2.6327e-02,  6.5033e-02,\n",
       "                         7.2450e-02,  6.7944e-02,  1.0965e-05,  7.0043e-03,  2.7701e-03,\n",
       "                         1.0776e-02,  5.8877e-02,  4.0650e-02,  6.0865e-02,  1.6837e-03,\n",
       "                         6.5468e-02,  4.6139e-02,  3.4625e-02,  6.1908e-02,  5.9247e-02,\n",
       "                         1.4966e-02,  4.8268e-02,  7.0565e-02,  1.8573e-02,  3.4102e-02,\n",
       "                         3.0984e-03,  2.2016e-02,  1.6035e-02,  1.8984e-02,  5.3142e-02,\n",
       "                         4.5420e-02,  1.5959e-02,  5.4922e-02,  3.8536e-02,  5.3210e-02,\n",
       "                         4.6028e-02,  7.1283e-02,  3.5592e-02,  6.3492e-02,  7.4372e-03,\n",
       "                         6.5251e-02,  7.2939e-02,  4.2796e-02,  6.1040e-02,  3.5612e-02,\n",
       "                         1.2290e-02,  4.9414e-02,  7.5646e-03,  3.1779e-02,  1.2175e-02,\n",
       "                         9.8786e-03,  6.1574e-02,  2.3852e-02,  3.1346e-02,  7.1003e-02,\n",
       "                         3.5094e-02,  1.0216e-02,  3.3625e-02,  4.4549e-02,  3.4668e-02,\n",
       "                         2.1920e-02,  4.9077e-02,  3.2537e-02,  5.0730e-02,  7.4908e-02,\n",
       "                         3.0913e-02,  2.5942e-02,  4.3566e-02,  9.4009e-03,  2.7813e-02,\n",
       "                         2.2261e-03,  9.1789e-03,  9.2724e-03,  6.8778e-02,  6.1478e-02,\n",
       "                         3.6913e-02,  4.9422e-02,  1.7053e-02,  5.2083e-02,  9.3869e-03,\n",
       "                        -5.0681e-04,  1.2070e-02,  6.8467e-02,  4.1851e-02,  3.1279e-02,\n",
       "                         2.3096e-02,  2.5240e-02,  5.7671e-02,  5.3039e-02,  1.9965e-02,\n",
       "                         1.6621e-02,  5.0485e-02,  5.7904e-02,  5.6652e-02,  6.1571e-02,\n",
       "                         3.0548e-02,  6.7628e-03,  5.2970e-02,  1.7385e-03,  1.0098e-02,\n",
       "                         1.4191e-02,  5.1033e-02, -6.1100e-03,  2.2170e-02,  6.4102e-02,\n",
       "                         6.8279e-02,  4.8405e-02,  2.2274e-02,  4.4223e-02,  4.6322e-02,\n",
       "                         7.3221e-02,  5.4531e-02,  4.4182e-02,  3.9409e-02,  6.5892e-02,\n",
       "                         1.5077e-02,  2.6761e-03,  7.4718e-02,  1.0768e-02,  5.4732e-02,\n",
       "                         3.0827e-02,  1.3982e-02,  1.3063e-02,  1.5887e-02,  5.6674e-02,\n",
       "                         1.9492e-02,  1.6043e-02,  7.0630e-02,  5.4955e-02,  3.6167e-02,\n",
       "                         6.7482e-02,  2.0058e-02,  1.8753e-02,  5.8343e-02,  5.9835e-02,\n",
       "                         3.5757e-02,  6.5493e-02,  8.1915e-03,  6.9928e-02,  2.5920e-03,\n",
       "                         6.0569e-02,  1.2113e-02,  2.4924e-02,  5.5705e-02,  5.9487e-02,\n",
       "                         1.6482e-02,  8.7571e-03,  4.1305e-02,  4.4001e-02,  1.7456e-02,\n",
       "                         6.3310e-02,  7.0838e-02,  7.0063e-02,  7.6870e-02,  6.0772e-02,\n",
       "                         2.0277e-02,  4.0761e-02,  7.4498e-02,  4.0816e-02,  1.1742e-02,\n",
       "                         4.6808e-02,  5.3873e-02,  2.6617e-02,  5.9961e-02,  5.0300e-03,\n",
       "                         2.9918e-02,  5.6600e-02,  6.3563e-02,  2.5718e-02,  5.0104e-02,\n",
       "                         1.8517e-02,  6.4455e-02,  6.7884e-02,  2.3393e-02,  7.7464e-02,\n",
       "                         2.1962e-02,  5.4341e-02,  2.9977e-03,  6.2254e-02,  6.1610e-02,\n",
       "                         6.2092e-02,  4.9948e-02,  8.3113e-02,  6.0832e-02,  7.4429e-02,\n",
       "                         4.3876e-02,  3.2847e-02,  2.4132e-03,  6.9028e-02,  2.6118e-02,\n",
       "                         4.4518e-02,  4.3724e-02,  4.4435e-02,  2.2938e-02,  3.8621e-02,\n",
       "                         1.3544e-02,  3.7540e-02,  3.6761e-02,  5.3193e-02,  2.6664e-03,\n",
       "                         5.4711e-02,  5.0002e-02,  1.5448e-02,  3.2341e-02,  5.9190e-02,\n",
       "                         5.8883e-02,  1.0195e-02,  7.0208e-02,  3.8520e-02,  2.4133e-02,\n",
       "                         6.0802e-02,  6.6619e-02,  5.7534e-02,  3.7564e-02,  4.7095e-02,\n",
       "                         9.9036e-03,  4.4532e-02,  6.4396e-02,  2.5990e-02,  3.1874e-02,\n",
       "                         1.0683e-02,  6.2140e-02,  6.6914e-02,  2.1393e-02,  5.3216e-02,\n",
       "                         3.1142e-02,  1.8797e-02,  6.0016e-02,  1.5554e-02,  3.8772e-02,\n",
       "                         5.9200e-03,  1.5193e-02,  4.3928e-02,  6.9773e-02,  4.8498e-03,\n",
       "                         5.3941e-02,  1.2157e-02,  4.0107e-02,  5.0533e-02,  2.4678e-02,\n",
       "                         7.3954e-02,  1.8926e-02,  3.3672e-02,  1.0699e-02,  6.4875e-02,\n",
       "                         7.7249e-02,  1.7562e-02,  3.2517e-02,  6.9576e-02,  3.0102e-02,\n",
       "                         3.1049e-02,  2.4689e-02,  5.5701e-02,  4.4943e-02,  9.8866e-03,\n",
       "                         5.5633e-02,  4.7923e-02,  4.5609e-02,  6.9154e-02,  1.7761e-03,\n",
       "                         4.9671e-02,  3.3794e-02,  3.7107e-02,  3.0764e-02,  7.4743e-02,\n",
       "                         1.2243e-02,  1.7174e-02,  4.1426e-02,  3.5400e-02,  5.8723e-02,\n",
       "                         3.2902e-02,  3.3691e-02,  7.0308e-02,  6.5955e-02,  4.2364e-02,\n",
       "                         1.4820e-02,  5.9063e-02,  4.9287e-02,  3.8345e-02,  3.1389e-02,\n",
       "                         2.5615e-02,  5.1978e-02,  5.5598e-03,  2.7111e-02,  3.9602e-02,\n",
       "                         1.1205e-02,  3.1094e-02,  1.8317e-02,  7.2591e-02,  1.5988e-02,\n",
       "                         6.8165e-02,  3.4817e-02,  2.2996e-02,  4.5762e-02,  1.0752e-02,\n",
       "                         2.5216e-02,  6.5980e-02,  4.4482e-03,  6.2396e-02,  8.6627e-03,\n",
       "                         4.1778e-02,  2.4949e-02,  5.7852e-02,  2.5335e-02,  6.9082e-02,\n",
       "                         2.1938e-02,  5.6281e-02,  6.3051e-02,  4.4045e-03,  5.5214e-02,\n",
       "                         9.2764e-03,  1.0810e-02,  2.8265e-02,  6.0916e-02,  4.0609e-02,\n",
       "                         6.7090e-02,  7.1581e-03,  5.5606e-02,  6.6362e-03,  2.9108e-02,\n",
       "                         1.2346e-02,  4.9618e-02,  3.4270e-02,  1.3116e-02,  3.1012e-02,\n",
       "                        -2.4976e-03,  1.2801e-02,  6.3032e-04,  1.5666e-02,  7.2645e-02,\n",
       "                         2.6552e-02,  1.7833e-02,  7.2155e-02,  4.6844e-03,  2.3135e-02,\n",
       "                         3.9778e-02,  3.1699e-02,  4.0886e-02,  6.6107e-02,  6.3330e-02,\n",
       "                         7.2558e-02,  5.8074e-02,  4.2731e-02,  5.3549e-02,  5.2330e-02,\n",
       "                         9.5755e-03,  4.6989e-02,  4.5155e-02,  2.4815e-02,  9.3283e-03,\n",
       "                         5.5389e-02,  8.5950e-03,  3.6354e-02,  4.2127e-02,  4.9384e-02,\n",
       "                         2.6752e-02,  2.1741e-02,  3.3166e-02,  5.7889e-02,  2.3291e-02,\n",
       "                         2.2826e-03,  2.9139e-02,  2.4649e-02,  5.4908e-02,  2.3865e-02,\n",
       "                         1.7144e-03,  3.8390e-02,  1.6311e-02,  5.3535e-02,  2.4612e-02,\n",
       "                         2.6815e-02,  4.8798e-02,  4.9636e-02,  3.6788e-02,  5.2544e-02,\n",
       "                         4.2396e-02,  1.3750e-02,  4.7061e-02,  4.4669e-02,  7.5676e-03,\n",
       "                         6.2254e-02,  4.9232e-03,  4.8443e-02,  3.9520e-02,  5.0283e-04,\n",
       "                         6.3416e-02,  6.2228e-02,  2.1677e-02,  3.4289e-02,  5.2122e-02,\n",
       "                         2.7141e-02,  4.0235e-02,  4.5752e-02,  2.6857e-02,  5.5995e-02,\n",
       "                         5.8241e-02,  1.0257e-02,  6.0350e-02,  4.8127e-02,  6.8376e-02,\n",
       "                         6.4607e-02,  1.0683e-02], device='cuda:0'))]),\n",
       "  'targetModel_state_dict': OrderedDict([('fc1.weight',\n",
       "                tensor([[-0.0643, -0.0060, -0.0968,  ...,  0.0024,  0.0515,  0.0332],\n",
       "                        [ 0.0447, -0.0436, -0.0284,  ...,  0.1375,  0.0381, -0.0696],\n",
       "                        [ 0.0180, -0.0077,  0.0272,  ...,  0.0707,  0.0670, -0.0595],\n",
       "                        ...,\n",
       "                        [-0.1327, -0.2020, -0.0895,  ...,  0.0465,  0.0284, -0.0021],\n",
       "                        [-0.0212, -0.0236, -0.0505,  ...,  0.0525,  0.0787,  0.1222],\n",
       "                        [ 0.0060, -0.0251,  0.0265,  ...,  0.0065, -0.1026, -0.1370]],\n",
       "                       device='cuda:0')),\n",
       "               ('fc1.bias',\n",
       "                tensor([-0.0034,  0.0150, -0.0583, -0.0317, -0.0089, -0.0281, -0.0411, -0.0656,\n",
       "                        -0.0332,  0.0464,  0.1508,  0.1326, -0.0333,  0.0468,  0.0047, -0.1089,\n",
       "                         0.1466,  0.0084,  0.1331,  0.0831,  0.1657, -0.0266,  0.1738, -0.0531,\n",
       "                         0.0339,  0.1062,  0.0911,  0.1590,  0.0981, -0.0713, -0.0957, -0.0262,\n",
       "                        -0.0148,  0.0597, -0.0004,  0.1098,  0.0798, -0.0264,  0.1218,  0.1639,\n",
       "                         0.0349, -0.0542,  0.0681,  0.0429,  0.0134, -0.0545,  0.1146, -0.0190,\n",
       "                         0.0776,  0.0023, -0.0876, -0.0770, -0.0621,  0.0749, -0.0580,  0.0891,\n",
       "                         0.1087, -0.0505, -0.0228,  0.0439, -0.0076,  0.1351,  0.0265, -0.0304,\n",
       "                         0.1836, -0.0954,  0.0930,  0.0885,  0.1360, -0.1183, -0.0200, -0.0156,\n",
       "                         0.0951, -0.1056,  0.0275, -0.0487,  0.1428,  0.1083,  0.0705, -0.0396,\n",
       "                         0.0063,  0.0678, -0.0078,  0.0593,  0.0585,  0.0387, -0.0585,  0.1397,\n",
       "                         0.0714,  0.1008,  0.0481,  0.0305,  0.0645, -0.0383,  0.1123,  0.1194,\n",
       "                        -0.0582, -0.0225,  0.1318, -0.0467,  0.0270, -0.0461, -0.0183,  0.0090,\n",
       "                         0.1209,  0.1561,  0.0861,  0.0449,  0.0128, -0.0278,  0.0533,  0.0221,\n",
       "                        -0.0091, -0.0850, -0.0146,  0.1546, -0.0409,  0.0295,  0.0551,  0.0280,\n",
       "                         0.0124,  0.0115, -0.0268,  0.0664,  0.0548,  0.1772,  0.1014,  0.0748,\n",
       "                        -0.0628, -0.0393, -0.0531,  0.0553, -0.0627,  0.1343,  0.1044,  0.0689,\n",
       "                        -0.0737, -0.0052,  0.0843, -0.0775,  0.0341, -0.0482,  0.1261, -0.0145,\n",
       "                        -0.1000,  0.0151,  0.1352,  0.1123,  0.0201,  0.0280,  0.1506,  0.0658,\n",
       "                         0.1220,  0.0246,  0.0385, -0.0188,  0.0309,  0.1457,  0.0415,  0.1361,\n",
       "                         0.0272,  0.0716,  0.0027,  0.0395, -0.0238,  0.0807, -0.0866,  0.0331,\n",
       "                        -0.0595,  0.1385, -0.0217, -0.0590,  0.0647, -0.0472,  0.1031,  0.1167,\n",
       "                         0.0961,  0.0280,  0.0161,  0.0680,  0.0469, -0.0532,  0.0018,  0.0329,\n",
       "                         0.0190, -0.0505, -0.0085,  0.0960,  0.0367,  0.1188, -0.0060,  0.0343,\n",
       "                         0.0705,  0.0595,  0.0091, -0.0549,  0.0281,  0.0020, -0.0040, -0.0037,\n",
       "                         0.0343,  0.1439, -0.0292,  0.0794,  0.1123, -0.0761, -0.0477,  0.0452,\n",
       "                         0.1416,  0.0832,  0.0217, -0.0460, -0.0447, -0.0326,  0.0799, -0.0970,\n",
       "                         0.1233,  0.2008,  0.0461,  0.1137,  0.0187,  0.1501, -0.0374,  0.0677,\n",
       "                         0.1470, -0.0798, -0.0063,  0.1071,  0.0798, -0.0931,  0.0756,  0.0726,\n",
       "                         0.0803, -0.0840, -0.0693,  0.0571,  0.0926,  0.0967,  0.1128,  0.0647,\n",
       "                         0.1112,  0.0529, -0.0387,  0.0413,  0.1187,  0.0240,  0.0714,  0.0713,\n",
       "                         0.0059,  0.0261,  0.1684,  0.1339,  0.0249,  0.0602, -0.1029,  0.0794],\n",
       "                       device='cuda:0')),\n",
       "               ('fc2.weight',\n",
       "                tensor([[ 0.1451, -0.0004, -0.0545,  ...,  0.1024,  0.2482,  0.0184],\n",
       "                        [ 0.1621, -0.0041, -0.0094,  ...,  0.0786,  0.1548, -0.0264],\n",
       "                        [ 0.1016, -0.0363,  0.0147,  ...,  0.1048,  0.2262,  0.0270],\n",
       "                        ...,\n",
       "                        [ 0.1621,  0.0180, -0.0513,  ...,  0.1245,  0.2557, -0.0013],\n",
       "                        [ 0.1411,  0.0179, -0.0245,  ...,  0.1059,  0.2368,  0.0125],\n",
       "                        [ 0.0968,  0.0775, -0.0678,  ...,  0.0567,  0.2562, -0.0537]],\n",
       "                       device='cuda:0')),\n",
       "               ('fc2.bias',\n",
       "                tensor([ 0.0599,  0.0363, -0.0145,  0.0417, -0.0444, -0.0236,  0.0094,  0.0160,\n",
       "                         0.0807,  0.0374, -0.0059,  0.0807,  0.0840,  0.0841,  0.0022,  0.0510,\n",
       "                         0.0576, -0.0542, -0.0239, -0.0172,  0.0037, -0.0020,  0.0790,  0.0606,\n",
       "                         0.0230,  0.0243, -0.1461,  0.0409, -0.0228,  0.0569,  0.0510,  0.0860,\n",
       "                        -0.0220,  0.0737, -0.0244,  0.0173,  0.0179,  0.0240, -0.0019,  0.0254,\n",
       "                         0.0328,  0.0084,  0.0586,  0.0514, -0.0134,  0.0780,  0.0023,  0.0534,\n",
       "                         0.0781, -0.0231,  0.0190, -0.0228,  0.0247,  0.0068,  0.0360, -0.0271,\n",
       "                         0.0735,  0.0395, -0.0332,  0.0337,  0.0294, -0.0034,  0.0268,  0.0412,\n",
       "                        -0.0086,  0.0640,  0.0886,  0.0557,  0.0408,  0.0355,  0.0952, -0.0352,\n",
       "                         0.0714, -0.0030, -0.0263,  0.0514,  0.0424,  0.0645,  0.0765, -0.0016,\n",
       "                         0.0842, -0.0011,  0.0826,  0.0233, -0.0102,  0.0457,  0.0239,  0.0685,\n",
       "                         0.0682,  0.0327,  0.0424,  0.0212,  0.0686, -0.0191,  0.0188,  0.0315,\n",
       "                        -0.0064,  0.0537, -0.0204,  0.0444, -0.1745,  0.0331,  0.0210, -0.0123,\n",
       "                         0.0483,  0.0232,  0.0331, -0.0029,  0.0765, -0.0143,  0.0292,  0.0027,\n",
       "                         0.0810,  0.0586, -0.0392,  0.0217,  0.0857, -0.0248, -0.0050, -0.0232,\n",
       "                        -0.0194, -0.0220,  0.0248,  0.0702, -0.0169, -0.0137,  0.0156,  0.0267,\n",
       "                         0.0785, -0.0012,  0.0321,  0.0887,  0.0341,  0.0320, -0.0036,  0.0451,\n",
       "                         0.0105, -0.0031, -0.0154, -0.0192,  0.0851, -0.0003,  0.0356,  0.0026,\n",
       "                        -0.0049,  0.0502,  0.0614, -0.0244, -0.0294,  0.0177, -0.0095, -0.0357,\n",
       "                        -0.1115,  0.0298,  0.0278, -0.0427,  0.0613,  0.0538, -0.0494, -0.0190,\n",
       "                        -0.0121,  0.0063,  0.0819,  0.0137,  0.0493,  0.0772,  0.0787,  0.0329,\n",
       "                         0.0535, -0.0238, -0.0249,  0.0771,  0.0821,  0.0196,  0.0592,  0.0470,\n",
       "                        -0.0072, -0.0116, -0.0212,  0.0621,  0.0282,  0.0167,  0.0124,  0.0888,\n",
       "                         0.0772,  0.0402,  0.0117,  0.0514,  0.0043,  0.0279,  0.0187,  0.0149,\n",
       "                         0.0345,  0.0820,  0.0919,  0.0523,  0.0153,  0.0793,  0.0373,  0.0427,\n",
       "                         0.0914,  0.0364, -0.0555,  0.0422, -0.0099,  0.0537,  0.0205,  0.0629,\n",
       "                         0.0283,  0.0211,  0.0194,  0.0936,  0.0046,  0.0777, -0.0213,  0.0956,\n",
       "                         0.0787, -0.0131, -0.0110, -0.0138, -0.0233,  0.0383,  0.0919,  0.0147,\n",
       "                         0.0643,  0.0068, -0.0146,  0.0673,  0.0278,  0.0508,  0.0550, -0.0395,\n",
       "                         0.0832,  0.0554,  0.0311, -0.0395,  0.0486,  0.0871,  0.0558,  0.0565,\n",
       "                         0.0628,  0.0509,  0.0720,  0.0705,  0.0257,  0.0702,  0.0019, -0.0102,\n",
       "                        -0.0166,  0.0163,  0.0512,  0.0362,  0.0775, -0.0128,  0.0803,  0.0664],\n",
       "                       device='cuda:0')),\n",
       "               ('fc3.weight',\n",
       "                tensor([[-0.0065,  0.1011,  0.0446,  ...,  0.0441,  0.0681,  0.0783],\n",
       "                        [ 0.0056, -0.0021,  0.0515,  ...,  0.0689,  0.0077,  0.0456],\n",
       "                        [ 0.0218,  0.0248,  0.0944,  ...,  0.1132, -0.0050,  0.0028],\n",
       "                        ...,\n",
       "                        [-0.0387,  0.0168, -0.0225,  ...,  0.0276,  0.0560, -0.0369],\n",
       "                        [ 0.0661,  0.0215,  0.0510,  ...,  0.0712,  0.0283,  0.0390],\n",
       "                        [ 0.0183,  0.0481,  0.0685,  ...,  0.1108,  0.0100, -0.0004]],\n",
       "                       device='cuda:0')),\n",
       "               ('fc3.bias',\n",
       "                tensor([ 6.7705e-02, -7.8271e-03,  9.1828e-02, -5.0688e-02,  5.0826e-02,\n",
       "                         3.5116e-02,  9.7673e-02, -1.8405e-04,  7.9428e-02, -3.5114e-02,\n",
       "                         7.8664e-03,  9.6581e-02,  3.9408e-02, -4.9654e-02,  3.8385e-02,\n",
       "                         6.4650e-02, -6.6670e-02,  7.9800e-02, -4.9197e-03,  2.1363e-02,\n",
       "                         4.5978e-02,  2.3419e-03,  3.0950e-04,  1.0559e-01,  4.4936e-02,\n",
       "                         5.4774e-02,  4.7226e-04,  9.6348e-02, -4.9697e-03,  4.4842e-02,\n",
       "                         4.4913e-02,  6.6349e-02, -2.2428e-02,  3.7031e-02,  4.1599e-02,\n",
       "                         2.4935e-03,  8.8754e-02, -5.3261e-02,  8.3122e-02, -5.0670e-02,\n",
       "                         8.3867e-02,  5.0991e-02, -1.2641e-02,  4.1992e-02,  5.9807e-02,\n",
       "                        -4.1193e-02,  2.2647e-02, -3.4714e-03,  8.1056e-02,  3.1432e-03,\n",
       "                         1.3793e-02, -3.2087e-02, -1.9500e-02, -4.1798e-02,  7.1161e-02,\n",
       "                         2.2532e-02,  7.3990e-03,  5.5943e-04,  1.0258e-01,  6.4331e-02,\n",
       "                         7.8370e-02,  1.0353e-01, -4.1222e-02,  5.0239e-03,  9.4295e-02,\n",
       "                        -3.1448e-02, -2.1055e-02,  6.0554e-02, -5.2680e-05, -1.6995e-02,\n",
       "                         6.5162e-02,  7.4660e-02, -4.4937e-02,  6.4808e-02, -3.0396e-02,\n",
       "                         2.0630e-02, -1.9053e-02,  8.2143e-02,  4.8126e-02,  8.6662e-02,\n",
       "                         2.6490e-02,  4.3103e-02, -6.9855e-03, -1.9933e-02,  9.8552e-03,\n",
       "                        -2.9201e-02,  5.7709e-02, -4.1749e-02,  8.2388e-02, -7.8159e-03,\n",
       "                         6.2147e-02,  9.9741e-02, -1.6340e-02,  9.7407e-02,  9.6378e-02,\n",
       "                        -5.6174e-02,  5.9441e-03,  1.7980e-02,  5.5334e-02,  8.3207e-02,\n",
       "                        -3.1619e-02,  2.0028e-02, -2.5443e-02, -2.1105e-02,  2.5669e-03,\n",
       "                         6.0031e-03,  8.4860e-03,  7.0199e-02,  8.5642e-02, -8.2769e-03,\n",
       "                         2.1481e-02,  6.0939e-02, -3.2030e-02,  1.1085e-02, -5.2639e-03,\n",
       "                         8.6092e-02,  8.3758e-02,  6.6857e-02, -2.2494e-02, -2.8956e-03,\n",
       "                         2.1291e-02,  5.9081e-02, -1.8426e-02,  6.1610e-02,  7.5241e-02,\n",
       "                         3.9773e-02,  8.5725e-02, -7.3203e-05,  2.2882e-02,  4.9318e-02,\n",
       "                         6.7593e-02,  1.6675e-03,  5.0411e-02,  9.9202e-02, -7.5235e-03,\n",
       "                         1.3156e-02,  4.2236e-02,  1.5647e-02,  6.6901e-02,  1.0597e-02,\n",
       "                         9.1926e-02, -5.3875e-02, -9.8965e-03,  5.5788e-02,  3.0823e-02,\n",
       "                         6.3902e-02,  3.3920e-02,  6.3564e-02,  2.2767e-02, -5.7573e-03,\n",
       "                         9.3601e-02, -5.8611e-02,  3.1256e-02,  8.8157e-02,  5.3269e-02,\n",
       "                        -9.3086e-03,  4.3062e-02,  6.6895e-02,  9.6279e-02,  5.7093e-03,\n",
       "                         3.8053e-02, -1.9180e-03,  1.3602e-02,  5.5835e-02,  7.6212e-03,\n",
       "                         2.0985e-02,  9.0285e-02, -4.5478e-03, -6.1018e-02, -2.3202e-02,\n",
       "                         4.4860e-02,  4.0708e-02,  7.4628e-02, -3.0232e-02,  1.4144e-02,\n",
       "                         1.3474e-02,  2.0843e-02,  1.3985e-02,  7.3776e-02,  7.7731e-02,\n",
       "                         8.7605e-02,  6.4183e-02, -2.1281e-02,  9.5029e-02,  5.6802e-02,\n",
       "                        -5.7019e-02,  1.0633e-01, -4.4200e-03,  7.5592e-02,  1.8050e-02,\n",
       "                         7.6417e-02,  4.6236e-02,  7.4131e-02,  7.6907e-02,  4.1906e-02,\n",
       "                         5.5375e-02,  5.1038e-03, -2.0680e-02,  4.3334e-02, -1.3529e-02,\n",
       "                         9.4398e-02, -3.7629e-02, -4.5577e-02,  2.3524e-02,  5.4507e-02,\n",
       "                         1.5821e-02,  3.0007e-03,  6.0992e-04,  3.4325e-02,  1.0129e-02,\n",
       "                        -2.7254e-03,  1.0468e-01, -1.5876e-02,  5.6492e-02, -5.0578e-02,\n",
       "                        -3.0805e-02, -5.1963e-02,  3.3250e-02,  7.8004e-02, -5.6316e-02,\n",
       "                         7.5644e-02, -1.8974e-03,  1.4290e-02, -1.8896e-02,  3.1192e-02,\n",
       "                         7.5429e-02,  7.1585e-02, -3.9482e-02,  3.0660e-02,  9.6238e-02,\n",
       "                        -1.3446e-03, -2.9857e-02,  7.7575e-02, -2.7355e-02,  6.8309e-02,\n",
       "                         7.9515e-03, -2.0452e-03, -3.9344e-02, -3.9624e-02,  8.2097e-02,\n",
       "                         6.2353e-02,  2.2776e-02,  5.9708e-02,  5.9127e-02, -1.2389e-03,\n",
       "                         1.0325e-01, -3.4171e-02, -3.4972e-02,  7.1586e-02,  8.0193e-02,\n",
       "                         7.1522e-02,  6.7850e-02,  3.4490e-02, -3.2543e-02, -1.8730e-02,\n",
       "                         6.9179e-02], device='cuda:0')),\n",
       "               ('fc4.weight',\n",
       "                tensor([[ 0.1502,  0.1310,  0.1433,  ..., -0.0426,  0.1228,  0.0929],\n",
       "                        [ 0.0944,  0.0735,  0.0705,  ...,  0.0390,  0.1371,  0.1363],\n",
       "                        [ 0.0581,  0.1465,  0.0519,  ..., -0.0511,  0.0434,  0.0948],\n",
       "                        ...,\n",
       "                        [ 0.0782,  0.0935,  0.1142,  ...,  0.0164,  0.1015,  0.1180],\n",
       "                        [ 0.0709,  0.1341,  0.0622,  ..., -0.0487,  0.1373,  0.0889],\n",
       "                        [ 0.0695,  0.1196,  0.0677,  ..., -0.0347,  0.0886,  0.0375]],\n",
       "                       device='cuda:0')),\n",
       "               ('fc4.bias',\n",
       "                tensor([ 9.7553e-02,  7.6601e-02,  1.3472e-02,  1.7982e-02,  1.4770e-01,\n",
       "                         2.4274e-01, -2.2728e-02,  4.3701e-02,  1.0090e-01,  1.0571e-01,\n",
       "                         9.0061e-02,  3.2470e-02,  7.8286e-02,  1.5594e-02,  1.0160e-01,\n",
       "                        -3.8960e-03,  9.5038e-02,  1.9801e-02,  4.2122e-02, -1.5946e-02,\n",
       "                         5.8534e-02,  3.8324e-02,  1.3719e-01, -6.9716e-03,  2.3334e-02,\n",
       "                        -4.6426e-02, -3.1099e-02, -4.2742e-02,  9.0892e-02,  1.8632e-01,\n",
       "                         5.3054e-02,  5.4202e-02,  7.6445e-02,  2.3582e-02,  9.5201e-02,\n",
       "                         2.3368e-02,  8.2415e-02,  3.2893e-02, -1.3682e-02,  6.6562e-02,\n",
       "                        -5.2270e-02,  5.9959e-02, -7.6388e-03, -9.7654e-03,  1.3170e-01,\n",
       "                        -6.0419e-02,  9.1717e-03, -4.2071e-03,  5.1760e-02,  6.2425e-02,\n",
       "                        -2.9432e-02,  1.6465e-01,  7.5948e-02,  4.7835e-02,  7.1324e-02,\n",
       "                         5.7066e-02, -4.9800e-03,  1.0852e-01,  1.0161e-01,  7.9005e-02,\n",
       "                        -1.6801e-02,  7.1831e-02,  6.1592e-03,  1.7825e-01,  4.0578e-02,\n",
       "                         4.9935e-02,  3.4070e-02,  7.4671e-02, -1.9086e-02, -5.4088e-02,\n",
       "                         9.6737e-02,  4.0193e-02,  5.0287e-02,  8.0949e-02,  6.1543e-02,\n",
       "                         2.6030e-02, -1.6226e-03,  8.4770e-02, -2.7151e-02,  9.6757e-02,\n",
       "                         8.0618e-02, -2.0518e-02, -5.4284e-02,  6.0656e-02,  1.7944e-02,\n",
       "                         4.8221e-02, -5.0831e-02,  1.9229e-01,  5.2907e-02, -2.9252e-02,\n",
       "                        -5.1512e-02, -4.7620e-02,  4.0680e-02, -7.9636e-03,  4.0598e-02,\n",
       "                         5.9393e-02,  4.6063e-02,  4.5291e-02,  8.2191e-02,  8.5280e-02,\n",
       "                         5.6807e-02,  5.8603e-02,  1.0273e-01,  2.4756e-02, -2.2505e-02,\n",
       "                         3.1370e-02,  3.8419e-02,  7.4358e-04,  9.9729e-02,  8.2035e-02,\n",
       "                         2.2334e-02, -6.8811e-03, -5.4084e-02, -5.2792e-02,  3.1844e-02,\n",
       "                         4.6030e-02,  4.4536e-02,  2.2820e-02,  9.1005e-02,  3.5548e-02,\n",
       "                         8.4975e-02, -8.9828e-03,  8.5283e-02,  2.7142e-01,  1.1608e-01,\n",
       "                         2.4126e-01,  1.0747e-01,  3.9820e-02, -5.5224e-02,  6.7079e-02,\n",
       "                         3.3250e-02,  7.9131e-02,  3.6118e-02,  7.1335e-02,  9.0913e-02,\n",
       "                         6.1424e-02,  4.5920e-02,  8.5122e-02,  6.2345e-02,  9.2077e-02,\n",
       "                         7.2057e-02,  4.5273e-02, -4.5772e-02,  1.7482e-02,  4.7422e-02,\n",
       "                         8.6548e-02,  5.6284e-02,  9.8011e-02,  1.6962e-02,  9.1542e-02,\n",
       "                         9.0881e-02,  1.0415e-02,  9.6958e-02, -4.8667e-02,  8.3721e-02,\n",
       "                         1.1515e-01,  1.0570e-01, -4.8010e-02,  4.5047e-02, -1.4281e-02,\n",
       "                        -2.3322e-02,  7.8518e-02, -1.3310e-02, -3.4048e-02,  4.6343e-02,\n",
       "                         6.7005e-02,  2.7111e-03,  4.8363e-02,  9.2318e-02,  8.9280e-02,\n",
       "                         2.5134e-01, -5.8237e-02, -2.5164e-02,  8.0974e-02,  7.8435e-02,\n",
       "                        -9.1144e-03,  1.0113e-01,  4.7270e-02,  3.8837e-02,  1.0077e-01,\n",
       "                         4.2794e-03,  7.8759e-03,  8.4284e-02, -1.2613e-02,  1.0613e-01,\n",
       "                         7.4266e-02,  5.7868e-02,  9.6713e-02,  4.6782e-02,  8.4058e-02,\n",
       "                         1.2297e-02, -2.8659e-02,  7.1890e-03,  6.1618e-02,  1.3515e-01,\n",
       "                         9.6991e-02,  8.5338e-02,  5.1661e-02,  2.3086e-01,  4.5328e-02,\n",
       "                         5.7012e-02,  1.7618e-01,  7.2051e-02,  2.0075e-02,  9.9328e-02,\n",
       "                         6.6560e-02,  9.8794e-02, -4.6163e-02,  6.6453e-02,  7.5828e-02,\n",
       "                         4.7508e-02, -3.6169e-02,  2.2966e-02,  4.5074e-02,  7.9162e-02,\n",
       "                         5.5888e-02, -3.0055e-02,  6.6313e-02, -2.3465e-03, -4.8908e-02,\n",
       "                         7.5645e-02,  1.5897e-01,  6.8189e-04,  2.4564e-02, -4.9644e-02,\n",
       "                        -7.7062e-03,  7.8621e-02,  5.0602e-02,  4.8174e-02,  4.5780e-02,\n",
       "                         7.3807e-02,  5.8838e-02,  3.3630e-02, -3.8788e-02,  1.0753e-01,\n",
       "                         3.3280e-02,  6.2785e-02,  9.8675e-02, -4.8311e-02,  2.3794e-02,\n",
       "                         1.1803e-02,  1.1940e-03,  9.6789e-02,  8.7650e-02,  2.3162e-01,\n",
       "                         7.2839e-02,  7.8364e-02,  6.6447e-02,  6.3047e-02,  5.8231e-02,\n",
       "                        -4.6908e-02,  1.9259e-01,  5.8270e-02,  7.5079e-02,  3.3640e-02,\n",
       "                         4.1575e-02,  5.4414e-02, -5.0670e-03,  6.7203e-02,  9.9807e-02,\n",
       "                        -1.3743e-05,  3.3413e-02, -3.1078e-02,  6.2862e-02,  9.6162e-02,\n",
       "                         2.9149e-03, -5.1309e-02, -4.1530e-02,  8.6841e-02,  7.7022e-02,\n",
       "                        -5.1663e-02,  7.2214e-02,  9.9231e-02,  1.4981e-01,  7.0731e-02,\n",
       "                         3.0695e-02,  6.2061e-02, -5.6716e-02,  6.5562e-02, -3.4274e-02,\n",
       "                         1.2480e-02,  9.3353e-02,  1.9328e-01,  2.0744e-02,  9.3229e-03,\n",
       "                         2.4442e-02, -3.8335e-02, -4.5468e-02, -5.1327e-02,  1.0085e-01,\n",
       "                         1.0732e-01,  5.2781e-02,  2.3885e-02,  1.0139e-02, -2.3128e-02,\n",
       "                         6.1918e-03,  4.2116e-02,  2.2728e-02, -4.9802e-02, -4.5001e-02,\n",
       "                         1.4657e-02,  6.0645e-02,  1.2747e-02,  8.0182e-02,  7.6132e-03,\n",
       "                         4.5194e-02,  7.8974e-02,  9.6706e-02, -4.5714e-02, -3.0489e-02,\n",
       "                         8.1682e-02,  1.0932e-01, -4.7715e-02,  3.8710e-02, -2.5511e-02,\n",
       "                         1.2067e-01, -6.4571e-03, -4.3763e-02,  2.3911e-02,  6.9322e-02,\n",
       "                        -4.4137e-02,  8.0932e-03,  9.2651e-02,  9.9573e-02,  4.3297e-02,\n",
       "                         7.7170e-02,  6.6795e-02,  1.0394e-01,  6.1457e-02,  5.5375e-03,\n",
       "                         1.0671e-02,  4.1702e-02,  9.3672e-03,  8.6722e-02,  2.7101e-02,\n",
       "                         8.5057e-02,  3.4416e-02,  9.5140e-02,  3.6246e-02,  4.4369e-02,\n",
       "                        -7.4034e-03,  4.9592e-02, -2.6074e-02,  9.2142e-02,  7.9404e-02,\n",
       "                         9.6846e-02, -3.7954e-02,  1.0263e-01,  9.5999e-02,  8.9792e-02,\n",
       "                         4.8311e-02,  6.8474e-02,  9.2606e-02,  8.7708e-02,  7.0951e-02,\n",
       "                         9.4374e-02,  2.7053e-02, -1.7560e-02,  7.1617e-02, -1.5062e-03,\n",
       "                        -4.7761e-02,  8.6449e-02,  1.0352e-01,  6.6706e-02,  7.1615e-02,\n",
       "                         8.5868e-02,  1.9025e-01,  2.7031e-01,  1.0121e-01,  7.4603e-02,\n",
       "                         2.6689e-02,  3.8686e-02, -1.4904e-03,  1.0365e-01, -8.1863e-03,\n",
       "                         6.4102e-02,  7.7583e-02,  1.0757e-01, -2.9191e-02, -2.7982e-02,\n",
       "                         7.6399e-02,  9.0354e-02,  2.3745e-01, -3.1546e-03,  9.7505e-02,\n",
       "                         3.7408e-03,  6.7881e-02,  3.1707e-02,  6.3505e-02,  4.0956e-02,\n",
       "                         1.0407e-01,  5.1578e-02,  8.9146e-02,  6.7909e-02,  7.2591e-02,\n",
       "                         5.0203e-02, -3.4259e-02,  7.2931e-02,  5.2575e-02, -2.5509e-03,\n",
       "                         5.0154e-02,  8.1541e-02, -4.2053e-02,  8.7114e-02,  1.1498e-02,\n",
       "                         1.0498e-01, -4.5066e-02,  5.9735e-02,  8.1768e-02,  1.6184e-02,\n",
       "                         7.6117e-02,  5.5117e-03,  1.8324e-02,  7.6733e-02,  1.4908e-01,\n",
       "                         8.0104e-02, -8.7619e-03,  1.0271e-01,  7.9172e-02,  7.3239e-02,\n",
       "                        -4.7130e-02,  7.0379e-03,  6.1235e-02,  1.2642e-02,  5.6394e-02,\n",
       "                         7.2760e-02, -4.0888e-02, -4.5150e-02,  1.1169e-01,  2.8645e-03,\n",
       "                        -3.5904e-03,  7.4077e-02, -1.4922e-02,  5.4082e-02, -2.2779e-02,\n",
       "                         7.7370e-02, -4.1333e-02, -2.8624e-02,  5.8071e-02,  9.5288e-02,\n",
       "                         4.6927e-02,  9.4129e-02,  1.2789e-01, -3.1097e-02,  8.0058e-02,\n",
       "                         8.3119e-02,  2.7419e-02,  7.2136e-02,  9.2670e-02,  8.6695e-02,\n",
       "                         7.6970e-02,  3.7841e-02,  6.2484e-02, -1.1730e-02, -1.6042e-02,\n",
       "                         1.0793e-01,  5.8573e-02,  6.5772e-02, -1.6297e-02,  1.4012e-01,\n",
       "                         7.7056e-02, -3.3294e-03,  5.2463e-02,  8.6329e-02,  4.5598e-02,\n",
       "                         5.5419e-02,  6.3293e-02,  1.0879e-01,  9.2090e-03,  9.3773e-02,\n",
       "                        -4.6591e-02, -3.8091e-02,  8.8768e-02,  4.0538e-02, -1.6652e-02,\n",
       "                         9.0537e-02,  1.4430e-01,  7.5777e-02,  5.8372e-02,  2.5584e-01,\n",
       "                         8.1387e-02,  4.0626e-02,  9.7474e-02,  7.6169e-04,  7.5639e-02,\n",
       "                        -4.4341e-02,  5.7889e-02,  7.0580e-02,  8.9947e-02,  2.1663e-02,\n",
       "                         8.2729e-02,  9.4202e-02,  7.1477e-02,  1.0515e-01,  1.3790e-02,\n",
       "                        -5.3920e-02,  9.1460e-02,  1.6628e-01,  6.3089e-02,  3.3896e-02,\n",
       "                         9.5542e-02,  9.7508e-02, -7.7477e-02, -7.7639e-03,  4.6172e-02,\n",
       "                        -4.3945e-02,  5.7296e-02, -5.2110e-02,  2.5892e-01,  6.2535e-02,\n",
       "                         3.4962e-02,  2.9578e-02], device='cuda:0')),\n",
       "               ('fc5.weight',\n",
       "                tensor([[0.0706, 0.1351, 0.1039,  ..., 0.1378, 0.0865, 0.0958],\n",
       "                        [0.1210, 0.0698, 0.1132,  ..., 0.0747, 0.0537, 0.0961],\n",
       "                        [0.0763, 0.0655, 0.1286,  ..., 0.0884, 0.0503, 0.1355],\n",
       "                        ...,\n",
       "                        [0.1288, 0.1326, 0.0763,  ..., 0.0724, 0.0746, 0.0864],\n",
       "                        [0.0689, 0.0567, 0.1152,  ..., 0.0754, 0.0521, 0.1230],\n",
       "                        [0.0701, 0.1213, 0.1335,  ..., 0.0780, 0.1105, 0.1083]],\n",
       "                       device='cuda:0')),\n",
       "               ('fc5.bias',\n",
       "                tensor([ 5.5964e-02,  5.7714e-02,  6.9460e-02,  3.5826e-02,  2.5021e-02,\n",
       "                        -5.6266e-03,  1.1225e-02,  7.0997e-02,  6.7016e-02,  4.0831e-02,\n",
       "                         4.8352e-02,  4.3072e-03,  1.5520e-02,  1.8824e-02,  8.2301e-02,\n",
       "                         2.8181e-02,  3.3876e-02,  4.5918e-02,  5.5353e-02,  3.6836e-02,\n",
       "                         5.8239e-02,  4.7303e-02,  3.8330e-02,  3.0714e-02,  6.8027e-02,\n",
       "                         5.1206e-02,  1.1197e-02,  5.0060e-02,  2.2199e-02,  3.3650e-02,\n",
       "                         6.7801e-02,  2.9059e-02,  2.5592e-02,  5.8378e-02,  2.8485e-03,\n",
       "                         4.1138e-02,  3.5451e-02,  5.6159e-02,  2.5670e-02,  6.7996e-02,\n",
       "                         3.0432e-02,  5.2035e-02,  1.9978e-02,  1.0741e-02,  3.9460e-02,\n",
       "                         5.8319e-03,  2.6897e-02,  3.7121e-02,  2.6295e-02,  3.4434e-02,\n",
       "                         3.2616e-02,  2.3259e-02,  3.0632e-03,  7.4687e-02,  3.0586e-02,\n",
       "                         4.6641e-02,  5.1360e-02,  3.2102e-02,  1.6519e-02,  3.6469e-02,\n",
       "                         4.1130e-02,  5.2228e-02,  2.7671e-02,  6.2952e-02, -2.3866e-03,\n",
       "                         5.2501e-02,  6.6695e-03,  7.3561e-02,  6.1228e-02,  7.0904e-02,\n",
       "                         1.4718e-02,  1.9975e-02,  6.3378e-02,  2.1474e-02,  7.2131e-02,\n",
       "                         7.1709e-02,  6.6591e-02,  2.4886e-02,  1.2938e-02,  7.9439e-03,\n",
       "                         3.1934e-02,  4.5226e-02,  7.0437e-02,  4.4487e-02,  5.3708e-02,\n",
       "                         3.2850e-02,  7.3796e-02,  1.1122e-02,  5.3305e-02,  6.4641e-02,\n",
       "                         6.0622e-02,  4.6062e-02,  2.6268e-02,  4.4033e-02, -2.2880e-03,\n",
       "                         1.0521e-02,  1.8722e-02,  7.7198e-02, -1.7430e-03,  7.0714e-02,\n",
       "                         6.7382e-02,  5.6957e-02,  4.8825e-02,  3.5602e-03,  3.4999e-02,\n",
       "                         1.8782e-02,  2.1563e-02,  1.1756e-02,  5.2336e-02, -1.2324e-03,\n",
       "                         3.5438e-02,  3.8705e-02,  8.8528e-03,  1.5055e-02,  4.3870e-02,\n",
       "                         5.3696e-02,  3.2199e-02,  6.7338e-02,  7.1268e-02,  7.8546e-02,\n",
       "                         3.9723e-02,  3.3295e-02,  5.6782e-02,  6.1809e-02,  4.3166e-02,\n",
       "                         6.8290e-02,  5.7991e-02,  5.8273e-02,  5.6745e-02,  4.0484e-02,\n",
       "                         4.1913e-02,  5.8098e-02,  3.7426e-02,  6.8795e-02,  5.5285e-02,\n",
       "                         6.2190e-02,  5.0762e-02,  2.5669e-02,  4.7072e-02,  3.7683e-02,\n",
       "                         3.3192e-03,  4.0232e-02,  7.9173e-02,  6.0424e-02,  4.7652e-02,\n",
       "                         6.7910e-02,  6.3143e-02,  3.2843e-02,  5.1391e-02,  3.0801e-02,\n",
       "                         1.7576e-02,  5.3683e-02,  1.5173e-02,  5.1588e-02,  2.4982e-02,\n",
       "                         3.2047e-02,  6.8584e-02,  1.5950e-02,  4.3919e-02,  1.0885e-02,\n",
       "                         3.0533e-02,  3.9781e-02,  4.3274e-02,  6.8215e-02,  5.4582e-02,\n",
       "                         3.6791e-02,  7.0441e-02,  1.1272e-02,  4.8272e-02,  3.6707e-02,\n",
       "                         5.6482e-02,  2.1983e-02,  2.3569e-02,  7.1630e-02,  6.5125e-02,\n",
       "                         8.3652e-02,  5.2886e-02,  6.1186e-02,  3.4791e-02,  3.7515e-02,\n",
       "                         7.7423e-02,  3.8625e-02,  4.7376e-02,  4.9762e-02,  5.6885e-02,\n",
       "                         2.2207e-02,  6.5058e-02,  3.4166e-02,  3.2890e-03,  2.9975e-03,\n",
       "                         4.8350e-02,  5.2808e-02,  3.9794e-02,  5.9502e-02,  2.0201e-02,\n",
       "                         5.8556e-02,  5.3217e-02,  2.6058e-02,  4.6718e-02,  1.0477e-02,\n",
       "                         2.1959e-02,  4.6659e-02,  3.6506e-02,  4.2561e-02,  4.9265e-02,\n",
       "                         4.8613e-02,  1.3413e-02,  2.0959e-02,  4.0583e-02,  6.5897e-02,\n",
       "                         5.6470e-02,  1.2217e-02,  2.4993e-02,  7.2960e-02,  9.9721e-03,\n",
       "                         6.6522e-02,  3.6646e-02,  6.8531e-02,  4.3811e-02,  2.1681e-02,\n",
       "                         1.2950e-02,  5.3444e-02,  5.9184e-02,  6.3427e-02,  1.6017e-02,\n",
       "                         2.3846e-02,  5.1764e-02,  3.0279e-02,  2.4702e-02,  8.1105e-03,\n",
       "                         2.5372e-02,  4.9509e-02,  3.3999e-02,  6.4314e-02,  4.3272e-02,\n",
       "                         7.3791e-02,  6.3606e-02,  3.2641e-03,  6.2404e-02,  3.5842e-02,\n",
       "                         5.6925e-03,  4.9324e-02,  3.2700e-02,  5.0359e-02,  1.8555e-02,\n",
       "                         2.8984e-02,  5.2713e-02,  2.1870e-02,  6.3170e-02,  4.4067e-02,\n",
       "                         4.7020e-02,  4.0375e-02,  8.3889e-03,  4.4203e-04,  3.0870e-02,\n",
       "                         5.0125e-02,  2.0838e-02,  4.8352e-02,  6.8440e-02,  5.1022e-02,\n",
       "                         9.0495e-03,  3.5674e-02,  5.3434e-03,  8.9219e-03,  6.9455e-02,\n",
       "                         2.0972e-02,  6.9786e-02,  6.1016e-03,  4.2292e-03,  1.6940e-02,\n",
       "                         4.8474e-02,  4.0964e-02,  4.2370e-02,  1.6830e-02,  2.4613e-02,\n",
       "                         6.0532e-02,  6.7282e-02,  6.1489e-02,  6.4270e-02,  3.7930e-02,\n",
       "                         5.0915e-02,  1.0338e-02,  8.5771e-03,  3.7206e-03,  6.5821e-02,\n",
       "                         8.2290e-03,  6.6442e-02, -4.1931e-03,  2.7152e-02,  5.0296e-02,\n",
       "                         5.8241e-02,  6.5061e-02,  7.0036e-02,  3.9362e-02,  4.2691e-02,\n",
       "                         1.9993e-02, -4.8794e-03,  4.0841e-02,  1.1340e-02,  3.7031e-02,\n",
       "                         4.4426e-02,  6.7266e-02,  4.5673e-02,  4.1564e-02,  4.1698e-02,\n",
       "                         3.4251e-02,  1.7811e-02,  2.3631e-03,  1.8440e-02,  4.7179e-02,\n",
       "                         1.7583e-02,  3.2557e-02,  5.3354e-02,  5.7313e-02,  1.6607e-02,\n",
       "                         4.7456e-02,  4.4339e-02,  1.4689e-02,  5.8940e-02,  7.1546e-02,\n",
       "                        -5.8600e-03,  6.7740e-02, -2.2195e-03,  4.5695e-02,  2.5728e-02,\n",
       "                         2.7165e-02,  1.2471e-02,  2.3468e-02,  8.2721e-03,  4.7795e-02,\n",
       "                         4.7589e-02,  2.6838e-02,  7.5146e-02,  1.5576e-03,  7.7662e-02,\n",
       "                         3.1868e-02,  6.3146e-02,  3.3956e-02,  4.1351e-02,  2.3498e-03,\n",
       "                         2.2349e-02,  1.0534e-02,  5.4240e-02,  3.7359e-02,  3.0305e-02,\n",
       "                         6.8444e-02,  1.5080e-02,  5.3708e-02,  3.7508e-02,  4.4151e-02,\n",
       "                         5.7466e-02,  7.4070e-02,  3.6721e-02,  2.7944e-02,  7.7630e-02,\n",
       "                         3.9192e-02,  9.4379e-03,  5.9667e-02,  2.6328e-02,  3.2080e-02,\n",
       "                         4.2128e-02,  1.9877e-02,  6.8832e-02,  2.4552e-02,  6.8843e-03,\n",
       "                         3.2229e-02,  7.6527e-02,  4.1490e-02,  4.5525e-02,  5.7216e-02,\n",
       "                         3.2112e-02,  5.5153e-02,  6.2152e-02,  6.8151e-02,  1.4437e-02,\n",
       "                         4.2754e-02,  4.5070e-02,  3.1589e-02,  4.5719e-02,  3.1471e-02,\n",
       "                         1.7597e-02,  5.8044e-02,  2.8991e-02,  6.8553e-02,  3.7681e-02,\n",
       "                        -8.9440e-04,  4.0124e-03,  1.0928e-02,  3.3416e-02,  4.8069e-02,\n",
       "                         6.8608e-02,  1.7942e-02,  3.9880e-02,  9.0672e-03,  5.6551e-02,\n",
       "                         1.1053e-02,  3.4534e-02,  4.5698e-02,  3.0563e-02,  2.6120e-02,\n",
       "                        -4.9003e-04,  7.1856e-02,  7.7879e-02,  5.8982e-02,  6.7376e-02,\n",
       "                         1.4608e-02,  4.3437e-02,  6.0555e-02,  5.3762e-03,  1.2003e-02,\n",
       "                         2.2141e-02,  3.6750e-02, -1.5311e-03,  3.8706e-02, -1.1227e-03,\n",
       "                         6.5274e-02,  2.3134e-02,  3.2669e-02,  3.5847e-03,  4.8788e-02,\n",
       "                         2.8525e-02,  6.7551e-02,  1.0037e-02,  4.3736e-02,  5.8278e-02,\n",
       "                         5.9608e-02,  3.6554e-02,  5.0838e-02,  6.9469e-02,  4.6980e-02,\n",
       "                         2.8742e-02,  2.6693e-02,  4.5051e-02,  1.2553e-02,  1.3857e-02,\n",
       "                         7.4897e-02,  5.7655e-03,  1.6859e-02,  5.8878e-02,  6.3127e-02,\n",
       "                         3.7925e-02,  4.2157e-02,  6.6524e-02,  3.1781e-02,  5.2921e-02,\n",
       "                         4.9813e-02,  2.6797e-03,  7.4680e-02,  1.6502e-02,  8.3106e-02,\n",
       "                         1.1685e-02,  1.4371e-02,  4.6309e-03,  2.9629e-02,  4.8589e-02,\n",
       "                         1.2615e-02,  7.3809e-02,  3.6036e-03,  2.1298e-02,  5.2874e-02,\n",
       "                         5.9320e-02,  1.1537e-02,  2.5273e-02,  4.0174e-02,  6.9898e-02,\n",
       "                         3.1919e-02,  2.9086e-02, -1.5622e-03,  6.4298e-03,  3.0002e-02,\n",
       "                         5.8775e-02,  2.1336e-02,  4.6651e-02,  5.6568e-02,  2.7939e-02,\n",
       "                         3.1303e-02,  3.7681e-02,  5.2301e-02,  7.7035e-03,  6.2409e-02,\n",
       "                         4.8532e-03,  7.6719e-02, -4.4810e-04,  7.1606e-02,  7.1010e-02,\n",
       "                         6.5139e-02,  5.2452e-02,  4.9996e-02,  4.5775e-03,  4.6239e-03,\n",
       "                         7.2140e-03,  4.4605e-02,  3.1069e-02,  1.4623e-02,  2.2778e-02,\n",
       "                         6.2927e-02,  8.4547e-03,  5.6753e-02,  3.8624e-02,  1.5409e-02,\n",
       "                         1.9014e-02,  2.6271e-02, -4.4908e-03,  1.3661e-02,  3.0860e-03,\n",
       "                         2.7717e-02,  6.7086e-02,  3.1293e-02,  2.4737e-02,  4.1141e-02,\n",
       "                         4.3319e-02,  2.2385e-02,  2.3144e-02,  5.2807e-02,  3.8041e-02,\n",
       "                         5.2514e-02,  2.4949e-02,  4.9267e-02,  2.2770e-02,  8.3398e-03,\n",
       "                         1.9407e-02,  1.0469e-02,  1.5385e-02,  6.3653e-02,  6.6311e-02,\n",
       "                         2.7835e-02,  5.0750e-02,  4.0080e-02,  5.3486e-03,  2.8413e-02,\n",
       "                         6.4209e-02,  3.6791e-02,  5.3177e-03,  1.8261e-02,  5.5284e-02,\n",
       "                         2.4864e-02,  4.7806e-02,  6.9205e-02,  5.9221e-02,  6.6764e-02,\n",
       "                         1.5294e-02,  3.6562e-02, -9.8489e-03,  7.4772e-02,  4.7796e-02,\n",
       "                         4.6418e-02,  2.4872e-02,  7.7914e-02,  2.6052e-02,  2.8509e-02,\n",
       "                         2.2841e-02,  6.9989e-02,  5.0292e-02,  3.7920e-02,  1.1777e-02,\n",
       "                         7.4676e-02, -5.6888e-03,  2.4984e-02,  2.3278e-02,  6.2786e-02,\n",
       "                         6.8759e-03,  4.9991e-02,  1.2397e-02,  3.8345e-02,  5.7645e-02,\n",
       "                         2.0753e-02,  4.5851e-02,  6.4974e-02,  2.0404e-02,  9.9946e-03,\n",
       "                         6.6271e-02,  2.9148e-02,  7.0356e-02,  7.5244e-03,  4.3925e-02,\n",
       "                         3.8012e-02,  7.6633e-02,  3.3452e-02,  6.9417e-02,  6.9101e-02,\n",
       "                         4.1640e-02, -7.4459e-04,  6.3535e-02,  3.3376e-02,  6.0088e-03,\n",
       "                         4.6898e-02,  2.6792e-02,  4.0404e-02,  7.7062e-02,  5.9073e-02,\n",
       "                         6.2460e-02,  1.8157e-02,  6.0816e-02,  2.6327e-02,  6.5033e-02,\n",
       "                         7.2450e-02,  6.7944e-02,  1.0965e-05,  7.0043e-03,  2.7701e-03,\n",
       "                         1.0776e-02,  5.8877e-02,  4.0650e-02,  6.0865e-02,  1.6837e-03,\n",
       "                         6.5468e-02,  4.6139e-02,  3.4625e-02,  6.1908e-02,  5.9247e-02,\n",
       "                         1.4966e-02,  4.8268e-02,  7.0565e-02,  1.8573e-02,  3.4102e-02,\n",
       "                         3.0984e-03,  2.2016e-02,  1.6035e-02,  1.8984e-02,  5.3142e-02,\n",
       "                         4.5420e-02,  1.5959e-02,  5.4922e-02,  3.8536e-02,  5.3210e-02,\n",
       "                         4.6028e-02,  7.1283e-02,  3.5592e-02,  6.3492e-02,  7.4372e-03,\n",
       "                         6.5251e-02,  7.2939e-02,  4.2796e-02,  6.1040e-02,  3.5612e-02,\n",
       "                         1.2290e-02,  4.9414e-02,  7.5646e-03,  3.1779e-02,  1.2175e-02,\n",
       "                         9.8786e-03,  6.1574e-02,  2.3852e-02,  3.1346e-02,  7.1003e-02,\n",
       "                         3.5094e-02,  1.0216e-02,  3.3625e-02,  4.4549e-02,  3.4668e-02,\n",
       "                         2.1920e-02,  4.9077e-02,  3.2537e-02,  5.0730e-02,  7.4908e-02,\n",
       "                         3.0913e-02,  2.5942e-02,  4.3566e-02,  9.4009e-03,  2.7813e-02,\n",
       "                         2.2261e-03,  9.1789e-03,  9.2724e-03,  6.8778e-02,  6.1478e-02,\n",
       "                         3.6913e-02,  4.9422e-02,  1.7053e-02,  5.2083e-02,  9.3869e-03,\n",
       "                        -5.0681e-04,  1.2070e-02,  6.8467e-02,  4.1851e-02,  3.1279e-02,\n",
       "                         2.3096e-02,  2.5240e-02,  5.7671e-02,  5.3039e-02,  1.9965e-02,\n",
       "                         1.6621e-02,  5.0485e-02,  5.7904e-02,  5.6652e-02,  6.1571e-02,\n",
       "                         3.0548e-02,  6.7628e-03,  5.2970e-02,  1.7385e-03,  1.0098e-02,\n",
       "                         1.4191e-02,  5.1033e-02, -6.1100e-03,  2.2170e-02,  6.4102e-02,\n",
       "                         6.8279e-02,  4.8405e-02,  2.2274e-02,  4.4223e-02,  4.6322e-02,\n",
       "                         7.3221e-02,  5.4531e-02,  4.4182e-02,  3.9409e-02,  6.5892e-02,\n",
       "                         1.5077e-02,  2.6761e-03,  7.4718e-02,  1.0768e-02,  5.4732e-02,\n",
       "                         3.0827e-02,  1.3982e-02,  1.3063e-02,  1.5887e-02,  5.6674e-02,\n",
       "                         1.9492e-02,  1.6043e-02,  7.0630e-02,  5.4955e-02,  3.6167e-02,\n",
       "                         6.7482e-02,  2.0058e-02,  1.8753e-02,  5.8343e-02,  5.9835e-02,\n",
       "                         3.5757e-02,  6.5493e-02,  8.1915e-03,  6.9928e-02,  2.5920e-03,\n",
       "                         6.0569e-02,  1.2113e-02,  2.4924e-02,  5.5705e-02,  5.9487e-02,\n",
       "                         1.6482e-02,  8.7571e-03,  4.1305e-02,  4.4001e-02,  1.7456e-02,\n",
       "                         6.3310e-02,  7.0838e-02,  7.0063e-02,  7.6870e-02,  6.0772e-02,\n",
       "                         2.0277e-02,  4.0761e-02,  7.4498e-02,  4.0816e-02,  1.1742e-02,\n",
       "                         4.6808e-02,  5.3873e-02,  2.6617e-02,  5.9961e-02,  5.0300e-03,\n",
       "                         2.9918e-02,  5.6600e-02,  6.3563e-02,  2.5718e-02,  5.0104e-02,\n",
       "                         1.8517e-02,  6.4455e-02,  6.7884e-02,  2.3393e-02,  7.7464e-02,\n",
       "                         2.1962e-02,  5.4341e-02,  2.9977e-03,  6.2254e-02,  6.1610e-02,\n",
       "                         6.2092e-02,  4.9948e-02,  8.3113e-02,  6.0832e-02,  7.4429e-02,\n",
       "                         4.3876e-02,  3.2847e-02,  2.4132e-03,  6.9028e-02,  2.6118e-02,\n",
       "                         4.4518e-02,  4.3724e-02,  4.4435e-02,  2.2938e-02,  3.8621e-02,\n",
       "                         1.3544e-02,  3.7540e-02,  3.6761e-02,  5.3193e-02,  2.6664e-03,\n",
       "                         5.4711e-02,  5.0002e-02,  1.5448e-02,  3.2341e-02,  5.9190e-02,\n",
       "                         5.8883e-02,  1.0195e-02,  7.0208e-02,  3.8520e-02,  2.4133e-02,\n",
       "                         6.0802e-02,  6.6619e-02,  5.7534e-02,  3.7564e-02,  4.7095e-02,\n",
       "                         9.9036e-03,  4.4532e-02,  6.4396e-02,  2.5990e-02,  3.1874e-02,\n",
       "                         1.0683e-02,  6.2140e-02,  6.6914e-02,  2.1393e-02,  5.3216e-02,\n",
       "                         3.1142e-02,  1.8797e-02,  6.0016e-02,  1.5554e-02,  3.8772e-02,\n",
       "                         5.9200e-03,  1.5193e-02,  4.3928e-02,  6.9773e-02,  4.8498e-03,\n",
       "                         5.3941e-02,  1.2157e-02,  4.0107e-02,  5.0533e-02,  2.4678e-02,\n",
       "                         7.3954e-02,  1.8926e-02,  3.3672e-02,  1.0699e-02,  6.4875e-02,\n",
       "                         7.7249e-02,  1.7562e-02,  3.2517e-02,  6.9576e-02,  3.0102e-02,\n",
       "                         3.1049e-02,  2.4689e-02,  5.5701e-02,  4.4943e-02,  9.8866e-03,\n",
       "                         5.5633e-02,  4.7923e-02,  4.5609e-02,  6.9154e-02,  1.7761e-03,\n",
       "                         4.9671e-02,  3.3794e-02,  3.7107e-02,  3.0764e-02,  7.4743e-02,\n",
       "                         1.2243e-02,  1.7174e-02,  4.1426e-02,  3.5400e-02,  5.8723e-02,\n",
       "                         3.2902e-02,  3.3691e-02,  7.0308e-02,  6.5955e-02,  4.2364e-02,\n",
       "                         1.4820e-02,  5.9063e-02,  4.9287e-02,  3.8345e-02,  3.1389e-02,\n",
       "                         2.5615e-02,  5.1978e-02,  5.5598e-03,  2.7111e-02,  3.9602e-02,\n",
       "                         1.1205e-02,  3.1094e-02,  1.8317e-02,  7.2591e-02,  1.5988e-02,\n",
       "                         6.8165e-02,  3.4817e-02,  2.2996e-02,  4.5762e-02,  1.0752e-02,\n",
       "                         2.5216e-02,  6.5980e-02,  4.4482e-03,  6.2396e-02,  8.6627e-03,\n",
       "                         4.1778e-02,  2.4949e-02,  5.7852e-02,  2.5335e-02,  6.9082e-02,\n",
       "                         2.1938e-02,  5.6281e-02,  6.3051e-02,  4.4045e-03,  5.5214e-02,\n",
       "                         9.2764e-03,  1.0810e-02,  2.8265e-02,  6.0916e-02,  4.0609e-02,\n",
       "                         6.7090e-02,  7.1581e-03,  5.5606e-02,  6.6362e-03,  2.9108e-02,\n",
       "                         1.2346e-02,  4.9618e-02,  3.4270e-02,  1.3116e-02,  3.1012e-02,\n",
       "                        -2.4976e-03,  1.2801e-02,  6.3032e-04,  1.5666e-02,  7.2645e-02,\n",
       "                         2.6552e-02,  1.7833e-02,  7.2155e-02,  4.6844e-03,  2.3135e-02,\n",
       "                         3.9778e-02,  3.1699e-02,  4.0886e-02,  6.6107e-02,  6.3330e-02,\n",
       "                         7.2558e-02,  5.8074e-02,  4.2731e-02,  5.3549e-02,  5.2330e-02,\n",
       "                         9.5755e-03,  4.6989e-02,  4.5155e-02,  2.4815e-02,  9.3283e-03,\n",
       "                         5.5389e-02,  8.5950e-03,  3.6354e-02,  4.2127e-02,  4.9384e-02,\n",
       "                         2.6752e-02,  2.1741e-02,  3.3166e-02,  5.7889e-02,  2.3291e-02,\n",
       "                         2.2826e-03,  2.9139e-02,  2.4649e-02,  5.4908e-02,  2.3865e-02,\n",
       "                         1.7144e-03,  3.8390e-02,  1.6311e-02,  5.3535e-02,  2.4612e-02,\n",
       "                         2.6815e-02,  4.8798e-02,  4.9636e-02,  3.6788e-02,  5.2544e-02,\n",
       "                         4.2396e-02,  1.3750e-02,  4.7061e-02,  4.4669e-02,  7.5676e-03,\n",
       "                         6.2254e-02,  4.9232e-03,  4.8443e-02,  3.9520e-02,  5.0283e-04,\n",
       "                         6.3416e-02,  6.2228e-02,  2.1677e-02,  3.4289e-02,  5.2122e-02,\n",
       "                         2.7141e-02,  4.0235e-02,  4.5752e-02,  2.6857e-02,  5.5995e-02,\n",
       "                         5.8241e-02,  1.0257e-02,  6.0350e-02,  4.8127e-02,  6.8376e-02,\n",
       "                         6.4607e-02,  1.0683e-02], device='cuda:0'))])},\n",
       " 'iteration': 48000,\n",
       " 'optimizer_state_dict': {'state': {0: {'step': tensor(47976.),\n",
       "    'exp_avg': tensor([[ 2.5497e+00,  9.2949e-01, -5.3207e-01,  ..., -2.2252e-01,\n",
       "              1.1709e-01,  3.6931e-01],\n",
       "            [ 3.7848e-01, -6.2390e-01,  6.5850e-01,  ...,  2.9648e-03,\n",
       "              5.2514e-02,  8.0116e-02],\n",
       "            [-8.6179e-01,  1.8094e-01,  3.0822e-01,  ..., -1.5943e-02,\n",
       "             -3.1404e-02, -2.8459e-02],\n",
       "            ...,\n",
       "            [ 4.0453e+00,  1.1302e+00, -2.2503e+00,  ..., -2.3075e-02,\n",
       "             -2.9818e-03,  2.5339e-01],\n",
       "            [-2.1886e+00, -2.3896e+00, -5.3480e+00,  ...,  8.2644e-01,\n",
       "              8.3966e-01,  1.4090e+00],\n",
       "            [-1.8616e-01,  1.1056e-01,  4.2818e-01,  ...,  8.5286e-02,\n",
       "             -1.3836e-02, -7.4431e-04]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[1.6079e+02, 1.7504e+02, 9.9206e+01,  ..., 2.6454e+00, 4.3218e+00,\n",
       "             2.4835e+00],\n",
       "            [6.0960e+01, 5.1383e+01, 4.9882e+01,  ..., 7.4353e-02, 7.1667e-02,\n",
       "             2.4156e-02],\n",
       "            [5.4786e+02, 5.2988e+02, 4.9540e+02,  ..., 5.3357e-01, 7.0395e-01,\n",
       "             2.8627e-01],\n",
       "            ...,\n",
       "            [5.9596e+01, 1.1273e+01, 6.7219e+01,  ..., 2.8889e+00, 3.2686e+00,\n",
       "             2.3790e+00],\n",
       "            [6.1484e+02, 5.3791e+02, 5.6893e+02,  ..., 4.2958e+00, 7.5112e+00,\n",
       "             7.1579e+00],\n",
       "            [2.0916e+01, 2.0710e+01, 1.9881e+01,  ..., 6.5993e-02, 4.9806e-02,\n",
       "             2.0031e-02]], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([[1.6489e+02, 1.8155e+02, 9.9906e+01,  ..., 2.6480e+00, 4.4068e+00,\n",
       "             2.4885e+00],\n",
       "            [6.5002e+01, 5.4739e+01, 5.3280e+01,  ..., 7.4404e-02, 7.3755e-02,\n",
       "             2.6036e-02],\n",
       "            [5.9735e+02, 5.7896e+02, 5.4323e+02,  ..., 5.3397e-01, 7.1118e-01,\n",
       "             2.9360e-01],\n",
       "            ...,\n",
       "            [5.9636e+01, 1.1273e+01, 6.7224e+01,  ..., 2.8894e+00, 3.3413e+00,\n",
       "             2.3875e+00],\n",
       "            [6.2892e+02, 5.4767e+02, 5.7930e+02,  ..., 4.3001e+00, 7.5819e+00,\n",
       "             7.1651e+00],\n",
       "            [2.2011e+01, 2.1609e+01, 2.0913e+01,  ..., 6.6252e-02, 4.9989e-02,\n",
       "             2.7461e-02]], device='cuda:0')},\n",
       "   1: {'step': tensor(47976.),\n",
       "    'exp_avg': tensor([ 3.1069e+00,  1.1813e+00, -1.9560e+00, -6.1523e-01,  7.5868e+00,\n",
       "             2.1379e+00,  5.2503e+00, -5.0068e-03, -4.3171e+00,  1.3996e+00,\n",
       "            -1.1641e-02, -3.1138e-01,  2.4610e+00,  9.7225e-01,  4.4690e+00,\n",
       "             1.4246e+01, -8.1234e-01,  8.5831e-01, -1.3205e-01, -2.4541e-01,\n",
       "            -5.0515e-01,  7.7074e+00, -1.2555e+00,  9.0835e-01,  3.3188e+00,\n",
       "             1.2555e-01,  1.3463e+00, -8.2998e-01,  1.8915e-01, -4.6874e-02,\n",
       "             7.6807e+00,  2.4189e+00,  1.8342e+00,  3.9119e+00,  2.9582e+00,\n",
       "             1.1599e+00,  2.5789e-01,  1.5348e+00,  3.8244e+00,  4.0050e+00,\n",
       "            -1.7300e-01,  7.9604e+00,  2.3043e-01,  3.9067e+00,  1.7436e+00,\n",
       "             1.1461e+01,  8.8738e-01, -7.4042e-01,  3.5916e+00,  5.4026e+00,\n",
       "            -6.2631e+00, -9.0917e-01,  8.7678e+00,  6.6849e-01, -6.5427e-01,\n",
       "             9.8806e-01, -7.3220e-02,  3.0166e+00,  2.3880e+00,  6.4547e+00,\n",
       "             1.0260e+00, -1.5211e+00, -2.3473e-01,  3.8173e+00, -1.5600e+00,\n",
       "             1.1288e-01, -1.5552e+00, -1.6126e-01, -9.3467e-01,  9.6094e+00,\n",
       "            -1.7572e-01,  5.6545e+00, -6.1144e+00, -1.3826e+00,  2.1229e+00,\n",
       "             6.1012e-01, -5.1244e-01,  1.9038e+00,  3.1653e+00, -3.8826e-01,\n",
       "             1.1955e+00, -3.2844e-03,  2.5686e+00,  2.5020e+00,  1.6883e+00,\n",
       "             1.0195e+00,  3.4155e+00, -1.2411e+00,  4.3647e+00, -1.2313e-01,\n",
       "             3.9602e-01,  4.6299e+00, -1.3299e+00,  2.5517e-01, -7.4284e-01,\n",
       "             6.3087e+00, -5.6546e-01,  2.2948e+00,  2.5655e+00,  1.5548e+00,\n",
       "             1.6409e+00,  5.8345e+00,  1.8848e+00,  8.5087e-01,  1.8956e-01,\n",
       "            -1.2961e+00,  1.5206e+00,  1.1253e+00, -4.4645e+00,  4.5597e+00,\n",
       "             2.9264e+00,  1.2150e+00,  3.6453e-01,  5.8272e+00,  6.6021e+00,\n",
       "             4.4690e+00,  2.7224e+00,  2.0795e+00,  1.7022e+00,  5.5900e-02,\n",
       "             2.6286e+00,  1.4394e+00,  9.5711e-01,  1.7330e-02, -4.9410e+00,\n",
       "             2.1511e+00,  1.1080e+00,  1.7027e+00,  2.8520e+00,  7.5973e-01,\n",
       "             2.0516e+00,  1.3737e+00, -4.1696e+00, -2.8141e-01,  1.6524e+00,\n",
       "             3.8172e-01, -1.5941e+00,  9.2547e+00,  1.0419e+00, -9.5398e-01,\n",
       "             4.6537e-01,  2.7169e+00,  8.7114e-01, -5.5054e-01,  9.9037e+00,\n",
       "            -2.3691e-03, -8.0765e-01,  4.8274e-01,  2.1976e+00,  6.0062e+00,\n",
       "             1.0461e+00,  9.8418e-01,  1.4479e+00,  3.1946e+00,  3.4030e+00,\n",
       "             3.7768e+00,  1.1947e+00, -4.6904e+00,  2.1124e+00,  7.4525e-01,\n",
       "             2.6842e-01, -5.7517e+00,  1.7252e+00,  8.2091e+00, -1.3939e-01,\n",
       "             1.3473e+00,  3.7102e+00,  1.2159e+00,  5.8872e+00, -1.8738e+00,\n",
       "            -5.7465e-01,  2.1835e+00,  2.1963e-01,  8.2556e+00,  4.9365e-01,\n",
       "             5.4956e-01,  6.7180e-01,  6.9304e+00,  3.3043e+00,  1.8984e-01,\n",
       "             2.8580e-01,  7.5515e+00,  7.1329e+00,  1.0942e-01,  1.1902e+01,\n",
       "            -1.7678e-01,  5.5188e+00,  6.8897e+00,  4.2685e+00, -5.8315e-01,\n",
       "             1.4645e+00,  1.2082e+00, -1.8214e+00, -2.3433e-01,  2.6286e+00,\n",
       "             5.6577e+00,  1.0583e+00,  2.2351e-01,  4.4239e-02,  9.5205e-01,\n",
       "             7.8293e-01, -5.8125e-01,  1.1707e+01,  1.4968e+00,  5.0908e-01,\n",
       "            -2.0246e+00,  9.8179e+00,  1.8221e-01,  3.8555e-01,  1.2965e+00,\n",
       "             2.8124e+00,  3.4508e+00, -4.5975e+00,  7.9979e-01,  7.9591e+00,\n",
       "             7.7086e+00,  7.3775e-01, -1.7479e+00, -1.2456e-01, -3.7250e-01,\n",
       "             1.4130e+00, -1.7165e+00, -3.5280e-01,  3.9048e-01, -4.9594e+00,\n",
       "             9.5830e-01,  1.4490e+00,  3.5020e+00,  6.9498e-01,  1.3715e+00,\n",
       "             4.4277e-01,  2.3134e+00, -8.7916e-02, -1.2400e+00,  5.7768e+00,\n",
       "             5.7645e-01,  9.6159e-01,  1.3184e+00, -3.1762e+00,  6.8583e+00,\n",
       "             1.1306e+00,  1.7989e+00, -1.0114e+00,  9.6962e-01,  3.8874e-01,\n",
       "             2.9846e+00, -6.0604e+00,  2.5838e+00,  2.6497e+00,  1.9850e+00,\n",
       "             3.9658e-01, -2.4115e+00,  1.5985e-01,  4.5886e+00, -3.0334e+00,\n",
       "            -2.4546e-01], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([9.3074e+02, 1.2568e+02, 1.3086e+03, 2.6234e+03, 2.0505e+03, 1.3133e+02,\n",
       "            2.6685e+03, 6.9396e+02, 1.5982e+03, 1.8637e+02, 7.2876e+01, 5.3153e+00,\n",
       "            6.5415e+02, 7.9201e+01, 1.0853e+02, 2.6663e+03, 5.8842e+01, 1.3783e+02,\n",
       "            2.2101e+01, 5.5732e+01, 4.8550e+01, 3.2329e+03, 8.7264e+01, 1.5685e+02,\n",
       "            8.8693e+02, 4.7291e+00, 1.0458e+02, 3.9909e+02, 3.7449e+01, 1.5657e+03,\n",
       "            1.2012e+03, 2.5059e+02, 2.1546e+02, 9.2549e+02, 7.7741e+01, 8.5155e+01,\n",
       "            4.8068e+01, 6.7203e+01, 8.2066e+02, 1.0303e+03, 6.7800e+02, 1.6027e+03,\n",
       "            3.3612e+01, 1.7406e+02, 3.9852e+02, 3.0209e+03, 1.3991e+01, 2.3045e+03,\n",
       "            1.3433e+03, 5.3832e+02, 3.0958e+03, 1.6777e+03, 8.8361e+02, 3.8912e+01,\n",
       "            1.2764e+03, 1.1447e+02, 4.6315e+00, 1.3534e+03, 7.5142e+01, 5.3513e+03,\n",
       "            3.6225e+02, 2.9776e+01, 1.8735e+02, 3.2880e+03, 2.3003e+02, 1.9076e+02,\n",
       "            8.7846e+01, 2.3569e+01, 1.2697e+01, 2.0978e+03, 7.7150e+02, 3.8054e+03,\n",
       "            9.6212e+02, 5.3970e+02, 6.8772e+02, 7.6771e+01, 9.1612e+01, 1.1227e+02,\n",
       "            3.9087e+02, 2.0643e+02, 4.2140e+02, 4.4653e+01, 2.2879e+02, 3.0040e+02,\n",
       "            3.9337e+01, 3.6795e+02, 9.1399e+02, 5.4274e+01, 1.4864e+03, 9.1984e+00,\n",
       "            3.5905e+03, 4.2169e+03, 3.7311e+01, 3.5633e+01, 7.1375e+00, 3.0890e+03,\n",
       "            1.5776e+02, 3.0129e+02, 2.9288e+02, 3.4857e+02, 2.3521e+02, 2.5860e+03,\n",
       "            1.3297e+02, 1.3322e+02, 3.1678e+01, 8.7284e+01, 9.5217e+01, 1.8575e+02,\n",
       "            3.2515e+03, 9.5200e+02, 4.3135e+02, 3.8986e+02, 2.1721e+01, 4.0829e+03,\n",
       "            2.4578e+02, 1.6469e+03, 1.2987e+03, 2.5266e+02, 3.2955e+02, 4.1534e+00,\n",
       "            2.5807e+03, 5.9875e+02, 6.0699e+01, 1.1122e+02, 3.4700e+03, 5.6146e+02,\n",
       "            1.9118e+01, 6.4848e+02, 3.6841e+02, 5.6399e+02, 5.2597e+02, 2.4074e+02,\n",
       "            2.2688e+03, 1.5224e+01, 3.7131e+02, 6.4516e+01, 6.5974e+01, 1.0422e+03,\n",
       "            3.1363e+01, 7.3883e+02, 1.6180e+01, 2.0342e+03, 8.7274e+01, 1.3652e+02,\n",
       "            2.2716e+03, 7.8051e+01, 1.0385e+02, 1.5376e+01, 5.2236e+02, 3.5245e+03,\n",
       "            3.8098e+02, 1.3977e+02, 2.2070e+02, 4.5614e+02, 4.3270e+03, 3.7976e+03,\n",
       "            8.9132e+01, 1.2383e+02, 7.7221e+01, 1.3059e+02, 6.5371e+01, 6.5257e+01,\n",
       "            6.5900e+01, 5.9423e+03, 1.6221e+02, 2.9662e+02, 4.4782e+02, 6.6182e+02,\n",
       "            4.0310e+02, 4.7461e+02, 4.1580e+02, 2.4350e+03, 2.5193e+01, 5.2798e+02,\n",
       "            6.2184e+01, 2.0557e+02, 2.2014e+02, 3.3909e+03, 2.3942e+03, 7.2972e+01,\n",
       "            1.7298e+02, 5.7494e+02, 1.0520e+03, 1.2363e+02, 4.3518e+03, 4.2175e+02,\n",
       "            1.3101e+03, 2.7074e+03, 1.9240e+03, 2.0320e+01, 1.3915e+03, 1.4035e+02,\n",
       "            2.0118e+02, 3.5376e+02, 7.7391e+02, 1.2293e+03, 7.5935e+03, 1.2393e+02,\n",
       "            4.3058e+03, 8.2622e+02, 3.2242e+01, 3.2121e+01, 2.2451e+03, 8.1349e+01,\n",
       "            3.0334e+02, 8.5104e+02, 7.4181e+03, 6.1991e+01, 1.7487e+01, 6.1111e+01,\n",
       "            1.6680e+02, 3.0025e+03, 2.5748e+03, 9.3723e+00, 1.9816e+03, 6.8471e+02,\n",
       "            1.1492e+01, 1.4583e+03, 2.4917e+00, 3.4286e+00, 5.5595e+01, 4.2674e+02,\n",
       "            1.9811e+03, 9.9320e+01, 4.6239e+02, 9.8894e+02, 4.4384e+01, 5.5988e+02,\n",
       "            1.6181e+01, 1.4241e+03, 2.5132e+01, 8.1525e+02, 1.1101e+01, 3.2284e+02,\n",
       "            1.9077e+03, 6.7175e+01, 1.4790e+02, 1.3327e+02, 3.5470e+02, 1.4464e+03,\n",
       "            1.2683e+01, 8.2851e+01, 1.1923e+03, 2.9303e+01, 2.8586e+02, 7.7722e+02,\n",
       "            8.6329e+02, 6.2280e+02, 3.3539e+03, 1.6566e+02, 1.3617e+02, 9.8830e+01,\n",
       "            2.3164e+01, 1.0278e+03, 1.9489e+03, 6.5997e+01], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([9.6349e+02, 1.3499e+02, 1.4226e+03, 2.6671e+03, 2.1593e+03, 1.4938e+02,\n",
       "            2.6847e+03, 7.4271e+02, 1.7351e+03, 1.9175e+02, 7.6656e+01, 6.0400e+00,\n",
       "            6.7660e+02, 8.7365e+01, 1.1455e+02, 2.7022e+03, 6.2299e+01, 1.4627e+02,\n",
       "            9.7687e+01, 5.9664e+01, 5.0721e+01, 3.3813e+03, 9.1949e+01, 1.6878e+02,\n",
       "            9.0570e+02, 8.2688e+00, 1.1516e+02, 4.3107e+02, 4.0279e+01, 1.6167e+03,\n",
       "            1.2139e+03, 2.7283e+02, 2.1922e+02, 1.0133e+03, 7.8304e+01, 9.5193e+01,\n",
       "            5.1293e+01, 7.2750e+01, 9.0023e+02, 1.1414e+03, 7.4373e+02, 1.6466e+03,\n",
       "            3.6824e+01, 1.8050e+02, 4.3508e+02, 3.0758e+03, 1.4737e+01, 2.3113e+03,\n",
       "            1.4448e+03, 5.7009e+02, 3.1098e+03, 1.8181e+03, 9.1961e+02, 4.1494e+01,\n",
       "            1.3367e+03, 1.2757e+02, 9.4676e+00, 1.3635e+03, 8.0265e+01, 5.4521e+03,\n",
       "            3.8775e+02, 3.0404e+01, 1.9955e+02, 3.4666e+03, 2.3362e+02, 1.9677e+02,\n",
       "            9.5429e+01, 5.3381e+01, 2.9418e+01, 2.1079e+03, 8.3058e+02, 3.9982e+03,\n",
       "            1.0192e+03, 5.4953e+02, 7.3960e+02, 8.3100e+01, 9.5144e+01, 1.2878e+02,\n",
       "            4.2251e+02, 2.2116e+02, 4.4235e+02, 4.9135e+01, 2.3110e+02, 3.2898e+02,\n",
       "            5.3458e+01, 4.0695e+02, 9.8828e+02, 5.5215e+01, 1.5495e+03, 2.0474e+01,\n",
       "            3.6927e+03, 4.4217e+03, 3.7877e+01, 4.3074e+01, 9.3025e+00, 3.3085e+03,\n",
       "            1.6920e+02, 3.2163e+02, 3.1081e+02, 3.6011e+02, 2.5013e+02, 2.5921e+03,\n",
       "            1.3420e+02, 1.4230e+02, 3.4644e+01, 9.3462e+01, 1.1114e+02, 2.0563e+02,\n",
       "            3.4237e+03, 1.0323e+03, 4.6949e+02, 4.1371e+02, 2.2358e+01, 4.1282e+03,\n",
       "            2.4832e+02, 1.8021e+03, 1.4078e+03, 2.7561e+02, 3.6516e+02, 1.5917e+01,\n",
       "            2.6362e+03, 6.5515e+02, 6.5277e+01, 1.2208e+02, 3.5496e+03, 6.2645e+02,\n",
       "            4.2048e+01, 7.1482e+02, 3.8766e+02, 6.0990e+02, 5.3106e+02, 2.6999e+02,\n",
       "            2.4206e+03, 1.5762e+01, 4.0768e+02, 7.0429e+01, 7.6530e+01, 1.0461e+03,\n",
       "            3.3841e+01, 8.1106e+02, 1.6787e+01, 2.1593e+03, 9.1787e+01, 1.4363e+02,\n",
       "            2.2716e+03, 8.5032e+01, 1.0636e+02, 3.1781e+01, 5.6663e+02, 3.6237e+03,\n",
       "            4.2537e+02, 1.5449e+02, 2.3674e+02, 4.8487e+02, 4.5351e+03, 3.8627e+03,\n",
       "            9.5193e+01, 1.2454e+02, 8.1066e+01, 2.5285e+02, 6.8446e+01, 6.5257e+01,\n",
       "            7.0147e+01, 5.9943e+03, 1.7241e+02, 3.2772e+02, 4.5827e+02, 7.1008e+02,\n",
       "            4.3082e+02, 5.1363e+02, 4.5169e+02, 2.5502e+03, 3.4924e+01, 5.3596e+02,\n",
       "            6.8286e+01, 2.1930e+02, 2.4377e+02, 3.4584e+03, 2.4964e+03, 7.9176e+01,\n",
       "            2.1584e+02, 5.7858e+02, 1.0640e+03, 1.3733e+02, 4.4098e+03, 4.5358e+02,\n",
       "            1.4014e+03, 2.8537e+03, 2.0861e+03, 2.0705e+01, 1.4740e+03, 1.5150e+02,\n",
       "            2.1361e+02, 3.7741e+02, 8.4984e+02, 1.2839e+03, 7.7148e+03, 1.2869e+02,\n",
       "            4.4252e+03, 8.9983e+02, 4.1398e+01, 3.3128e+01, 2.2473e+03, 8.7190e+01,\n",
       "            3.3540e+02, 9.2577e+02, 7.4927e+03, 6.4188e+01, 1.7950e+01, 6.5697e+01,\n",
       "            1.6771e+02, 3.1008e+03, 2.7312e+03, 2.1466e+01, 2.1076e+03, 6.9066e+02,\n",
       "            1.3266e+01, 1.4921e+03, 3.1360e+00, 9.2156e+00, 5.6848e+01, 4.5916e+02,\n",
       "            2.0921e+03, 1.0752e+02, 4.9189e+02, 1.0717e+03, 4.7492e+01, 5.9752e+02,\n",
       "            1.7431e+01, 1.4426e+03, 3.0586e+01, 8.9190e+02, 2.3705e+01, 3.4329e+02,\n",
       "            1.9088e+03, 7.2188e+01, 1.6578e+02, 1.4797e+02, 3.7932e+02, 1.5101e+03,\n",
       "            3.7906e+01, 8.8626e+01, 1.2717e+03, 4.2090e+01, 3.2089e+02, 8.4198e+02,\n",
       "            9.3475e+02, 6.7405e+02, 3.4866e+03, 1.6877e+02, 1.4435e+02, 9.8926e+01,\n",
       "            2.5522e+01, 1.0345e+03, 1.9760e+03, 6.9138e+01], device='cuda:0')},\n",
       "   2: {'step': tensor(47976.),\n",
       "    'exp_avg': tensor([[-9.1062e-03,  6.4751e-02,  1.0717e-01,  ..., -1.0559e-03,\n",
       "              3.7397e-03,  3.1750e-02],\n",
       "            [-9.8046e-03,  7.1337e-02,  1.1589e-01,  ..., -1.2382e-03,\n",
       "              4.0451e-03,  3.2461e-02],\n",
       "            [-1.6771e-02,  4.9763e-02,  6.6666e-02,  ..., -8.8036e-03,\n",
       "              5.9464e-03, -3.1087e-02],\n",
       "            ...,\n",
       "            [-1.3175e-02,  4.9830e-02,  1.3061e-01,  ...,  3.5457e-05,\n",
       "              5.4015e-03,  4.7004e-02],\n",
       "            [-1.0641e-02,  7.5356e-02,  1.2497e-01,  ..., -1.2140e-03,\n",
       "              4.3596e-03,  3.6078e-02],\n",
       "            [-1.1660e-02,  8.4063e-02,  1.3961e-01,  ..., -1.4226e-03,\n",
       "              4.7941e-03,  3.9565e-02]], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[0.0041, 0.0517, 0.8092,  ..., 0.0104, 0.0024, 0.3765],\n",
       "            [0.0048, 0.0628, 0.9950,  ..., 0.0123, 0.0029, 0.4617],\n",
       "            [0.0099, 0.0770, 0.8102,  ..., 0.0245, 0.0059, 0.4055],\n",
       "            ...,\n",
       "            [0.0071, 0.0719, 0.8796,  ..., 0.0178, 0.0042, 0.4382],\n",
       "            [0.0056, 0.0705, 1.1019,  ..., 0.0142, 0.0033, 0.5126],\n",
       "            [0.0068, 0.0872, 1.3748,  ..., 0.0172, 0.0040, 0.6384]],\n",
       "           device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([[0.0041, 0.0588, 0.8881,  ..., 0.0104, 0.0024, 0.4031],\n",
       "            [0.0049, 0.0716, 1.0919,  ..., 0.0124, 0.0029, 0.4944],\n",
       "            [0.0099, 0.1062, 0.8909,  ..., 0.0246, 0.0059, 0.4337],\n",
       "            ...,\n",
       "            [0.0072, 0.0901, 0.9666,  ..., 0.0179, 0.0042, 0.4692],\n",
       "            [0.0056, 0.0813, 1.2100,  ..., 0.0142, 0.0033, 0.5490],\n",
       "            [0.0068, 0.0997, 1.5092,  ..., 0.0173, 0.0040, 0.6838]],\n",
       "           device='cuda:0')},\n",
       "   3: {'step': tensor(47976.),\n",
       "    'exp_avg': tensor([ 0.3330,  0.3581,  0.2566,  0.4214,  1.5977,  0.3939,  0.4041,  0.2252,\n",
       "             0.4114,  0.4201,  0.6727,  0.3141,  0.4339,  0.4138,  0.4090,  0.1955,\n",
       "             0.5021, -0.9060,  0.1415,  0.1441,  0.4262,  0.2336,  0.3277,  0.4328,\n",
       "             0.3862,  0.4325,  1.3605,  0.3799,  0.3862,  0.2409,  0.3749,  0.3820,\n",
       "             0.3866,  0.4436, -5.2321,  0.4063,  0.4205,  0.4148,  0.3550,  0.3899,\n",
       "             0.3679, -1.8123,  0.4215,  0.3724,  0.3564,  0.5269,  0.3519,  0.3910,\n",
       "             0.4524,  0.3128,  0.3297,  0.3766,  0.3857,  0.2299,  0.3499,  0.4322,\n",
       "             0.4424,  0.4287,  0.3466,  0.3887,  0.4294,  0.3431,  0.3766,  0.4284,\n",
       "             0.4585,  0.4264,  0.4354,  0.4007,  0.3769,  0.4233,  0.4537, -1.7267,\n",
       "             0.5500,  0.4666, -2.3300,  0.3952,  0.4968,  0.4748,  0.4012,  0.3774,\n",
       "             0.3533,  0.3755,  0.3700,  0.3413,  0.3270,  0.4188,  0.2462,  0.4038,\n",
       "             0.4465,  0.2351,  0.4437,  0.2926,  0.4390,  0.3631,  0.4166,  0.6686,\n",
       "             0.4099,  0.4702, -0.3657,  0.3281, -0.0116,  0.4063,  0.4171,  0.4318,\n",
       "             0.3594,  0.6492,  0.4778,  0.3792,  0.3840,  0.4258,  0.2530,  0.3539,\n",
       "             0.3661,  0.3667,  0.5747, -1.9533,  0.4193,  0.2457,  0.1081,  0.4469,\n",
       "             0.4641,  0.4926,  0.3988,  0.4643,  0.4302,  0.3002, -4.0740,  0.4010,\n",
       "             0.3741,  0.3960,  0.4495,  0.3610,  0.3696,  0.4904,  0.3522,  0.3681,\n",
       "             0.4109,  0.5120,  0.3463,  0.5384,  0.4460,  0.5136,  0.4174,  0.3926,\n",
       "             0.4253,  0.3874,  0.4607,  0.4225,  0.2784,  0.4336,  0.3516, -0.5726,\n",
       "             2.4712,  0.3866,  0.4194,  0.2421,  0.5342,  0.3564,  0.6296, -1.0072,\n",
       "             0.2694,  0.3896,  0.3899,  0.4066,  0.3957,  0.3594,  0.4381,  0.4698,\n",
       "             0.3685,  0.3799,  0.3637,  0.4077,  0.4279,  0.4769,  0.5447,  0.4245,\n",
       "             0.1765,  0.3831,  0.4450,  0.4529,  0.4230,  0.4400,  0.4335,  0.4345,\n",
       "             0.3906,  0.2026,  0.1435,  0.3574,  0.3072,  0.3856,  0.4020,  0.4503,\n",
       "             0.3587,  0.4247,  0.4233,  0.7591,  0.1953,  0.3988,  0.4848,  0.6554,\n",
       "             0.4324,  0.3764,  0.8848,  0.4414,  0.2188,  0.4955,  0.4555,  0.3807,\n",
       "             0.3469,  0.4001,  0.3991,  0.3909,  0.4299,  0.5190,  0.3136,  0.4458,\n",
       "             0.5292,  0.3566,  0.4823,  0.4814,  0.2509,  0.3124,  0.4004,  0.3404,\n",
       "             0.5399,  0.1787,  0.3818,  0.3841,  0.3944,  0.4160,  0.5931, -0.9750,\n",
       "             0.3271,  0.3770,  0.4072,  0.2257,  0.4088,  0.3726,  0.4746,  0.3802,\n",
       "             0.4496,  0.4585,  0.3767,  0.4016,  0.6564,  0.3677,  0.3541,  0.4161,\n",
       "            -1.7416,  0.2816,  0.4357,  0.3871,  0.4316,  0.4313,  0.3873,  0.4293],\n",
       "           device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([  9.2935,  11.3495,  11.6136,  12.0169, 310.4145,  13.1605,  13.2903,\n",
       "              9.4802,  14.1268,  14.8283,  87.1915,   8.3905,  15.5952,  15.3623,\n",
       "             13.4577,  15.2803,  19.3663, 200.9650,  11.4216,   9.3717,  15.2435,\n",
       "             14.0899,  28.3369,  16.0706,  13.0512,  15.8718, 492.7124,  14.6819,\n",
       "             13.1015,  10.1275,  12.2188,  12.5611,  20.9638,  18.4291, 675.0219,\n",
       "             13.7036,  13.9485,  14.3851,  10.7595,  12.7793,  11.4980, 236.3660,\n",
       "             14.6737,  11.9975,  14.0431,  24.2464,  12.9048,  12.6860,  16.8865,\n",
       "              8.5817,   9.4844,  11.8433,  12.6203,   8.9672,  10.0496,  15.7273,\n",
       "             15.8283,  15.2239, 130.9098,  12.2472,  13.9152,  10.1717,  12.0343,\n",
       "             14.0504,  17.7420,  15.1442,  16.3348,  13.4982,  12.3184,  14.8313,\n",
       "             17.6243, 168.1636,  26.3505,  16.5449, 268.9312,  13.1142,  18.0395,\n",
       "             16.2828,  13.4798,  12.1844,  10.6861,  12.0267,  11.7192,  24.5476,\n",
       "             12.1097,  14.7127,  12.4349,  13.5103,  17.0967,  45.3887,  14.6558,\n",
       "             11.2829,  18.6466,  10.9778,  14.4818,  52.2060,  13.9822,  17.0954,\n",
       "             11.6215,   9.3406, 145.4273,  13.7881,  14.6669,  15.7251,  10.9127,\n",
       "             24.5166,  18.2302,  12.3976,  12.6587,  14.9878,  22.2816,  10.6326,\n",
       "             11.4290,  11.3313,  23.5703, 352.7007,  14.9499,   9.6390,  11.8350,\n",
       "             16.5328,  12.1106,  39.7914,  13.4232,  17.1823,  14.3099,   9.0013,\n",
       "            564.6190,  14.7698,  11.8390,  14.0299,  15.1358,  11.3647,  11.5035,\n",
       "             17.1851,  10.7732,  11.6519,  14.0945,  15.9498,   9.6687,  77.8280,\n",
       "             16.6884,  19.3367,  14.8501,  13.1164,  14.4580,  12.4188,  18.0148,\n",
       "             15.8047,  17.7056,  11.6303,   9.9234,  95.4123, 836.9790,  12.5745,\n",
       "             13.7435,  54.0647,  26.1455,  10.9771,  19.7270, 159.1410,   8.5527,\n",
       "             12.6494,  13.1363,  14.2075,  13.1521,  11.1320,  16.3995,  18.8345,\n",
       "             11.6444,  12.3196,  11.2895,  13.7495,  15.0094,  23.7963,  31.0546,\n",
       "             15.5068,  12.9230,  12.3435,  14.4073,  17.8356,  15.1487,  14.5679,\n",
       "             15.9156,  15.9010,  12.8178,   8.6785,  12.3212,  10.9899,   8.2559,\n",
       "             12.3251,  13.5275,  16.6779,  11.0106,  15.1096,  15.0862,  26.0600,\n",
       "              9.3395,  12.9213,  12.4090,  31.2740,  15.9030,  11.6208,  70.0150,\n",
       "             17.6928,   9.0643,  23.3149,  19.1921,  12.3401,  10.1627,  13.1924,\n",
       "             13.6512,  13.1003,  15.6779,  19.2765,   8.6151,  16.9834,  23.4043,\n",
       "             10.4498, 114.8658,  10.8436,  11.8756,   8.2491,  13.0814,   9.8290,\n",
       "             22.9844,  16.3377,  14.4059,  12.6127,  13.4360,  14.9236,  27.6492,\n",
       "             65.9881,   9.3829,  12.1127,  15.2775,   9.5528,  14.0168,  12.0599,\n",
       "             19.3775,  12.2939,  16.9245,  17.9506,  12.2534,  14.0719,  14.8973,\n",
       "             11.5538,  10.6595,  15.1784, 257.5651,  15.6739,  19.6679,  12.7490,\n",
       "             15.3999,  11.3920,  12.6618,  15.7226], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([ 10.2494,  12.5156,  12.8031,  13.3474, 338.0425,  14.5115,  14.6631,\n",
       "             10.3591,  15.6007,  16.3659,  94.4689,   9.2521,  17.2212,  16.9561,\n",
       "             14.8388,  16.7284,  21.3709, 219.2755,  12.4268,  10.3159,  16.8190,\n",
       "             15.3080,  31.2762,  17.7347,  14.3965,  17.5094, 526.0626,  16.2340,\n",
       "             14.4482,  11.1518,  13.4794,  13.8594,  23.1245,  20.3580, 737.4155,\n",
       "             15.1129,  15.4017,  15.8673,  11.8710,  14.0997,  12.6891, 258.5021,\n",
       "             16.1929,  13.2325,  15.3549,  26.7854,  14.2448,  14.0056,  18.6340,\n",
       "              9.4495,  10.4695,  13.0597,  13.9280,   9.8620,  11.0900,  17.3457,\n",
       "             17.4736,  16.7963, 142.2948,  13.5205,  15.3682,  11.2126,  13.2693,\n",
       "             15.5419,  19.5592,  16.7212,  18.0334,  14.8919,  13.5743,  16.3662,\n",
       "             19.4421, 185.4693,  29.0918,  18.2367, 295.9212,  14.4653,  19.9227,\n",
       "             18.0093,  14.8751,  13.4463,  11.7927,  13.2696,  12.9247,  26.6298,\n",
       "             13.4691,  16.2390,  13.5803,  14.9268,  18.8596,  49.3959,  16.1982,\n",
       "             12.3461,  20.5927,  12.1087,  15.9828,  56.8467,  15.4228,  18.8613,\n",
       "             12.8280,  10.3011, 147.9868,  15.2076,  16.1861,  17.3328,  12.0454,\n",
       "             27.0962,  20.1214,  13.6737,  13.9708,  16.5281,  24.2800,  11.7290,\n",
       "             12.6079,  12.4948,  25.5825, 390.1921,  16.4964,  10.6209,  12.9285,\n",
       "             18.2408,  13.3904,  43.2929,  14.8094,  18.9786,  15.8143,   9.9788,\n",
       "            618.9303,  16.3291,  13.0518,  15.4799,  16.7070,  12.5466,  12.6877,\n",
       "             19.0205,  11.8704,  12.8497,  15.5439,  17.5796,  10.6701,  84.7244,\n",
       "             18.4192,  21.3478,  16.3860,  14.4559,  15.9451,  13.7127,  19.8847,\n",
       "             17.4340,  19.3032,  12.9199,  10.9473, 104.0810, 913.3231,  13.8772,\n",
       "             15.1733,  58.7437,  28.8854,  12.1218,  20.8765, 175.5482,   9.4157,\n",
       "             13.9533,  14.4888,  15.6702,  14.5097,  12.2836,  18.0930,  20.7896,\n",
       "             12.8497,  13.5855,  12.4576,  15.1818,  16.5489,  26.2839,  34.3052,\n",
       "             17.1227,  14.0618,  13.6137,  15.9237,  19.6715,  16.7077,  16.0735,\n",
       "             17.5571,  17.5398,  14.1487,   9.6100,  13.3946,  12.1227,   9.0980,\n",
       "             13.6028,  14.9279,  18.4030,  12.1474,  16.6713,  16.6572,  28.9341,\n",
       "             10.2929,  14.2554,  13.7592,  34.6579,  17.5508,  12.8255,  75.9886,\n",
       "             19.5247,   9.8925,  25.7365,  21.2154,  13.6167,  11.2076,  14.5520,\n",
       "             15.0475,  14.4455,  17.2986,  21.3587,   9.4989,  18.7491,  25.8302,\n",
       "             11.5298, 123.9961,  11.9966,  13.0114,   9.0998,  14.4503,  10.8402,\n",
       "             25.3804,  17.8074,  15.9035,  13.9098,  14.8237,  16.4687,  30.6013,\n",
       "             72.6806,  10.3534,  13.3535,  16.8604,  10.4857,  15.4690,  13.3141,\n",
       "             21.3745,  13.5737,  18.6759,  19.8074,  13.5237,  15.5267,  16.5055,\n",
       "             12.7552,  11.7462,  16.7549, 286.0228,  17.1691,  21.7374,  14.0688,\n",
       "             17.0073,  12.5912,  13.9699,  17.3430], device='cuda:0')},\n",
       "   4: {'step': tensor(47976.),\n",
       "    'exp_avg': tensor([[0.0353, 0.0394, 0.0131,  ..., 0.0173, 0.0382, 0.0356],\n",
       "            [0.0336, 0.0369, 0.0124,  ..., 0.0164, 0.0360, 0.0336],\n",
       "            [0.0310, 0.0333, 0.0121,  ..., 0.0160, 0.0333, 0.0321],\n",
       "            ...,\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0364, 0.0401, 0.0129,  ..., 0.0171, 0.0391, 0.0363],\n",
       "            [0.0334, 0.0370, 0.0123,  ..., 0.0163, 0.0356, 0.0334]],\n",
       "           device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[0.0904, 0.0826, 0.0307,  ..., 0.0484, 0.0828, 0.0696],\n",
       "            [0.0804, 0.0734, 0.0275,  ..., 0.0433, 0.0737, 0.0620],\n",
       "            [0.0777, 0.0712, 0.0261,  ..., 0.0411, 0.0712, 0.0596],\n",
       "            ...,\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0872, 0.0795, 0.0299,  ..., 0.0471, 0.0799, 0.0674],\n",
       "            [0.0796, 0.0728, 0.0270,  ..., 0.0425, 0.0730, 0.0613]],\n",
       "           device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([[0.0986, 0.0900, 0.0332,  ..., 0.0530, 0.0903, 0.0757],\n",
       "            [0.0877, 0.0800, 0.0297,  ..., 0.0474, 0.0803, 0.0674],\n",
       "            [0.0847, 0.0775, 0.0282,  ..., 0.0450, 0.0775, 0.0648],\n",
       "            ...,\n",
       "            [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "            [0.0951, 0.0866, 0.0323,  ..., 0.0516, 0.0871, 0.0732],\n",
       "            [0.0869, 0.0794, 0.0292,  ..., 0.0466, 0.0795, 0.0666]],\n",
       "           device='cuda:0')},\n",
       "   5: {'step': tensor(47976.),\n",
       "    'exp_avg': tensor([ 4.8403e-02,  4.4579e-02,  3.3210e-02,  5.6052e-45,  4.3548e-02,\n",
       "             5.6052e-45,  3.9372e-02,  3.9841e-02,  4.9260e-02,  5.6052e-45,\n",
       "             4.3047e-02,  3.9063e-02,  4.3511e-02,  0.0000e+00,  4.7210e-02,\n",
       "             5.0616e-02,  5.6052e-45,  4.7427e-02,  4.6780e-02,  4.5220e-02,\n",
       "             3.8869e-02,  5.0842e-02,  5.6052e-45,  2.7609e-02,  5.1553e-02,\n",
       "             4.9499e-02,  5.6052e-45,  3.8593e-02,  4.9131e-02,  4.2602e-02,\n",
       "             4.8263e-02,  4.6237e-02,  4.2854e-02,  4.6543e-02,  4.5550e-02,\n",
       "             4.4851e-02,  4.2303e-02,  0.0000e+00,  3.8729e-02,  5.6052e-45,\n",
       "             4.4020e-02,  4.7846e-02,  4.6460e-02,  4.4121e-02,  4.4958e-02,\n",
       "             7.8452e-11,  4.6956e-02,  4.8423e-02,  4.7183e-02,  4.5260e-02,\n",
       "             4.1012e-02,  0.0000e+00,  4.5241e-02,  5.6052e-45,  4.7939e-02,\n",
       "             4.5989e-02,  4.3051e-02,  5.6052e-45,  3.8953e-02,  4.1263e-02,\n",
       "             4.0730e-02,  3.7837e-02,  0.0000e+00,  4.8721e-02,  4.0166e-02,\n",
       "             0.0000e+00,  5.6052e-45,  4.8250e-02,  5.6052e-45,  5.6052e-45,\n",
       "             2.7442e-02,  4.6624e-02,  0.0000e+00,  4.0633e-02,  6.1988e-02,\n",
       "             4.3124e-02,  2.3437e-19,  3.9829e-02,  4.2917e-02,  4.3753e-02,\n",
       "             4.5061e-02,  5.0454e-02,  4.0194e-02,  4.4823e-02,  5.6052e-45,\n",
       "             5.6052e-45,  4.0995e-02,  5.6052e-45,  4.0625e-02,  4.5708e-02,\n",
       "             5.0094e-02,  3.7515e-02,  4.5795e-02,  3.8372e-02,  6.9863e-03,\n",
       "             0.0000e+00,  0.0000e+00,  4.5771e-02,  4.0986e-02,  4.9533e-02,\n",
       "             5.6052e-45, -5.6052e-45,  5.6052e-45,  4.6069e-02,  5.6052e-45,\n",
       "             4.3463e-02,  4.4563e-02,  4.8011e-02,  2.6654e-02,  4.7381e-02,\n",
       "             3.8383e-02,  3.9742e-02,  4.6756e-02,  4.6830e-02,  4.9979e-02,\n",
       "             3.9791e-02,  4.4903e-02,  4.3396e-02,  5.6052e-45,  5.6052e-45,\n",
       "             4.8658e-02,  5.3151e-02,  0.0000e+00,  2.7803e-02,  3.1516e-02,\n",
       "             5.6052e-45,  4.1018e-02,  5.3756e-02,  4.5530e-02,  4.0712e-02,\n",
       "             3.3128e-02,  4.7485e-02,  4.7733e-02,  4.1460e-02,  5.6052e-45,\n",
       "             4.5737e-02,  4.7510e-02,  4.6858e-02,  4.7342e-02,  4.5212e-02,\n",
       "             3.6642e-02,  0.0000e+00,  4.4540e-02,  4.1662e-02,  4.2569e-02,\n",
       "             3.8991e-02,  4.7444e-02,  4.7180e-02,  4.4621e-02,  5.1232e-02,\n",
       "             4.4519e-02,  0.0000e+00,  5.7206e-02,  5.1953e-02,  3.9940e-02,\n",
       "             5.6052e-45,  5.1664e-02,  4.6440e-02,  3.8773e-02,  4.9252e-02,\n",
       "             4.7019e-02,  5.6052e-45,  5.6052e-45,  4.3647e-02,  4.2941e-02,\n",
       "             5.0135e-02,  4.0034e-02,  5.6052e-45,  5.6052e-45,  4.8037e-02,\n",
       "            -5.6052e-45,  4.7869e-02,  3.4975e-02,  4.2758e-02,  5.6052e-45,\n",
       "             5.6052e-45,  3.4281e-02,  4.2472e-02,  3.7890e-02,  3.4087e-02,\n",
       "             4.0641e-02,  4.7008e-02,  4.9008e-02,  3.8702e-02,  4.2681e-02,\n",
       "             5.6052e-45,  6.1845e-03,  5.1499e-02,  4.6987e-02,  4.3944e-02,\n",
       "             4.6447e-02,  4.0679e-02,  5.0037e-02,  4.3670e-02,  4.9512e-02,\n",
       "             4.6588e-02,  4.3143e-02,  4.2242e-02,  4.9523e-02,  5.6052e-45,\n",
       "             2.7881e-02,  0.0000e+00,  5.6052e-45,  4.8468e-02,  3.2260e-02,\n",
       "             5.6052e-45,  4.5960e-02,  4.3549e-02,  5.6052e-45,  4.4678e-02,\n",
       "             4.6082e-02,  2.8045e-02,  5.6052e-45,  1.5797e-02, -5.6052e-45,\n",
       "             5.6052e-45,  5.6052e-45,  4.2599e-02,  2.8230e-02,  0.0000e+00,\n",
       "             3.3577e-02,  5.5865e-02,  4.5046e-02,  4.7014e-02,  4.2522e-02,\n",
       "             2.4140e-03,  4.6777e-02,  5.6052e-45,  4.5398e-02,  2.9498e-02,\n",
       "             4.9131e-02,  5.6052e-45,  3.9181e-02,  0.0000e+00,  4.3339e-02,\n",
       "             4.3727e-02,  4.7419e-02,  5.6052e-45,  4.9937e-02,  4.1681e-02,\n",
       "             4.5549e-02,  4.5325e-02,  3.7529e-02,  3.9395e-02,  5.6052e-45,\n",
       "             2.6033e-02,  4.4897e-02,  5.6052e-45,  4.6542e-02,  3.0052e-02,\n",
       "             4.8142e-02,  4.2017e-02,  4.3076e-02,  0.0000e+00,  5.0314e-02,\n",
       "             4.5871e-02], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([1.7815e-01, 1.5551e-01, 1.5935e-01, 3.7257e-34, 1.5678e-01, 1.2810e-30,\n",
       "            1.6644e-01, 1.5752e-01, 1.6740e-01, 5.9055e-13, 1.5033e-01, 1.5439e-01,\n",
       "            1.5773e-01, 0.0000e+00, 1.5720e-01, 1.7644e-01, 2.7654e-27, 1.7427e-01,\n",
       "            1.5361e-01, 1.5247e-01, 1.6075e-01, 1.6168e-01, 4.5282e-34, 1.6140e-01,\n",
       "            1.7566e-01, 1.7633e-01, 5.2941e-30, 1.6503e-01, 1.5668e-01, 1.4548e-01,\n",
       "            1.7505e-01, 1.7131e-01, 1.5602e-01, 1.5573e-01, 1.6366e-01, 1.5761e-01,\n",
       "            1.6551e-01, 0.0000e+00, 1.5927e-01, 1.7177e-33, 1.5653e-01, 1.5843e-01,\n",
       "            1.5473e-01, 1.6877e-01, 1.6211e-01, 9.7774e-12, 1.7384e-01, 1.6215e-01,\n",
       "            1.6419e-01, 1.6303e-01, 1.5924e-01, 0.0000e+00, 1.5789e-01, 8.6978e-37,\n",
       "            1.5044e-01, 1.6257e-01, 1.6031e-01, 1.1334e-13, 1.5761e-01, 1.5839e-01,\n",
       "            1.5924e-01, 1.5261e-01, 0.0000e+00, 1.7021e-01, 1.6329e-01, 0.0000e+00,\n",
       "            1.1354e-35, 1.8158e-01, 4.4391e-31, 6.0337e-14, 1.4926e-01, 1.5781e-01,\n",
       "            0.0000e+00, 1.7826e-01, 1.5267e-01, 1.5895e-01, 1.4289e-11, 1.5734e-01,\n",
       "            1.4853e-01, 1.4437e-01, 1.5265e-01, 1.6915e-01, 1.5347e-01, 1.5718e-01,\n",
       "            9.8022e-30, 2.6309e-36, 1.4975e-01, 2.1407e-23, 1.7237e-01, 1.6035e-01,\n",
       "            1.6256e-01, 1.4804e-01, 1.6697e-01, 1.6419e-01, 1.1927e-01, 0.0000e+00,\n",
       "            0.0000e+00, 1.5779e-01, 1.5257e-01, 1.6722e-01, 1.1456e-31, 5.0725e-31,\n",
       "            5.1764e-33, 1.6328e-01, 3.3134e-31, 1.6501e-01, 1.4840e-01, 1.7439e-01,\n",
       "            1.5662e-01, 1.6297e-01, 1.6114e-01, 1.5436e-01, 1.5975e-01, 1.7670e-01,\n",
       "            1.6097e-01, 1.5944e-01, 1.6264e-01, 1.4620e-01, 3.1187e-30, 5.2851e-10,\n",
       "            1.6152e-01, 1.6766e-01, 0.0000e+00, 1.5229e-01, 1.5608e-01, 2.1714e-12,\n",
       "            1.6467e-01, 1.6975e-01, 1.6033e-01, 1.5848e-01, 1.6235e-01, 1.8214e-01,\n",
       "            1.6144e-01, 1.7043e-01, 2.4277e-36, 1.7248e-01, 1.7356e-01, 1.7102e-01,\n",
       "            1.6399e-01, 1.8164e-01, 1.6578e-01, 0.0000e+00, 1.7106e-01, 1.6625e-01,\n",
       "            1.6247e-01, 1.6833e-01, 1.6911e-01, 1.5925e-01, 1.5742e-01, 1.4862e-01,\n",
       "            1.5344e-01, 0.0000e+00, 1.6173e-01, 1.7732e-01, 1.6025e-01, 5.4117e-31,\n",
       "            1.6558e-01, 1.6011e-01, 1.5315e-01, 1.6293e-01, 1.5979e-01, 4.2868e-31,\n",
       "            1.2058e-29, 1.4447e-01, 1.5606e-01, 1.6601e-01, 1.6511e-01, 4.4964e-33,\n",
       "            1.4581e-11, 1.7079e-01, 1.3084e-30, 1.5535e-01, 1.5413e-01, 1.4923e-01,\n",
       "            1.1625e-30, 9.7827e-33, 1.5993e-01, 1.5759e-01, 1.5766e-01, 1.6083e-01,\n",
       "            1.5902e-01, 1.8029e-01, 1.5187e-01, 1.5113e-01, 1.5625e-01, 1.3448e-31,\n",
       "            2.5548e-01, 1.6385e-01, 1.6967e-01, 1.6767e-01, 1.6332e-01, 1.6383e-01,\n",
       "            1.8057e-01, 1.5959e-01, 1.5330e-01, 1.6069e-01, 1.5786e-01, 1.6001e-01,\n",
       "            1.6558e-01, 6.6344e-36, 1.4198e-01, 0.0000e+00, 1.8093e-10, 1.6027e-01,\n",
       "            1.7536e-01, 2.1779e-30, 1.5822e-01, 1.6773e-01, 1.3763e-30, 1.5802e-01,\n",
       "            1.7014e-01, 1.5097e-01, 4.4508e-31, 6.0490e-02, 9.4411e-18, 2.7584e-34,\n",
       "            2.7141e-33, 1.5404e-01, 1.7920e-01, 0.0000e+00, 1.6332e-01, 1.5902e-01,\n",
       "            1.6163e-01, 1.5279e-01, 1.4969e-01, 1.2295e-02, 1.5799e-01, 2.0679e-22,\n",
       "            1.6611e-01, 1.4973e-01, 1.6743e-01, 5.9348e-33, 1.5960e-01, 0.0000e+00,\n",
       "            1.6648e-01, 1.5881e-01, 1.5956e-01, 4.1754e-14, 1.4409e-01, 1.7175e-01,\n",
       "            1.5986e-01, 1.8114e-01, 1.6506e-01, 1.5983e-01, 9.9126e-33, 1.6217e-01,\n",
       "            1.6832e-01, 5.9526e-17, 1.6818e-01, 1.5341e-01, 1.5560e-01, 1.6549e-01,\n",
       "            1.7397e-01, 0.0000e+00, 1.6638e-01, 1.5891e-01], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([1.9704e-01, 1.7217e-01, 1.7564e-01, 1.0391e-14, 1.7337e-01, 8.7377e-10,\n",
       "            1.8366e-01, 1.7393e-01, 1.8519e-01, 2.7487e-10, 1.6634e-01, 1.7037e-01,\n",
       "            1.7464e-01, 0.0000e+00, 1.7371e-01, 1.9540e-01, 1.6121e-06, 1.9285e-01,\n",
       "            1.7001e-01, 1.6877e-01, 1.7745e-01, 1.7898e-01, 3.1031e-13, 1.7774e-01,\n",
       "            1.9438e-01, 1.9531e-01, 3.6063e-09, 1.8209e-01, 1.7354e-01, 1.6114e-01,\n",
       "            1.9388e-01, 1.8967e-01, 1.7274e-01, 1.7234e-01, 1.8128e-01, 1.7428e-01,\n",
       "            1.8273e-01, 0.0000e+00, 1.7589e-01, 1.1242e-12, 1.7344e-01, 1.7514e-01,\n",
       "            1.7133e-01, 1.8680e-01, 1.7944e-01, 1.0937e-11, 1.9241e-01, 1.7950e-01,\n",
       "            1.8168e-01, 1.8010e-01, 1.7588e-01, 0.0000e+00, 1.7448e-01, 6.0931e-16,\n",
       "            1.6637e-01, 1.7983e-01, 1.7743e-01, 4.6517e-09, 1.7397e-01, 1.7487e-01,\n",
       "            1.7581e-01, 1.6840e-01, 0.0000e+00, 1.8812e-01, 1.8034e-01, 0.0000e+00,\n",
       "            7.8591e-15, 2.0113e-01, 3.0253e-10, 5.7829e-11, 1.6441e-01, 1.7453e-01,\n",
       "            0.0000e+00, 1.9678e-01, 1.6907e-01, 1.7583e-01, 1.9311e-11, 1.7361e-01,\n",
       "            1.6423e-01, 1.5994e-01, 1.6869e-01, 1.8699e-01, 1.6992e-01, 1.7387e-01,\n",
       "            6.5814e-09, 1.7850e-15, 1.6569e-01, 6.2370e-14, 1.9022e-01, 1.7761e-01,\n",
       "            1.8001e-01, 1.6338e-01, 1.8491e-01, 1.8119e-01, 1.2843e-01, 0.0000e+00,\n",
       "            0.0000e+00, 1.7439e-01, 1.6847e-01, 1.8518e-01, 7.7255e-11, 3.4568e-10,\n",
       "            3.5493e-12, 1.8081e-01, 2.2553e-10, 1.8260e-01, 1.6423e-01, 1.9303e-01,\n",
       "            1.7262e-01, 1.8032e-01, 1.7789e-01, 1.7047e-01, 1.7688e-01, 1.9556e-01,\n",
       "            1.7813e-01, 1.7597e-01, 1.7973e-01, 1.6153e-01, 2.1282e-09, 6.1319e-09,\n",
       "            1.7863e-01, 1.8576e-01, 0.0000e+00, 1.6775e-01, 1.7202e-01, 3.5607e-08,\n",
       "            1.8186e-01, 1.8791e-01, 1.7751e-01, 1.7496e-01, 1.7899e-01, 2.0149e-01,\n",
       "            1.7841e-01, 1.8815e-01, 1.6887e-15, 1.9098e-01, 1.9220e-01, 1.8916e-01,\n",
       "            1.8128e-01, 2.0116e-01, 1.8290e-01, 0.0000e+00, 1.8929e-01, 1.8372e-01,\n",
       "            1.7947e-01, 1.8587e-01, 1.8705e-01, 1.7624e-01, 1.7408e-01, 1.6449e-01,\n",
       "            1.6963e-01, 0.0000e+00, 1.7912e-01, 1.9639e-01, 1.7689e-01, 3.7356e-10,\n",
       "            1.8331e-01, 1.7690e-01, 1.6906e-01, 1.8039e-01, 1.7667e-01, 2.9204e-10,\n",
       "            8.2402e-09, 1.5999e-01, 1.7276e-01, 1.8379e-01, 1.8232e-01, 3.0761e-12,\n",
       "            1.0314e-10, 1.8909e-01, 8.8294e-10, 1.7196e-01, 1.7004e-01, 1.6518e-01,\n",
       "            7.9274e-10, 6.6839e-12, 1.7634e-01, 1.7447e-01, 1.7401e-01, 1.7738e-01,\n",
       "            1.7560e-01, 1.9914e-01, 1.6815e-01, 1.6686e-01, 1.7269e-01, 6.2509e-11,\n",
       "            2.7612e-01, 1.8130e-01, 1.8759e-01, 1.8553e-01, 1.8076e-01, 1.8080e-01,\n",
       "            1.9997e-01, 1.7636e-01, 1.6963e-01, 1.7786e-01, 1.7459e-01, 1.7707e-01,\n",
       "            1.8340e-01, 4.4832e-15, 1.5646e-01, 0.0000e+00, 6.1660e-09, 1.7737e-01,\n",
       "            1.9332e-01, 1.4708e-09, 1.7494e-01, 1.8569e-01, 9.3685e-10, 1.7491e-01,\n",
       "            1.8814e-01, 1.6629e-01, 3.0267e-10, 6.4452e-02, 1.6290e-11, 1.9055e-13,\n",
       "            4.6902e-13, 1.7038e-01, 1.9742e-01, 0.0000e+00, 1.8006e-01, 1.7600e-01,\n",
       "            1.7899e-01, 1.6915e-01, 1.6563e-01, 1.3030e-02, 1.7497e-01, 2.3702e-12,\n",
       "            1.8368e-01, 1.6501e-01, 1.8531e-01, 4.0854e-12, 1.7615e-01, 0.0000e+00,\n",
       "            1.8385e-01, 1.7571e-01, 1.7648e-01, 4.9087e-10, 1.5937e-01, 1.8967e-01,\n",
       "            1.7663e-01, 2.0045e-01, 1.8200e-01, 1.7648e-01, 6.7524e-12, 1.7858e-01,\n",
       "            1.8633e-01, 2.1859e-12, 1.8618e-01, 1.6903e-01, 1.7207e-01, 1.8279e-01,\n",
       "            1.9252e-01, 0.0000e+00, 1.8421e-01, 1.7565e-01], device='cuda:0')},\n",
       "   6: {'step': tensor(47976.),\n",
       "    'exp_avg': tensor([[0.0080, 0.0084, 0.0103,  ..., 0.0000, 0.0084, 0.0089],\n",
       "            [0.0085, 0.0089, 0.0110,  ..., 0.0000, 0.0090, 0.0095],\n",
       "            [0.0084, 0.0087, 0.0107,  ..., 0.0000, 0.0088, 0.0093],\n",
       "            ...,\n",
       "            [0.0085, 0.0089, 0.0109,  ..., 0.0000, 0.0089, 0.0094],\n",
       "            [0.0079, 0.0083, 0.0102,  ..., 0.0000, 0.0083, 0.0088],\n",
       "            [0.0085, 0.0089, 0.0110,  ..., 0.0000, 0.0089, 0.0095]],\n",
       "           device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[0.0076, 0.0084, 0.0107,  ..., 0.0000, 0.0084, 0.0089],\n",
       "            [0.0086, 0.0096, 0.0123,  ..., 0.0000, 0.0096, 0.0102],\n",
       "            [0.0082, 0.0091, 0.0117,  ..., 0.0000, 0.0091, 0.0097],\n",
       "            ...,\n",
       "            [0.0085, 0.0095, 0.0122,  ..., 0.0000, 0.0095, 0.0100],\n",
       "            [0.0074, 0.0082, 0.0106,  ..., 0.0000, 0.0082, 0.0087],\n",
       "            [0.0086, 0.0095, 0.0123,  ..., 0.0000, 0.0096, 0.0101]],\n",
       "           device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([[0.0082, 0.0090, 0.0116,  ..., 0.0000, 0.0091, 0.0096],\n",
       "            [0.0093, 0.0103, 0.0133,  ..., 0.0000, 0.0104, 0.0110],\n",
       "            [0.0089, 0.0099, 0.0127,  ..., 0.0000, 0.0099, 0.0105],\n",
       "            ...,\n",
       "            [0.0092, 0.0102, 0.0131,  ..., 0.0000, 0.0102, 0.0109],\n",
       "            [0.0080, 0.0089, 0.0114,  ..., 0.0000, 0.0089, 0.0094],\n",
       "            [0.0093, 0.0103, 0.0132,  ..., 0.0000, 0.0103, 0.0109]],\n",
       "           device='cuda:0')},\n",
       "   7: {'step': tensor(47976.),\n",
       "    'exp_avg': tensor([ 7.7003e-04,  8.1982e-04,  8.0172e-04,  7.9278e-04, -1.4471e-03,\n",
       "            -2.1411e-04,  0.0000e+00,  8.1836e-04,  7.8671e-04,  8.0928e-04,\n",
       "             8.1618e-04,  8.0051e-04,  8.1227e-04,  8.1396e-04,  8.0631e-04,\n",
       "             2.4604e-04,  8.1221e-04,  8.0005e-04,  8.1392e-04,  0.0000e+00,\n",
       "             8.1124e-04,  1.7270e-04, -9.3503e-04,  8.1284e-04,  8.2408e-04,\n",
       "             5.6052e-45,  5.6052e-45,  2.7685e-11,  8.2221e-04, -3.3334e-04,\n",
       "             8.2179e-04,  8.2346e-04,  8.0468e-04,  8.0366e-04,  8.0259e-04,\n",
       "             8.0500e-04,  7.8313e-04,  8.0573e-04,  5.6052e-45,  8.0041e-04,\n",
       "             5.6052e-45,  8.1420e-04,  5.6052e-45,  0.0000e+00, -3.6622e-04,\n",
       "             5.6052e-45,  7.7569e-04,  7.3277e-04,  8.1793e-04,  8.0616e-04,\n",
       "             5.6052e-45,  1.1752e-03,  8.1178e-04,  8.0254e-04,  8.2431e-04,\n",
       "             8.1872e-04,  8.1563e-04,  8.1039e-04,  7.8851e-04,  6.9405e-04,\n",
       "             5.6052e-45,  8.1515e-04,  7.8994e-04, -2.2206e-04,  7.8163e-04,\n",
       "             8.2064e-04,  7.8598e-04,  8.1585e-04,  0.0000e+00,  5.6052e-45,\n",
       "             8.0724e-04,  8.1366e-04,  8.2440e-04,  8.1153e-04,  8.0812e-04,\n",
       "             7.9053e-04,  7.9675e-04,  8.2839e-04,  5.6052e-45,  8.0076e-04,\n",
       "             8.0184e-04,  0.0000e+00,  5.6052e-45,  8.1746e-04,  8.0506e-04,\n",
       "             8.0958e-04,  0.0000e+00, -1.8468e-04,  8.1312e-04,  5.6052e-45,\n",
       "             0.0000e+00,  0.0000e+00,  8.1778e-04,  7.5901e-04,  8.1176e-04,\n",
       "             8.0402e-04,  7.8679e-04,  8.1274e-04,  8.0773e-04,  8.0349e-04,\n",
       "             8.1176e-04,  8.0675e-04,  8.1653e-04,  7.8246e-04,  5.6052e-45,\n",
       "             8.1371e-04,  8.2025e-04,  8.1820e-04,  8.0936e-04,  8.1753e-04,\n",
       "             8.1494e-04,  7.9259e-04,  5.6052e-45,  0.0000e+00,  8.1032e-04,\n",
       "             8.0449e-04,  8.1989e-04,  8.2615e-04,  8.0518e-04,  8.2339e-04,\n",
       "             6.6167e-04,  7.9883e-04,  8.0520e-04, -2.2081e-04, -1.3851e-03,\n",
       "            -6.4082e-04,  8.1306e-04,  8.1321e-04,  1.7554e-12,  7.8097e-04,\n",
       "             8.1261e-04,  8.1597e-04, -8.7533e-05,  8.1079e-04,  5.3283e-04,\n",
       "             8.1144e-04,  8.1690e-04,  8.0752e-04,  8.1161e-04,  8.0845e-04,\n",
       "             8.1975e-04,  8.2549e-04,  0.0000e+00,  7.8585e-04,  8.1386e-04,\n",
       "             8.0772e-04,  8.1099e-04,  8.0813e-04,  7.7303e-04,  8.0145e-04,\n",
       "             7.9096e-04,  8.2221e-04,  8.1843e-04,  1.8168e-07,  7.9989e-04,\n",
       "            -5.5313e-04,  8.0379e-04,  0.0000e+00,  8.1423e-04,  0.0000e+00,\n",
       "             0.0000e+00,  8.1426e-04,  5.6052e-45,  5.6052e-45,  8.0815e-04,\n",
       "             8.1445e-04,  8.0773e-04,  8.1863e-04,  7.9829e-04,  8.0661e-04,\n",
       "            -5.0493e-04,  0.0000e+00,  7.6447e-05,  7.9875e-04,  8.2346e-04,\n",
       "             7.7074e-04,  7.7746e-04,  8.1829e-04,  2.7278e-04,  7.9924e-04,\n",
       "             7.7632e-04,  7.5073e-04,  8.0453e-04,  7.2720e-04,  8.1033e-04,\n",
       "             8.1743e-04,  8.1033e-04,  8.1199e-04,  8.2455e-04,  8.0211e-04,\n",
       "            -1.6963e-04,  4.2355e-10,  7.8319e-04,  8.1386e-04, -9.6910e-04,\n",
       "             8.0649e-04,  8.0847e-04,  2.7327e-04, -5.0974e-04,  7.9895e-04,\n",
       "             8.2124e-04, -4.0873e-04,  7.9906e-04,  7.9878e-04,  8.0803e-04,\n",
       "             8.2604e-04,  7.9870e-04,  3.2295e-21,  8.1168e-04,  8.1531e-04,\n",
       "             8.1307e-04,  5.6052e-45,  8.0683e-04,  7.7933e-04,  8.0696e-04,\n",
       "             7.5841e-04,  0.0000e+00, -5.5578e-04,  5.6052e-45,  5.6052e-45,\n",
       "             8.1353e-04, -7.5439e-04,  7.7973e-04,  8.1138e-04,  0.0000e+00,\n",
       "             7.8885e-04,  8.1870e-04,  8.1640e-04,  8.1736e-04,  8.1513e-04,\n",
       "             8.1780e-04,  8.2251e-04,  8.1534e-04,  0.0000e+00,  8.0878e-04,\n",
       "             7.8334e-04,  8.1876e-04,  8.1721e-04,  0.0000e+00,  7.9015e-04,\n",
       "             8.1410e-04,  7.8522e-04,  8.0759e-04,  8.0836e-04, -4.8643e-04,\n",
       "             8.1132e-04,  8.1089e-04,  8.1417e-04,  8.1250e-04,  7.7087e-04,\n",
       "             5.6052e-45, -4.3406e-04,  8.0276e-04,  8.1483e-04,  8.0704e-04,\n",
       "             8.1558e-04,  8.2045e-04,  7.9263e-04,  8.0026e-04,  8.1148e-04,\n",
       "             5.6052e-45,  8.2302e-04,  5.6052e-45,  8.1182e-04,  8.0632e-04,\n",
       "             7.8340e-04,  2.7027e-11,  5.6052e-45,  8.1202e-04, -5.6511e-04,\n",
       "             0.0000e+00,  8.1849e-04,  8.1518e-04, -1.1532e-03,  8.1185e-04,\n",
       "             7.5845e-04,  8.0865e-04,  3.1887e-34,  8.1337e-04,  0.0000e+00,\n",
       "             8.0812e-04,  7.9779e-04, -9.2925e-04,  8.2093e-04,  7.8382e-04,\n",
       "             7.9819e-04,  0.0000e+00,  5.6052e-45,  5.6052e-45,  8.0753e-04,\n",
       "             8.1217e-04,  8.0899e-04,  7.9189e-04,  3.0503e-04,  1.6854e-27,\n",
       "             8.1099e-04,  8.1927e-04,  8.1803e-04,  0.0000e+00,  1.6816e-44,\n",
       "             7.9481e-04,  8.0808e-04,  7.9178e-04,  8.0989e-04,  8.1326e-04,\n",
       "             8.2578e-04,  7.9380e-04,  8.1126e-04,  5.6052e-45,  5.6052e-45,\n",
       "             8.1663e-04,  8.0912e-04,  0.0000e+00,  8.0629e-04,  0.0000e+00,\n",
       "            -6.6186e-04,  2.3916e-38,  5.6052e-45,  8.1412e-04,  8.1626e-04,\n",
       "             0.0000e+00,  7.9310e-04,  8.1642e-04,  8.0575e-04,  8.1221e-04,\n",
       "             8.0225e-04,  8.1840e-04,  8.1360e-04,  8.0018e-04,  7.8959e-04,\n",
       "             8.1371e-04,  8.1607e-04,  8.0330e-04,  8.0854e-04,  8.0323e-04,\n",
       "             8.0799e-04,  8.1815e-04,  8.1346e-04,  5.6052e-45,  8.1323e-04,\n",
       "             1.8737e-04,  8.1687e-04,  1.3077e-04,  8.1847e-04,  8.0622e-04,\n",
       "             8.1074e-04,  0.0000e+00,  8.0532e-04,  7.9900e-04,  8.1880e-04,\n",
       "             8.1462e-04,  7.9551e-04,  7.8716e-04,  8.0518e-04,  8.1076e-04,\n",
       "             8.1162e-04,  7.4967e-04,  5.6052e-45,  8.0131e-04,  7.7047e-04,\n",
       "             5.6052e-45,  8.2450e-04,  8.1801e-04,  7.9180e-04,  8.0647e-04,\n",
       "             7.7884e-04, -2.8931e-04, -6.8562e-04,  7.9513e-04,  8.1851e-04,\n",
       "             8.1180e-04,  8.0081e-04,  8.1407e-04,  7.9747e-04,  7.5045e-04,\n",
       "             8.1690e-04,  8.1198e-04,  8.0533e-04,  0.0000e+00,  3.9580e-12,\n",
       "             8.1005e-04,  8.2032e-04, -5.6837e-04,  7.8014e-04,  7.9019e-04,\n",
       "             7.6680e-04,  8.0842e-04,  7.9135e-04,  8.2007e-04,  8.1645e-04,\n",
       "             8.1027e-04,  8.1779e-04,  8.1095e-04,  8.1125e-04,  8.1086e-04,\n",
       "             7.7763e-04,  0.0000e+00,  8.2218e-04,  8.1276e-04,  7.8417e-04,\n",
       "             8.2658e-04,  8.0414e-04,  5.6052e-45,  8.0351e-04,  7.6840e-04,\n",
       "             8.0668e-04,  0.0000e+00,  8.0031e-04,  8.2273e-04,  7.6489e-04,\n",
       "             8.1460e-04,  1.2039e-04,  8.2067e-04,  8.0718e-04,  1.1737e-03,\n",
       "             8.1856e-04,  5.6052e-45,  7.8807e-04,  8.2144e-04,  7.5116e-04,\n",
       "             0.0000e+00,  8.1275e-04,  7.9224e-04,  7.8757e-04,  8.1456e-04,\n",
       "             8.1000e-04,  0.0000e+00,  0.0000e+00, -3.3338e-04,  7.7515e-04,\n",
       "             7.8464e-04,  3.9349e-04,  0.0000e+00,  6.6759e-04,  5.6052e-45,\n",
       "             8.2244e-04,  1.4259e-34,  0.0000e+00,  8.1591e-04,  8.0285e-04,\n",
       "             8.1097e-04,  8.0624e-04, -1.2190e-03,  5.6052e-45,  7.9952e-04,\n",
       "             8.1951e-04,  8.0024e-04,  8.1328e-04,  8.1730e-04,  5.9434e-04,\n",
       "             8.1396e-04,  8.1349e-04,  7.9151e-04,  5.6052e-45,  5.6052e-45,\n",
       "             8.0868e-04,  8.2184e-04,  7.8292e-04,  0.0000e+00, -1.0748e-03,\n",
       "             8.0757e-04,  7.8689e-04,  8.1585e-04,  8.0218e-04,  8.0451e-04,\n",
       "             7.9806e-04,  8.2017e-04,  8.1030e-04,  8.2058e-04,  8.1910e-04,\n",
       "             0.0000e+00,  0.0000e+00,  8.1258e-04,  7.2646e-04,  5.6052e-45,\n",
       "             8.1340e-04, -1.3041e-03,  4.5744e-04,  8.0492e-04, -1.6091e-03,\n",
       "             8.1735e-04,  8.0540e-04,  8.0973e-04,  7.8009e-04,  8.1177e-04,\n",
       "             5.6052e-45,  8.1449e-04,  8.1918e-04,  8.1393e-04,  8.0633e-04,\n",
       "             8.1992e-04,  8.1145e-04,  8.1069e-04,  8.1390e-04,  8.1860e-04,\n",
       "             2.4875e-11,  8.0445e-04, -1.6381e-03,  8.2983e-04,  2.9433e-04,\n",
       "             8.0261e-04,  8.1118e-04,  3.0824e-09,  7.7790e-04,  7.9714e-04,\n",
       "            -9.3346e-06,  8.0867e-04,  0.0000e+00, -1.3182e-03,  8.1709e-04,\n",
       "             7.6228e-04,  8.1982e-04], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([1.3392e-04, 1.5322e-04, 1.4621e-04, 1.4268e-04, 2.1796e-05, 4.5153e-05,\n",
       "            0.0000e+00, 1.5253e-04, 1.4065e-04, 1.4883e-04, 1.5202e-04, 1.4602e-04,\n",
       "            1.4994e-04, 1.5088e-04, 1.4784e-04, 3.0240e-07, 1.5015e-04, 1.4558e-04,\n",
       "            1.5090e-04, 0.0000e+00, 1.4969e-04, 4.5775e-06, 4.7205e-06, 1.5020e-04,\n",
       "            1.5488e-04, 1.7633e-36, 6.4654e-22, 7.9499e-16, 1.5380e-04, 5.2908e-05,\n",
       "            1.5395e-04, 1.5423e-04, 1.4721e-04, 1.4626e-04, 1.4683e-04, 1.4723e-04,\n",
       "            1.3917e-04, 1.4778e-04, 5.9799e-19, 1.4561e-04, 3.3621e-32, 1.5059e-04,\n",
       "            4.6704e-34, 0.0000e+00, 4.2846e-05, 4.4018e-35, 1.3592e-04, 1.2117e-04,\n",
       "            1.5221e-04, 1.4795e-04, 4.9282e-16, 3.1693e-04, 1.5004e-04, 1.4630e-04,\n",
       "            1.5526e-04, 1.5259e-04, 1.5111e-04, 1.4937e-04, 1.4140e-04, 1.0880e-04,\n",
       "            6.2734e-35, 1.5138e-04, 1.4154e-04, 8.7829e-06, 1.3869e-04, 1.5316e-04,\n",
       "            1.4014e-04, 1.5154e-04, 0.0000e+00, 2.3113e-35, 1.4799e-04, 1.5070e-04,\n",
       "            1.5512e-04, 1.4967e-04, 1.4848e-04, 1.4232e-04, 1.4381e-04, 1.5569e-04,\n",
       "            1.5210e-15, 1.4593e-04, 1.4625e-04, 0.0000e+00, 6.2400e-36, 1.5184e-04,\n",
       "            1.4769e-04, 1.4865e-04, 0.0000e+00, 4.7143e-05, 1.5093e-04, 3.8705e-36,\n",
       "            0.0000e+00, 0.0000e+00, 1.5177e-04, 1.3064e-04, 1.4973e-04, 1.4651e-04,\n",
       "            1.4084e-04, 1.5032e-04, 1.4822e-04, 1.4673e-04, 1.4972e-04, 1.4831e-04,\n",
       "            1.5164e-04, 1.3877e-04, 8.5998e-16, 1.5025e-04, 1.5344e-04, 1.5226e-04,\n",
       "            1.4870e-04, 1.5241e-04, 1.5095e-04, 1.4214e-04, 1.1859e-31, 0.0000e+00,\n",
       "            1.4932e-04, 1.4701e-04, 1.5323e-04, 1.5562e-04, 1.4741e-04, 1.5482e-04,\n",
       "            9.5779e-05, 1.4549e-04, 1.4747e-04, 1.0449e-04, 1.0271e-05, 1.0909e-04,\n",
       "            1.5084e-04, 1.5017e-04, 8.8470e-15, 1.3812e-04, 1.4992e-04, 1.5159e-04,\n",
       "            1.4875e-08, 1.4967e-04, 6.0264e-05, 1.4986e-04, 1.5190e-04, 1.4823e-04,\n",
       "            1.4984e-04, 1.4832e-04, 1.5303e-04, 1.5558e-04, 0.0000e+00, 1.4039e-04,\n",
       "            1.5110e-04, 1.4800e-04, 1.4964e-04, 1.4878e-04, 1.3537e-04, 1.4613e-04,\n",
       "            1.4167e-04, 1.5396e-04, 1.5268e-04, 6.7291e-13, 1.4581e-04, 2.9991e-05,\n",
       "            1.4728e-04, 0.0000e+00, 1.5098e-04, 0.0000e+00, 0.0000e+00, 1.5074e-04,\n",
       "            3.4018e-14, 9.2575e-35, 1.4845e-04, 1.5092e-04, 1.4822e-04, 1.5231e-04,\n",
       "            1.4495e-04, 1.4829e-04, 4.5116e-05, 0.0000e+00, 5.2853e-07, 1.4470e-04,\n",
       "            1.5473e-04, 1.3462e-04, 1.3705e-04, 1.5240e-04, 1.3763e-05, 1.4521e-04,\n",
       "            1.3669e-04, 1.2760e-04, 1.4735e-04, 1.1931e-04, 1.4927e-04, 1.5233e-04,\n",
       "            1.4955e-04, 1.4983e-04, 1.5495e-04, 1.4645e-04, 2.3215e-07, 7.9578e-15,\n",
       "            1.3892e-04, 1.5071e-04, 5.4503e-06, 1.4784e-04, 1.4857e-04, 1.3864e-05,\n",
       "            6.5630e-05, 1.4520e-04, 1.5377e-04, 1.0275e-04, 1.4493e-04, 1.4523e-04,\n",
       "            1.4825e-04, 1.5523e-04, 1.4514e-04, 6.4367e-15, 1.4952e-04, 1.5162e-04,\n",
       "            1.5034e-04, 6.1695e-31, 1.4800e-04, 1.3800e-04, 1.4769e-04, 1.3008e-04,\n",
       "            0.0000e+00, 1.4337e-06, 1.0873e-32, 2.1176e-33, 1.5056e-04, 2.5467e-06,\n",
       "            1.3770e-04, 1.4954e-04, 0.0000e+00, 1.4126e-04, 1.5263e-04, 1.5132e-04,\n",
       "            1.5234e-04, 1.5135e-04, 1.5222e-04, 1.5401e-04, 1.5168e-04, 0.0000e+00,\n",
       "            1.4843e-04, 1.3918e-04, 1.5279e-04, 1.5205e-04, 0.0000e+00, 1.4143e-04,\n",
       "            1.5050e-04, 1.4016e-04, 1.4833e-04, 1.4856e-04, 8.2891e-05, 1.5001e-04,\n",
       "            1.4948e-04, 1.5086e-04, 1.4982e-04, 1.3447e-04, 2.5715e-30, 9.5001e-05,\n",
       "            1.4642e-04, 1.5136e-04, 1.4811e-04, 1.5116e-04, 1.5300e-04, 1.4279e-04,\n",
       "            1.4538e-04, 1.4985e-04, 7.7063e-20, 1.5430e-04, 2.1166e-15, 1.4986e-04,\n",
       "            1.4738e-04, 1.3939e-04, 2.2904e-12, 4.3654e-16, 1.5034e-04, 8.5308e-07,\n",
       "            0.0000e+00, 1.5227e-04, 1.5123e-04, 9.9237e-06, 1.5032e-04, 1.3000e-04,\n",
       "            1.4886e-04, 5.0498e-14, 1.5093e-04, 0.0000e+00, 1.4844e-04, 1.4434e-04,\n",
       "            4.2314e-06, 1.5366e-04, 1.3908e-04, 1.4481e-04, 0.0000e+00, 3.8606e-15,\n",
       "            1.0091e-16, 1.4831e-04, 1.4975e-04, 1.4889e-04, 1.4249e-04, 1.7618e-05,\n",
       "            8.9456e-15, 1.4951e-04, 1.5302e-04, 1.5222e-04, 0.0000e+00, 2.6852e-12,\n",
       "            1.4349e-04, 1.4885e-04, 1.4202e-04, 1.4942e-04, 1.5043e-04, 1.5555e-04,\n",
       "            1.4302e-04, 1.4948e-04, 9.9018e-15, 3.1294e-14, 1.5183e-04, 1.4869e-04,\n",
       "            0.0000e+00, 1.4751e-04, 0.0000e+00, 1.3456e-06, 1.1842e-17, 5.4201e-37,\n",
       "            1.5066e-04, 1.5226e-04, 0.0000e+00, 1.4256e-04, 1.5208e-04, 1.4782e-04,\n",
       "            1.5002e-04, 1.4600e-04, 1.5262e-04, 1.5027e-04, 1.4526e-04, 1.4142e-04,\n",
       "            1.5049e-04, 1.5172e-04, 1.4638e-04, 1.4892e-04, 1.4631e-04, 1.4799e-04,\n",
       "            1.5263e-04, 1.5066e-04, 1.0400e-14, 1.5024e-04, 5.3790e-06, 1.5215e-04,\n",
       "            1.9298e-06, 1.5249e-04, 1.4762e-04, 1.4952e-04, 0.0000e+00, 1.4713e-04,\n",
       "            1.4565e-04, 1.5274e-04, 1.5110e-04, 1.4342e-04, 1.4064e-04, 1.4737e-04,\n",
       "            1.4933e-04, 1.5007e-04, 1.2708e-04, 4.2670e-17, 1.4550e-04, 1.3403e-04,\n",
       "            1.7485e-36, 1.5470e-04, 1.5230e-04, 1.4245e-04, 1.4763e-04, 1.3753e-04,\n",
       "            8.3761e-05, 6.6552e-05, 1.4376e-04, 1.5239e-04, 1.4975e-04, 1.4584e-04,\n",
       "            1.5043e-04, 1.4398e-04, 1.2728e-04, 1.5214e-04, 1.5004e-04, 1.4764e-04,\n",
       "            0.0000e+00, 2.2525e-14, 1.4909e-04, 1.5333e-04, 5.9329e-05, 1.3790e-04,\n",
       "            1.4179e-04, 1.3273e-04, 1.4863e-04, 1.4185e-04, 1.5273e-04, 1.5161e-04,\n",
       "            1.4897e-04, 1.5225e-04, 1.4968e-04, 1.5027e-04, 1.4950e-04, 1.3695e-04,\n",
       "            0.0000e+00, 1.5394e-04, 1.5024e-04, 1.3960e-04, 1.5557e-04, 1.4718e-04,\n",
       "            1.3939e-11, 1.4688e-04, 1.3380e-04, 1.4786e-04, 0.0000e+00, 1.4581e-04,\n",
       "            1.5461e-04, 1.3256e-04, 1.5110e-04, 1.6510e-06, 1.5338e-04, 1.4826e-04,\n",
       "            3.1391e-04, 1.5250e-04, 4.7335e-37, 1.4081e-04, 1.5391e-04, 1.2758e-04,\n",
       "            0.0000e+00, 1.5018e-04, 1.4269e-04, 1.4082e-04, 1.5145e-04, 1.4898e-04,\n",
       "            0.0000e+00, 0.0000e+00, 3.4862e-05, 1.3634e-04, 1.3971e-04, 3.0962e-05,\n",
       "            0.0000e+00, 9.0236e-05, 1.7326e-16, 1.5379e-04, 4.4034e-14, 0.0000e+00,\n",
       "            1.5183e-04, 1.4659e-04, 1.4922e-04, 1.4781e-04, 1.7257e-05, 4.0219e-34,\n",
       "            1.4571e-04, 1.5295e-04, 1.4536e-04, 1.5006e-04, 1.5182e-04, 7.6590e-05,\n",
       "            1.5091e-04, 1.5062e-04, 1.4245e-04, 1.2770e-15, 3.7142e-17, 1.4859e-04,\n",
       "            1.5327e-04, 1.3917e-04, 0.0000e+00, 9.3991e-06, 1.4804e-04, 1.4005e-04,\n",
       "            1.5164e-04, 1.4633e-04, 1.4683e-04, 1.4455e-04, 1.5333e-04, 1.4948e-04,\n",
       "            1.5314e-04, 1.5265e-04, 0.0000e+00, 0.0000e+00, 1.5049e-04, 1.1925e-04,\n",
       "            6.3420e-19, 1.5083e-04, 1.7609e-05, 4.3836e-05, 1.4720e-04, 1.5867e-05,\n",
       "            1.5200e-04, 1.4710e-04, 1.4889e-04, 1.3821e-04, 1.4976e-04, 4.6727e-16,\n",
       "            1.5103e-04, 1.5294e-04, 1.5089e-04, 1.4791e-04, 1.5312e-04, 1.5019e-04,\n",
       "            1.4994e-04, 1.5113e-04, 1.5297e-04, 5.7460e-12, 1.4728e-04, 2.9446e-05,\n",
       "            1.5663e-04, 1.6634e-05, 1.4685e-04, 1.4953e-04, 3.2012e-14, 1.3732e-04,\n",
       "            1.4427e-04, 1.8633e-10, 1.4893e-04, 0.0000e+00, 2.8502e-05, 1.5155e-04,\n",
       "            1.3150e-04, 1.5279e-04], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([1.4850e-04, 1.6997e-04, 1.6217e-04, 1.5825e-04, 2.2167e-05, 4.6512e-05,\n",
       "            0.0000e+00, 1.6921e-04, 1.5598e-04, 1.6509e-04, 1.6863e-04, 1.6195e-04,\n",
       "            1.6632e-04, 1.6737e-04, 1.6398e-04, 3.0240e-07, 1.6655e-04, 1.6147e-04,\n",
       "            1.6738e-04, 0.0000e+00, 1.6605e-04, 4.9098e-06, 4.7447e-06, 1.6661e-04,\n",
       "            1.7182e-04, 1.1832e-15, 7.8219e-15, 4.4744e-10, 1.7061e-04, 5.4130e-05,\n",
       "            1.7079e-04, 1.7109e-04, 1.6328e-04, 1.6223e-04, 1.6286e-04, 1.6331e-04,\n",
       "            1.5433e-04, 1.6392e-04, 2.8967e-17, 1.6151e-04, 7.7847e-13, 1.6704e-04,\n",
       "            3.1624e-13, 0.0000e+00, 4.3683e-05, 2.8984e-14, 1.5073e-04, 1.3431e-04,\n",
       "            1.6885e-04, 1.6410e-04, 1.6895e-15, 3.5190e-04, 1.6643e-04, 1.6227e-04,\n",
       "            1.7224e-04, 1.6927e-04, 1.6762e-04, 1.6569e-04, 1.5682e-04, 1.2055e-04,\n",
       "            4.3423e-14, 1.6793e-04, 1.5698e-04, 8.8670e-06, 1.5381e-04, 1.6991e-04,\n",
       "            1.5542e-04, 1.6810e-04, 0.0000e+00, 1.5510e-14, 1.6415e-04, 1.6716e-04,\n",
       "            1.7209e-04, 1.6601e-04, 1.6470e-04, 1.5784e-04, 1.5951e-04, 1.7272e-04,\n",
       "            3.5414e-13, 1.6186e-04, 1.6222e-04, 0.0000e+00, 4.0270e-15, 1.6844e-04,\n",
       "            1.6381e-04, 1.6489e-04, 0.0000e+00, 4.8666e-05, 1.6742e-04, 2.7060e-15,\n",
       "            0.0000e+00, 0.0000e+00, 1.6836e-04, 1.4485e-04, 1.6608e-04, 1.6250e-04,\n",
       "            1.5620e-04, 1.6674e-04, 1.6441e-04, 1.6275e-04, 1.6608e-04, 1.6450e-04,\n",
       "            1.6821e-04, 1.5389e-04, 1.1554e-14, 1.6667e-04, 1.7022e-04, 1.6891e-04,\n",
       "            1.6494e-04, 1.6907e-04, 1.6745e-04, 1.5764e-04, 6.8695e-11, 0.0000e+00,\n",
       "            1.6563e-04, 1.6306e-04, 1.6998e-04, 1.7264e-04, 1.6350e-04, 1.7175e-04,\n",
       "            1.0598e-04, 1.6137e-04, 1.6357e-04, 1.0839e-04, 1.0333e-05, 1.1337e-04,\n",
       "            1.6732e-04, 1.6657e-04, 2.1253e-14, 1.5318e-04, 1.6630e-04, 1.6815e-04,\n",
       "            1.4875e-08, 1.6602e-04, 6.6525e-05, 1.6623e-04, 1.6850e-04, 1.6442e-04,\n",
       "            1.6620e-04, 1.6452e-04, 1.6975e-04, 1.7259e-04, 0.0000e+00, 1.5570e-04,\n",
       "            1.6761e-04, 1.6417e-04, 1.6599e-04, 1.6503e-04, 1.5012e-04, 1.6208e-04,\n",
       "            1.5711e-04, 1.7080e-04, 1.6936e-04, 5.5257e-11, 1.6172e-04, 3.1736e-05,\n",
       "            1.6336e-04, 0.0000e+00, 1.6748e-04, 0.0000e+00, 0.0000e+00, 1.6721e-04,\n",
       "            1.1748e-11, 6.0164e-14, 1.6466e-04, 1.6741e-04, 1.6440e-04, 1.6896e-04,\n",
       "            1.6078e-04, 1.6448e-04, 4.6426e-05, 0.0000e+00, 5.4783e-07, 1.6049e-04,\n",
       "            1.7164e-04, 1.4927e-04, 1.5197e-04, 1.6905e-04, 1.5017e-05, 1.6106e-04,\n",
       "            1.5158e-04, 1.4147e-04, 1.6344e-04, 1.3225e-04, 1.6558e-04, 1.6898e-04,\n",
       "            1.6589e-04, 1.6620e-04, 1.7189e-04, 1.6244e-04, 2.3323e-07, 1.2156e-13,\n",
       "            1.5406e-04, 1.6718e-04, 5.4615e-06, 1.6398e-04, 1.6480e-04, 1.5137e-05,\n",
       "            6.7872e-05, 1.6104e-04, 1.7059e-04, 1.0771e-04, 1.6074e-04, 1.6108e-04,\n",
       "            1.6444e-04, 1.7220e-04, 1.6098e-04, 1.4847e-14, 1.6585e-04, 1.6819e-04,\n",
       "            1.6677e-04, 8.1658e-12, 1.6416e-04, 1.5304e-04, 1.6382e-04, 1.4422e-04,\n",
       "            0.0000e+00, 1.4358e-06, 7.5744e-12, 8.2444e-17, 1.6701e-04, 2.5538e-06,\n",
       "            1.5270e-04, 1.6588e-04, 0.0000e+00, 1.5666e-04, 1.6932e-04, 1.6786e-04,\n",
       "            1.6899e-04, 1.6789e-04, 1.6886e-04, 1.7085e-04, 1.6826e-04, 0.0000e+00,\n",
       "            1.6463e-04, 1.5435e-04, 1.6950e-04, 1.6866e-04, 0.0000e+00, 1.5686e-04,\n",
       "            1.6695e-04, 1.5544e-04, 1.6452e-04, 1.6478e-04, 8.5741e-05, 1.6639e-04,\n",
       "            1.6581e-04, 1.6734e-04, 1.6619e-04, 1.4911e-04, 1.2084e-09, 9.8967e-05,\n",
       "            1.6240e-04, 1.6789e-04, 1.6428e-04, 1.6768e-04, 1.6972e-04, 1.5837e-04,\n",
       "            1.6125e-04, 1.6622e-04, 1.0204e-16, 1.7118e-04, 5.2804e-14, 1.6623e-04,\n",
       "            1.6348e-04, 1.5458e-04, 8.8810e-12, 6.0538e-15, 1.6676e-04, 8.5378e-07,\n",
       "            0.0000e+00, 1.6892e-04, 1.6776e-04, 9.9526e-06, 1.6675e-04, 1.4414e-04,\n",
       "            1.6511e-04, 1.7011e-13, 1.6742e-04, 0.0000e+00, 1.6466e-04, 1.6010e-04,\n",
       "            4.2474e-06, 1.7046e-04, 1.5424e-04, 1.6061e-04, 0.0000e+00, 3.7363e-13,\n",
       "            9.7981e-14, 1.6450e-04, 1.6611e-04, 1.6515e-04, 1.5804e-04, 1.9279e-05,\n",
       "            1.3977e-14, 1.6584e-04, 1.6974e-04, 1.6886e-04, 0.0000e+00, 1.5791e-11,\n",
       "            1.5915e-04, 1.6510e-04, 1.5751e-04, 1.6574e-04, 1.6687e-04, 1.7256e-04,\n",
       "            1.5862e-04, 1.6581e-04, 1.1414e-13, 1.1580e-12, 1.6842e-04, 1.6493e-04,\n",
       "            0.0000e+00, 1.6361e-04, 0.0000e+00, 1.3490e-06, 7.3710e-10, 3.1428e-16,\n",
       "            1.6712e-04, 1.6890e-04, 0.0000e+00, 1.5811e-04, 1.6871e-04, 1.6396e-04,\n",
       "            1.6641e-04, 1.6193e-04, 1.6930e-04, 1.6669e-04, 1.6112e-04, 1.5684e-04,\n",
       "            1.6694e-04, 1.6830e-04, 1.6236e-04, 1.6518e-04, 1.6228e-04, 1.6415e-04,\n",
       "            1.6931e-04, 1.6712e-04, 8.2868e-09, 1.6665e-04, 5.7703e-06, 1.6878e-04,\n",
       "            2.0320e-06, 1.6916e-04, 1.6374e-04, 1.6586e-04, 0.0000e+00, 1.6320e-04,\n",
       "            1.6155e-04, 1.6944e-04, 1.6760e-04, 1.5907e-04, 1.5597e-04, 1.6346e-04,\n",
       "            1.6564e-04, 1.6647e-04, 1.4089e-04, 2.2167e-11, 1.6138e-04, 1.4862e-04,\n",
       "            1.1639e-15, 1.7162e-04, 1.6895e-04, 1.5799e-04, 1.6376e-04, 1.5252e-04,\n",
       "            8.6727e-05, 6.8676e-05, 1.5944e-04, 1.6905e-04, 1.6611e-04, 1.6176e-04,\n",
       "            1.6687e-04, 1.5969e-04, 1.4112e-04, 1.6876e-04, 1.6644e-04, 1.6376e-04,\n",
       "            0.0000e+00, 2.5145e-14, 1.6538e-04, 1.7009e-04, 6.1078e-05, 1.5293e-04,\n",
       "            1.5725e-04, 1.4718e-04, 1.6487e-04, 1.5732e-04, 1.6942e-04, 1.6817e-04,\n",
       "            1.6525e-04, 1.6889e-04, 1.6604e-04, 1.6669e-04, 1.6583e-04, 1.5186e-04,\n",
       "            0.0000e+00, 1.7077e-04, 1.6666e-04, 1.5482e-04, 1.7259e-04, 1.6324e-04,\n",
       "            8.4616e-10, 1.6292e-04, 1.4837e-04, 1.6401e-04, 0.0000e+00, 1.6172e-04,\n",
       "            1.7151e-04, 1.4698e-04, 1.6761e-04, 1.7401e-06, 1.7015e-04, 1.6445e-04,\n",
       "            3.4836e-04, 1.6917e-04, 3.2438e-16, 1.5616e-04, 1.7073e-04, 1.4144e-04,\n",
       "            0.0000e+00, 1.6659e-04, 1.5825e-04, 1.5617e-04, 1.6799e-04, 1.6526e-04,\n",
       "            0.0000e+00, 0.0000e+00, 3.5243e-05, 1.5119e-04, 1.5494e-04, 3.4034e-05,\n",
       "            0.0000e+00, 9.9262e-05, 2.9102e-14, 1.7060e-04, 1.2759e-12, 0.0000e+00,\n",
       "            1.6842e-04, 1.6259e-04, 1.6552e-04, 1.6395e-04, 1.7333e-05, 2.6034e-13,\n",
       "            1.6161e-04, 1.6967e-04, 1.6123e-04, 1.6645e-04, 1.6842e-04, 8.4665e-05,\n",
       "            1.6740e-04, 1.6708e-04, 1.5799e-04, 1.2623e-14, 7.3893e-16, 1.6482e-04,\n",
       "            1.7003e-04, 1.5433e-04, 0.0000e+00, 9.4400e-06, 1.6421e-04, 1.5532e-04,\n",
       "            1.6821e-04, 1.6230e-04, 1.6286e-04, 1.6033e-04, 1.7009e-04, 1.6581e-04,\n",
       "            1.6988e-04, 1.6933e-04, 0.0000e+00, 0.0000e+00, 1.6694e-04, 1.3218e-04,\n",
       "            2.6709e-15, 1.6731e-04, 1.7781e-05, 4.8342e-05, 1.6327e-04, 1.6032e-05,\n",
       "            1.6862e-04, 1.6316e-04, 1.6515e-04, 1.5327e-04, 1.6612e-04, 5.0496e-15,\n",
       "            1.6753e-04, 1.6965e-04, 1.6738e-04, 1.6407e-04, 1.6986e-04, 1.6659e-04,\n",
       "            1.6632e-04, 1.6764e-04, 1.6969e-04, 2.8242e-11, 1.6336e-04, 3.0122e-05,\n",
       "            1.7376e-04, 1.8213e-05, 1.6288e-04, 1.6586e-04, 4.8563e-14, 1.5228e-04,\n",
       "            1.6002e-04, 1.1951e-08, 1.6519e-04, 0.0000e+00, 2.9680e-05, 1.6812e-04,\n",
       "            1.4580e-04, 1.6949e-04], device='cuda:0')},\n",
       "   8: {'step': tensor(47976.),\n",
       "    'exp_avg': tensor([[0.0016, 0.0018, 0.0018,  ..., 0.0018, 0.0017, 0.0018],\n",
       "            [0.0016, 0.0018, 0.0018,  ..., 0.0018, 0.0017, 0.0018],\n",
       "            [0.0016, 0.0018, 0.0018,  ..., 0.0018, 0.0016, 0.0018],\n",
       "            ...,\n",
       "            [0.0016, 0.0018, 0.0018,  ..., 0.0018, 0.0016, 0.0018],\n",
       "            [0.0016, 0.0018, 0.0018,  ..., 0.0018, 0.0017, 0.0018],\n",
       "            [0.0016, 0.0018, 0.0018,  ..., 0.0018, 0.0016, 0.0017]],\n",
       "           device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([[0.0003, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004],\n",
       "            [0.0003, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004],\n",
       "            [0.0003, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004],\n",
       "            ...,\n",
       "            [0.0003, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004],\n",
       "            [0.0003, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004],\n",
       "            [0.0003, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004]],\n",
       "           device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([[0.0003, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004],\n",
       "            [0.0003, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004],\n",
       "            [0.0003, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004],\n",
       "            ...,\n",
       "            [0.0003, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004],\n",
       "            [0.0003, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004],\n",
       "            [0.0003, 0.0004, 0.0004,  ..., 0.0004, 0.0003, 0.0004]],\n",
       "           device='cuda:0')},\n",
       "   9: {'step': tensor(47976.),\n",
       "    'exp_avg': tensor([9.2026e-06, 9.3314e-06, 7.9283e-06, 7.8301e-06, 8.4788e-06, 9.3647e-06,\n",
       "            8.8296e-06, 7.8939e-06, 8.7193e-06, 8.0220e-06, 8.3862e-06, 8.1681e-06,\n",
       "            7.4580e-06, 7.5872e-06, 7.2149e-06, 9.3422e-06, 8.4122e-06, 8.6076e-06,\n",
       "            7.3715e-06, 9.0016e-06, 9.3271e-06, 6.8492e-06, 8.0170e-06, 7.7253e-06,\n",
       "            9.0417e-06, 8.0617e-06, 8.6068e-06, 8.5895e-06, 7.8609e-06, 8.1293e-06,\n",
       "            8.9694e-06, 8.5004e-06, 7.4915e-06, 8.5330e-06, 9.7668e-06, 9.2712e-06,\n",
       "            8.1199e-06, 9.2984e-06, 7.6896e-06, 8.3433e-06, 8.9475e-06, 8.8641e-06,\n",
       "            8.3587e-06, 9.2734e-06, 9.9422e-06, 1.0147e-05, 7.4051e-06, 1.0027e-05,\n",
       "            7.8263e-06, 8.6065e-06, 8.0281e-06, 7.5050e-06, 9.6154e-06, 8.8042e-06,\n",
       "            8.4426e-06, 9.4586e-06, 9.0231e-06, 8.4588e-06, 9.0137e-06, 7.8899e-06,\n",
       "            9.8008e-06, 7.7614e-06, 8.9248e-06, 8.7598e-06, 8.8332e-06, 7.9255e-06,\n",
       "            8.7889e-06, 7.9125e-06, 7.2719e-06, 8.3600e-06, 7.8179e-06, 9.2175e-06,\n",
       "            9.2912e-06, 7.7136e-06, 7.6609e-06, 8.7244e-06, 8.5407e-06, 8.8759e-06,\n",
       "            7.6125e-06, 8.9839e-06, 7.5919e-06, 7.9686e-06, 8.4917e-06, 8.6680e-06,\n",
       "            8.3023e-06, 8.6246e-06, 7.5160e-06, 8.2966e-06, 8.9473e-06, 8.2501e-06,\n",
       "            8.3244e-06, 9.1966e-06, 8.0249e-06, 7.2981e-06, 8.6680e-06, 8.1160e-06,\n",
       "            8.8183e-06, 7.3309e-06, 8.8113e-06, 7.9730e-06, 7.5073e-06, 8.1394e-06,\n",
       "            8.6092e-06, 8.6695e-06, 8.2621e-06, 7.9660e-06, 8.8315e-06, 7.7819e-06,\n",
       "            8.1241e-06, 8.7080e-06, 9.2033e-06, 8.7033e-06, 8.3233e-06, 8.6623e-06,\n",
       "            8.4659e-06, 8.4046e-06, 8.4458e-06, 8.2102e-06, 8.6039e-06, 7.9886e-06,\n",
       "            8.0217e-06, 8.5788e-06, 9.5143e-06, 9.2667e-06, 9.1821e-06, 8.3863e-06,\n",
       "            8.5365e-06, 7.7963e-06, 8.4818e-06, 8.9018e-06, 8.9280e-06, 8.4838e-06,\n",
       "            8.4189e-06, 7.8436e-06, 8.4805e-06, 8.8552e-06, 8.7848e-06, 8.6669e-06,\n",
       "            9.2278e-06, 8.4849e-06, 8.8722e-06, 8.9902e-06, 8.7080e-06, 8.0607e-06,\n",
       "            8.0969e-06, 8.5891e-06, 7.7144e-06, 7.7026e-06, 7.7918e-06, 9.7145e-06,\n",
       "            8.8461e-06, 1.0086e-05, 8.5153e-06, 8.8269e-06, 7.3273e-06, 9.4333e-06,\n",
       "            9.2359e-06, 9.2546e-06, 8.0956e-06, 8.5727e-06, 8.3364e-06, 8.2478e-06,\n",
       "            8.7207e-06, 7.8017e-06, 8.1993e-06, 8.9213e-06, 7.9906e-06, 8.9122e-06,\n",
       "            9.2290e-06, 8.1022e-06, 9.4856e-06, 9.7153e-06, 9.1132e-06, 8.2087e-06,\n",
       "            8.7230e-06, 7.5423e-06, 9.0376e-06, 9.1863e-06, 8.6836e-06, 8.8586e-06,\n",
       "            7.9212e-06, 7.8177e-06, 9.0690e-06, 8.8129e-06, 7.9678e-06, 8.9951e-06,\n",
       "            8.9590e-06, 7.3724e-06, 9.3382e-06, 8.0086e-06, 8.6114e-06, 1.0365e-05,\n",
       "            7.4042e-06, 7.9337e-06, 8.3169e-06, 9.0859e-06, 8.5500e-06, 8.9565e-06,\n",
       "            8.9594e-06, 8.3865e-06, 8.3902e-06, 8.9119e-06, 7.7701e-06, 7.3036e-06,\n",
       "            8.3044e-06, 9.6141e-06, 8.9325e-06, 9.1265e-06, 8.0765e-06, 8.0552e-06,\n",
       "            7.9644e-06, 8.6358e-06, 7.9853e-06, 7.7701e-06, 7.2863e-06, 7.8032e-06,\n",
       "            8.4909e-06, 8.5842e-06, 8.6884e-06, 8.6405e-06, 8.9985e-06, 9.4670e-06,\n",
       "            8.3746e-06, 9.0136e-06, 8.8308e-06, 8.0867e-06, 8.8533e-06, 9.3559e-06,\n",
       "            7.2573e-06, 8.5009e-06, 8.9367e-06, 8.4901e-06, 8.7922e-06, 8.3269e-06,\n",
       "            8.0024e-06, 8.3517e-06, 8.2496e-06, 8.5951e-06, 7.2557e-06, 8.5590e-06,\n",
       "            9.3402e-06, 8.2929e-06, 9.1195e-06, 8.2613e-06, 8.9809e-06, 1.0140e-05,\n",
       "            8.2219e-06, 8.3106e-06, 8.8466e-06, 9.0901e-06, 8.2801e-06, 9.0054e-06,\n",
       "            8.5007e-06, 9.2475e-06, 8.2716e-06, 8.6712e-06, 8.4065e-06, 8.5715e-06,\n",
       "            8.1814e-06, 9.7100e-06, 8.4461e-06, 8.2784e-06, 8.0412e-06, 8.9991e-06,\n",
       "            8.6589e-06, 8.1728e-06, 8.3322e-06, 9.0261e-06, 8.4703e-06, 8.0394e-06,\n",
       "            8.9209e-06, 7.6807e-06, 7.7235e-06, 7.4208e-06, 8.0739e-06, 8.8325e-06,\n",
       "            9.0557e-06, 8.3893e-06, 7.8315e-06, 8.4879e-06, 8.6382e-06, 9.6630e-06,\n",
       "            9.4839e-06, 8.4971e-06, 7.5601e-06, 8.8620e-06, 8.5456e-06, 9.4801e-06,\n",
       "            8.0880e-06, 7.2590e-06, 8.8536e-06, 1.0365e-05, 8.4968e-06, 8.5394e-06,\n",
       "            7.7367e-06, 8.8898e-06, 9.5481e-06, 7.5920e-06, 9.1951e-06, 8.1279e-06,\n",
       "            8.2254e-06, 8.0069e-06, 7.6547e-06, 9.5054e-06, 9.1938e-06, 9.1137e-06,\n",
       "            8.8420e-06, 8.5556e-06, 9.4652e-06, 9.9442e-06, 8.0853e-06, 8.4529e-06,\n",
       "            8.7386e-06, 8.4327e-06, 8.3959e-06, 8.2732e-06, 8.5344e-06, 8.6659e-06,\n",
       "            9.4280e-06, 8.9824e-06, 9.7919e-06, 8.5921e-06, 8.3097e-06, 7.9797e-06,\n",
       "            8.6287e-06, 8.6366e-06, 8.4067e-06, 8.5064e-06, 8.1644e-06, 9.7950e-06,\n",
       "            8.5450e-06, 8.0730e-06, 8.7762e-06, 9.0633e-06, 7.3535e-06, 8.6950e-06,\n",
       "            8.6662e-06, 9.7355e-06, 7.9849e-06, 9.0192e-06, 8.5436e-06, 7.6030e-06,\n",
       "            8.6197e-06, 8.1933e-06, 9.3745e-06, 8.4936e-06, 7.6146e-06, 8.3432e-06,\n",
       "            7.7257e-06, 8.0538e-06, 7.9188e-06, 8.2389e-06, 8.8315e-06, 8.6328e-06,\n",
       "            8.8273e-06, 7.7376e-06, 9.4293e-06, 9.1632e-06, 8.1498e-06, 7.8026e-06,\n",
       "            8.3451e-06, 8.7157e-06, 8.4365e-06, 8.9565e-06, 8.4321e-06, 9.8952e-06,\n",
       "            8.2000e-06, 8.6049e-06, 8.4021e-06, 9.3603e-06, 8.4800e-06, 9.0860e-06,\n",
       "            9.2132e-06, 8.3931e-06, 8.4553e-06, 8.5878e-06, 8.2528e-06, 9.3334e-06,\n",
       "            8.1624e-06, 8.9367e-06, 8.4070e-06, 7.6374e-06, 8.0539e-06, 8.3936e-06,\n",
       "            9.6372e-06, 9.2782e-06, 8.9931e-06, 8.1507e-06, 8.8309e-06, 6.9589e-06,\n",
       "            9.0820e-06, 8.5902e-06, 8.8015e-06, 8.3767e-06, 8.6054e-06, 7.1269e-06,\n",
       "            8.1273e-06, 7.5363e-06, 8.8697e-06, 8.6905e-06, 8.6113e-06, 8.0984e-06,\n",
       "            7.7565e-06, 8.7658e-06, 7.4767e-06, 8.5854e-06, 9.1589e-06, 9.2313e-06,\n",
       "            8.3337e-06, 8.8286e-06, 7.6477e-06, 8.7175e-06, 8.6688e-06, 8.5905e-06,\n",
       "            8.9038e-06, 8.8024e-06, 9.2790e-06, 8.0431e-06, 7.5247e-06, 8.3771e-06,\n",
       "            8.8487e-06, 8.2833e-06, 9.2767e-06, 9.0760e-06, 8.3459e-06, 8.9305e-06,\n",
       "            8.8213e-06, 9.0252e-06, 8.3008e-06, 8.4619e-06, 8.6584e-06, 8.1213e-06,\n",
       "            8.7194e-06, 8.4255e-06, 8.2599e-06, 8.2013e-06, 9.4048e-06, 7.8275e-06,\n",
       "            8.6421e-06, 8.4669e-06, 9.1324e-06, 8.3236e-06, 8.7240e-06, 7.8780e-06,\n",
       "            7.4435e-06, 8.7185e-06, 8.7215e-06, 7.8250e-06, 9.2921e-06, 8.2561e-06,\n",
       "            8.5426e-06, 8.9798e-06, 8.5220e-06, 9.1948e-06, 7.8196e-06, 8.4569e-06,\n",
       "            8.1069e-06, 8.0128e-06, 7.5155e-06, 8.1382e-06, 9.1992e-06, 8.2062e-06,\n",
       "            8.9210e-06, 1.0131e-05, 8.5048e-06, 8.1415e-06, 9.1310e-06, 8.9297e-06,\n",
       "            8.6994e-06, 8.4153e-06, 7.8437e-06, 8.8519e-06, 8.1931e-06, 8.4347e-06,\n",
       "            8.1615e-06, 8.1988e-06, 7.5535e-06, 9.2745e-06, 7.1747e-06, 8.4618e-06,\n",
       "            8.4166e-06, 8.7290e-06, 9.4074e-06, 8.8822e-06, 8.7737e-06, 8.4964e-06,\n",
       "            8.6419e-06, 8.4911e-06, 9.6396e-06, 8.5539e-06, 8.4844e-06, 8.5361e-06,\n",
       "            8.1239e-06, 8.6650e-06, 8.4573e-06, 8.9579e-06, 8.8327e-06, 9.0455e-06,\n",
       "            7.6322e-06, 8.1502e-06, 9.1084e-06, 8.4007e-06, 8.7678e-06, 8.9572e-06,\n",
       "            8.6779e-06, 8.9053e-06, 8.3533e-06, 8.4437e-06, 8.9270e-06, 7.7187e-06,\n",
       "            8.3244e-06, 9.1125e-06, 9.0622e-06, 8.4539e-06, 8.5962e-06, 8.5848e-06,\n",
       "            9.8248e-06, 9.1464e-06, 8.1032e-06, 9.1406e-06, 8.5735e-06, 8.2716e-06,\n",
       "            9.6409e-06, 8.7906e-06, 8.3951e-06, 8.3355e-06, 9.7244e-06, 9.1698e-06,\n",
       "            8.9078e-06, 8.2604e-06, 9.0998e-06, 7.8528e-06, 7.9179e-06, 9.2243e-06,\n",
       "            8.5325e-06, 8.1047e-06, 8.8916e-06, 7.5307e-06, 8.8401e-06, 7.9990e-06,\n",
       "            8.0679e-06, 8.2678e-06, 9.3215e-06, 8.3275e-06, 9.3892e-06, 7.7596e-06,\n",
       "            8.9107e-06, 8.2850e-06, 8.1264e-06, 8.9005e-06, 8.4547e-06, 7.0083e-06,\n",
       "            8.7108e-06, 7.8928e-06, 8.1167e-06, 8.7563e-06, 9.2370e-06, 9.3856e-06,\n",
       "            8.6668e-06, 8.8473e-06, 8.4608e-06, 8.3994e-06, 8.2044e-06, 7.2623e-06,\n",
       "            9.0820e-06, 1.0292e-05, 8.3733e-06, 8.6286e-06, 9.3026e-06, 8.7709e-06,\n",
       "            8.9863e-06, 8.2379e-06, 8.1931e-06, 8.2226e-06, 9.2076e-06, 8.9095e-06,\n",
       "            8.1726e-06, 8.2309e-06, 8.1585e-06, 8.6846e-06, 9.4351e-06, 9.4316e-06,\n",
       "            8.6778e-06, 7.4937e-06, 7.5687e-06, 9.1583e-06, 8.1454e-06, 7.7335e-06,\n",
       "            7.7798e-06, 8.0023e-06, 7.5865e-06, 8.3281e-06, 8.8409e-06, 9.3525e-06,\n",
       "            8.6651e-06, 7.4938e-06, 8.6641e-06, 8.6292e-06, 8.2009e-06, 9.1544e-06,\n",
       "            9.0707e-06, 7.9575e-06, 8.3880e-06, 7.4165e-06, 9.3375e-06, 7.7560e-06,\n",
       "            8.9471e-06, 9.4245e-06, 7.9684e-06, 9.1128e-06, 8.1878e-06, 8.3382e-06,\n",
       "            8.8779e-06, 8.7776e-06, 8.3171e-06, 8.3144e-06, 9.0238e-06, 8.7185e-06,\n",
       "            8.8214e-06, 8.3946e-06, 1.0478e-05, 8.3298e-06, 8.7508e-06, 8.1536e-06,\n",
       "            8.5550e-06, 8.4571e-06, 8.2015e-06, 8.2923e-06, 8.8046e-06, 8.8856e-06,\n",
       "            8.9267e-06, 8.6944e-06, 8.1669e-06, 8.8191e-06, 8.4066e-06, 8.5769e-06,\n",
       "            7.5781e-06, 9.9587e-06, 9.3709e-06, 8.3244e-06, 8.8904e-06, 9.6059e-06,\n",
       "            8.6477e-06, 7.4142e-06, 8.6191e-06, 8.9405e-06, 8.2366e-06, 7.8751e-06,\n",
       "            8.9235e-06, 8.5320e-06, 8.2619e-06, 8.0855e-06, 9.0214e-06, 8.0387e-06,\n",
       "            8.9772e-06, 7.1813e-06, 8.9605e-06, 9.3087e-06, 8.5154e-06, 9.3295e-06,\n",
       "            9.2231e-06, 8.4418e-06, 9.0963e-06, 9.2107e-06, 8.8995e-06, 7.5262e-06,\n",
       "            7.5895e-06, 7.2226e-06, 8.5827e-06, 7.9382e-06, 9.1931e-06, 8.8499e-06,\n",
       "            8.9189e-06, 8.4447e-06, 8.2854e-06, 8.5840e-06, 8.9449e-06, 9.3281e-06,\n",
       "            9.0414e-06, 9.3383e-06, 8.6007e-06, 7.4255e-06, 8.7629e-06, 8.4009e-06,\n",
       "            7.4776e-06, 8.3100e-06, 8.4057e-06, 9.5054e-06, 8.9212e-06, 8.8046e-06,\n",
       "            8.3864e-06, 9.1139e-06, 8.9822e-06, 8.1156e-06, 8.2149e-06, 8.5287e-06,\n",
       "            8.8430e-06, 7.6975e-06, 8.8746e-06, 7.8168e-06, 7.8966e-06, 8.0507e-06,\n",
       "            8.9598e-06, 8.3223e-06, 8.8122e-06, 8.6662e-06, 9.2317e-06, 8.3877e-06,\n",
       "            9.0064e-06, 8.5782e-06, 8.4376e-06, 8.2615e-06, 9.2977e-06, 7.4160e-06,\n",
       "            9.3313e-06, 7.6810e-06, 8.0480e-06, 8.6977e-06, 8.2466e-06, 8.5798e-06,\n",
       "            8.6358e-06, 8.0400e-06, 8.2302e-06, 8.9770e-06, 7.7674e-06, 9.4627e-06,\n",
       "            8.7073e-06, 8.4866e-06, 8.7085e-06, 8.7602e-06, 8.3733e-06, 7.4054e-06,\n",
       "            9.3755e-06, 8.5546e-06, 7.7242e-06, 8.4044e-06, 9.5600e-06, 8.5481e-06,\n",
       "            8.4259e-06, 8.5396e-06, 8.2616e-06, 7.9264e-06, 8.3766e-06, 9.1082e-06,\n",
       "            9.2772e-06, 9.2794e-06, 8.8719e-06, 8.4340e-06, 8.5036e-06, 8.1984e-06,\n",
       "            8.0494e-06, 8.5960e-06, 9.2289e-06, 7.4035e-06, 8.9134e-06, 7.9765e-06,\n",
       "            8.3331e-06, 8.1861e-06, 8.2725e-06, 7.5630e-06, 9.0625e-06, 8.8668e-06,\n",
       "            8.4352e-06, 8.6841e-06, 8.1697e-06, 8.9979e-06, 8.5719e-06, 9.2545e-06,\n",
       "            8.9580e-06, 7.8758e-06, 9.3862e-06, 8.7308e-06, 8.1527e-06, 9.3595e-06,\n",
       "            8.8230e-06, 9.0141e-06, 8.0484e-06, 8.3977e-06, 8.8570e-06, 8.4778e-06,\n",
       "            7.4657e-06, 8.4417e-06, 9.2399e-06, 8.8440e-06, 8.7296e-06, 7.4427e-06,\n",
       "            8.1622e-06, 9.5047e-06, 7.6786e-06, 8.6538e-06, 8.3492e-06, 8.0172e-06,\n",
       "            8.4704e-06, 9.6810e-06, 8.9869e-06, 8.6554e-06, 8.4439e-06, 7.4166e-06,\n",
       "            9.3035e-06, 8.1648e-06, 8.7360e-06, 8.0292e-06, 8.3194e-06, 8.9843e-06,\n",
       "            9.1618e-06, 8.8339e-06, 8.3490e-06, 8.1540e-06, 9.3817e-06, 9.8468e-06,\n",
       "            9.1399e-06, 7.7038e-06, 9.6315e-06, 9.1503e-06, 7.9250e-06, 8.8159e-06,\n",
       "            8.3337e-06, 9.7488e-06, 8.2759e-06, 8.2915e-06, 8.8958e-06, 7.6190e-06,\n",
       "            8.6298e-06, 8.4522e-06, 8.5267e-06, 8.4311e-06, 8.8277e-06, 8.8812e-06,\n",
       "            8.2397e-06, 8.9647e-06, 9.0318e-06, 7.3349e-06, 8.9349e-06, 8.7894e-06,\n",
       "            9.1993e-06, 6.9687e-06, 8.9727e-06, 7.4831e-06, 8.3372e-06, 8.1055e-06,\n",
       "            8.6786e-06, 8.4228e-06, 7.6452e-06, 8.4221e-06, 8.3264e-06, 8.1894e-06,\n",
       "            8.6280e-06, 9.4883e-06, 8.7785e-06, 8.7876e-06, 7.9109e-06, 9.3228e-06,\n",
       "            9.3330e-06, 8.4997e-06, 8.9497e-06, 7.8386e-06, 8.4500e-06, 8.7353e-06,\n",
       "            8.5688e-06, 8.7702e-06, 8.2359e-06, 8.2004e-06, 6.9282e-06, 8.4208e-06,\n",
       "            8.9414e-06, 8.5004e-06, 8.8143e-06, 8.8236e-06, 7.6461e-06, 9.2889e-06,\n",
       "            9.4468e-06, 8.6167e-06, 8.8667e-06, 8.8239e-06, 7.4659e-06, 7.7619e-06,\n",
       "            1.0069e-05, 8.4182e-06, 8.5812e-06, 8.3579e-06, 9.3821e-06, 8.8219e-06,\n",
       "            7.7263e-06, 9.0278e-06, 9.3136e-06, 8.1701e-06, 8.0895e-06, 8.6143e-06,\n",
       "            8.7557e-06, 8.3118e-06, 9.1393e-06, 1.0067e-05, 8.5763e-06, 8.5413e-06,\n",
       "            7.4931e-06, 8.9989e-06, 9.1328e-06, 8.2808e-06, 8.9629e-06, 7.1693e-06,\n",
       "            8.5381e-06, 7.9441e-06, 9.2433e-06, 8.1572e-06, 9.3925e-06, 8.3691e-06,\n",
       "            7.9773e-06, 8.2265e-06, 8.2947e-06, 8.4443e-06, 8.4308e-06, 8.8137e-06,\n",
       "            8.6285e-06, 8.9034e-06, 9.0638e-06, 8.9167e-06, 8.4387e-06, 9.0395e-06,\n",
       "            8.0992e-06, 8.4956e-06, 9.1978e-06, 9.3624e-06, 8.0960e-06, 8.3706e-06,\n",
       "            8.1198e-06, 8.7556e-06, 9.3520e-06, 9.3276e-06, 8.6984e-06, 8.3888e-06,\n",
       "            8.8767e-06, 8.7130e-06, 8.6773e-06, 9.3601e-06, 9.4035e-06, 8.7368e-06,\n",
       "            8.4862e-06, 8.4885e-06, 8.5079e-06, 8.8441e-06, 8.5464e-06, 9.0260e-06,\n",
       "            8.9247e-06, 8.8583e-06, 8.6301e-06, 9.0982e-06, 8.9498e-06, 8.9105e-06,\n",
       "            7.8990e-06, 8.7822e-06, 9.1097e-06, 9.3031e-06, 7.9389e-06, 9.4793e-06,\n",
       "            8.3163e-06, 9.4262e-06, 7.6153e-06, 7.5650e-06, 8.6064e-06, 8.8570e-06,\n",
       "            8.3450e-06, 7.8274e-06, 7.6029e-06, 8.5561e-06, 8.8514e-06, 9.0072e-06,\n",
       "            7.6454e-06, 8.5401e-06, 7.5811e-06, 7.8088e-06, 8.2376e-06, 9.1017e-06,\n",
       "            8.6967e-06, 7.7785e-06, 8.9753e-06, 8.6342e-06, 7.7911e-06, 7.3441e-06,\n",
       "            8.5421e-06, 8.6062e-06, 7.7983e-06, 8.5710e-06, 9.1835e-06, 8.5936e-06,\n",
       "            9.0002e-06, 8.5495e-06, 8.2787e-06, 9.2021e-06, 7.6603e-06, 8.7883e-06,\n",
       "            8.3331e-06, 7.7306e-06], device='cuda:0'),\n",
       "    'exp_avg_sq': tensor([1.8027e-08, 1.8028e-08, 1.8039e-08, 1.8041e-08, 1.8040e-08, 1.8030e-08,\n",
       "            1.8036e-08, 1.8040e-08, 1.8027e-08, 1.8036e-08, 1.8035e-08, 1.8040e-08,\n",
       "            1.8045e-08, 1.8046e-08, 1.8042e-08, 1.8026e-08, 1.8035e-08, 1.8030e-08,\n",
       "            1.8040e-08, 1.8032e-08, 1.8026e-08, 1.8046e-08, 1.8036e-08, 1.8044e-08,\n",
       "            1.8029e-08, 1.8035e-08, 1.8035e-08, 1.8028e-08, 1.8043e-08, 1.8034e-08,\n",
       "            1.8028e-08, 1.8032e-08, 1.8048e-08, 1.8037e-08, 1.8026e-08, 1.8031e-08,\n",
       "            1.8040e-08, 1.8026e-08, 1.8042e-08, 1.8035e-08, 1.8034e-08, 1.8031e-08,\n",
       "            1.8039e-08, 1.8033e-08, 1.8023e-08, 1.8024e-08, 1.8048e-08, 1.8025e-08,\n",
       "            1.8042e-08, 1.8035e-08, 1.8042e-08, 1.8043e-08, 1.8029e-08, 1.8029e-08,\n",
       "            1.8040e-08, 1.8025e-08, 1.8031e-08, 1.8035e-08, 1.8030e-08, 1.8039e-08,\n",
       "            1.8026e-08, 1.8040e-08, 1.8030e-08, 1.8029e-08, 1.8030e-08, 1.8038e-08,\n",
       "            1.8035e-08, 1.8037e-08, 1.8046e-08, 1.8032e-08, 1.8046e-08, 1.8028e-08,\n",
       "            1.8025e-08, 1.8042e-08, 1.8043e-08, 1.8029e-08, 1.8037e-08, 1.8031e-08,\n",
       "            1.8047e-08, 1.8034e-08, 1.8045e-08, 1.8039e-08, 1.8034e-08, 1.8032e-08,\n",
       "            1.8036e-08, 1.8036e-08, 1.8041e-08, 1.8042e-08, 1.8027e-08, 1.8034e-08,\n",
       "            1.8036e-08, 1.8024e-08, 1.8040e-08, 1.8045e-08, 1.8035e-08, 1.8037e-08,\n",
       "            1.8034e-08, 1.8041e-08, 1.8032e-08, 1.8037e-08, 1.8041e-08, 1.8038e-08,\n",
       "            1.8033e-08, 1.8035e-08, 1.8036e-08, 1.8039e-08, 1.8033e-08, 1.8044e-08,\n",
       "            1.8038e-08, 1.8032e-08, 1.8028e-08, 1.8033e-08, 1.8034e-08, 1.8031e-08,\n",
       "            1.8035e-08, 1.8037e-08, 1.8036e-08, 1.8037e-08, 1.8034e-08, 1.8038e-08,\n",
       "            1.8035e-08, 1.8038e-08, 1.8023e-08, 1.8021e-08, 1.8030e-08, 1.8029e-08,\n",
       "            1.8032e-08, 1.8042e-08, 1.8035e-08, 1.8030e-08, 1.8032e-08, 1.8036e-08,\n",
       "            1.8030e-08, 1.8034e-08, 1.8038e-08, 1.8030e-08, 1.8031e-08, 1.8033e-08,\n",
       "            1.8030e-08, 1.8035e-08, 1.8028e-08, 1.8031e-08, 1.8036e-08, 1.8037e-08,\n",
       "            1.8040e-08, 1.8027e-08, 1.8037e-08, 1.8042e-08, 1.8040e-08, 1.8028e-08,\n",
       "            1.8036e-08, 1.8022e-08, 1.8033e-08, 1.8028e-08, 1.8045e-08, 1.8028e-08,\n",
       "            1.8025e-08, 1.8031e-08, 1.8038e-08, 1.8036e-08, 1.8039e-08, 1.8038e-08,\n",
       "            1.8034e-08, 1.8040e-08, 1.8037e-08, 1.8035e-08, 1.8038e-08, 1.8037e-08,\n",
       "            1.8029e-08, 1.8040e-08, 1.8028e-08, 1.8027e-08, 1.8028e-08, 1.8035e-08,\n",
       "            1.8034e-08, 1.8044e-08, 1.8030e-08, 1.8025e-08, 1.8036e-08, 1.8028e-08,\n",
       "            1.8035e-08, 1.8036e-08, 1.8033e-08, 1.8031e-08, 1.8038e-08, 1.8029e-08,\n",
       "            1.8027e-08, 1.8045e-08, 1.8029e-08, 1.8042e-08, 1.8034e-08, 1.8016e-08,\n",
       "            1.8044e-08, 1.8042e-08, 1.8036e-08, 1.8032e-08, 1.8036e-08, 1.8027e-08,\n",
       "            1.8027e-08, 1.8035e-08, 1.8038e-08, 1.8029e-08, 1.8039e-08, 1.8050e-08,\n",
       "            1.8036e-08, 1.8029e-08, 1.8031e-08, 1.8035e-08, 1.8035e-08, 1.8036e-08,\n",
       "            1.8037e-08, 1.8032e-08, 1.8038e-08, 1.8042e-08, 1.8047e-08, 1.8040e-08,\n",
       "            1.8032e-08, 1.8034e-08, 1.8033e-08, 1.8032e-08, 1.8031e-08, 1.8020e-08,\n",
       "            1.8035e-08, 1.8034e-08, 1.8032e-08, 1.8043e-08, 1.8030e-08, 1.8027e-08,\n",
       "            1.8042e-08, 1.8038e-08, 1.8027e-08, 1.8034e-08, 1.8033e-08, 1.8034e-08,\n",
       "            1.8042e-08, 1.8031e-08, 1.8036e-08, 1.8034e-08, 1.8047e-08, 1.8032e-08,\n",
       "            1.8026e-08, 1.8037e-08, 1.8033e-08, 1.8034e-08, 1.8031e-08, 1.8022e-08,\n",
       "            1.8032e-08, 1.8039e-08, 1.8033e-08, 1.8032e-08, 1.8035e-08, 1.8029e-08,\n",
       "            1.8033e-08, 1.8030e-08, 1.8040e-08, 1.8032e-08, 1.8036e-08, 1.8036e-08,\n",
       "            1.8036e-08, 1.8027e-08, 1.8038e-08, 1.8035e-08, 1.8040e-08, 1.8028e-08,\n",
       "            1.8034e-08, 1.8040e-08, 1.8031e-08, 1.8033e-08, 1.8037e-08, 1.8036e-08,\n",
       "            1.8033e-08, 1.8038e-08, 1.8041e-08, 1.8043e-08, 1.8039e-08, 1.8036e-08,\n",
       "            1.8030e-08, 1.8041e-08, 1.8038e-08, 1.8036e-08, 1.8034e-08, 1.8025e-08,\n",
       "            1.8030e-08, 1.8038e-08, 1.8045e-08, 1.8031e-08, 1.8031e-08, 1.8028e-08,\n",
       "            1.8036e-08, 1.8043e-08, 1.8032e-08, 1.8024e-08, 1.8034e-08, 1.8038e-08,\n",
       "            1.8041e-08, 1.8029e-08, 1.8027e-08, 1.8043e-08, 1.8025e-08, 1.8041e-08,\n",
       "            1.8035e-08, 1.8035e-08, 1.8041e-08, 1.8028e-08, 1.8025e-08, 1.8029e-08,\n",
       "            1.8035e-08, 1.8034e-08, 1.8026e-08, 1.8024e-08, 1.8040e-08, 1.8040e-08,\n",
       "            1.8033e-08, 1.8034e-08, 1.8037e-08, 1.8038e-08, 1.8028e-08, 1.8035e-08,\n",
       "            1.8027e-08, 1.8030e-08, 1.8026e-08, 1.8029e-08, 1.8034e-08, 1.8040e-08,\n",
       "            1.8036e-08, 1.8035e-08, 1.8040e-08, 1.8038e-08, 1.8039e-08, 1.8024e-08,\n",
       "            1.8034e-08, 1.8039e-08, 1.8029e-08, 1.8031e-08, 1.8044e-08, 1.8035e-08,\n",
       "            1.8032e-08, 1.8030e-08, 1.8037e-08, 1.8033e-08, 1.8035e-08, 1.8041e-08,\n",
       "            1.8033e-08, 1.8035e-08, 1.8029e-08, 1.8033e-08, 1.8048e-08, 1.8037e-08,\n",
       "            1.8043e-08, 1.8035e-08, 1.8041e-08, 1.8038e-08, 1.8033e-08, 1.8035e-08,\n",
       "            1.8031e-08, 1.8041e-08, 1.8031e-08, 1.8027e-08, 1.8039e-08, 1.8044e-08,\n",
       "            1.8034e-08, 1.8032e-08, 1.8032e-08, 1.8031e-08, 1.8041e-08, 1.8021e-08,\n",
       "            1.8037e-08, 1.8035e-08, 1.8034e-08, 1.8030e-08, 1.8033e-08, 1.8028e-08,\n",
       "            1.8027e-08, 1.8030e-08, 1.8035e-08, 1.8035e-08, 1.8036e-08, 1.8027e-08,\n",
       "            1.8040e-08, 1.8034e-08, 1.8043e-08, 1.8041e-08, 1.8044e-08, 1.8030e-08,\n",
       "            1.8027e-08, 1.8031e-08, 1.8034e-08, 1.8035e-08, 1.8030e-08, 1.8045e-08,\n",
       "            1.8026e-08, 1.8037e-08, 1.8034e-08, 1.8040e-08, 1.8028e-08, 1.8044e-08,\n",
       "            1.8035e-08, 1.8037e-08, 1.8031e-08, 1.8036e-08, 1.8036e-08, 1.8036e-08,\n",
       "            1.8041e-08, 1.8034e-08, 1.8044e-08, 1.8041e-08, 1.8028e-08, 1.8029e-08,\n",
       "            1.8037e-08, 1.8037e-08, 1.8045e-08, 1.8029e-08, 1.8036e-08, 1.8036e-08,\n",
       "            1.8033e-08, 1.8036e-08, 1.8031e-08, 1.8038e-08, 1.8043e-08, 1.8033e-08,\n",
       "            1.8033e-08, 1.8036e-08, 1.8036e-08, 1.8034e-08, 1.8038e-08, 1.8027e-08,\n",
       "            1.8034e-08, 1.8031e-08, 1.8036e-08, 1.8036e-08, 1.8036e-08, 1.8039e-08,\n",
       "            1.8033e-08, 1.8044e-08, 1.8043e-08, 1.8038e-08, 1.8030e-08, 1.8039e-08,\n",
       "            1.8033e-08, 1.8037e-08, 1.8029e-08, 1.8036e-08, 1.8031e-08, 1.8045e-08,\n",
       "            1.8041e-08, 1.8037e-08, 1.8032e-08, 1.8041e-08, 1.8028e-08, 1.8036e-08,\n",
       "            1.8035e-08, 1.8033e-08, 1.8037e-08, 1.8031e-08, 1.8039e-08, 1.8037e-08,\n",
       "            1.8041e-08, 1.8038e-08, 1.8049e-08, 1.8037e-08, 1.8022e-08, 1.8042e-08,\n",
       "            1.8033e-08, 1.8018e-08, 1.8030e-08, 1.8038e-08, 1.8029e-08, 1.8030e-08,\n",
       "            1.8036e-08, 1.8037e-08, 1.8041e-08, 1.8032e-08, 1.8043e-08, 1.8038e-08,\n",
       "            1.8039e-08, 1.8039e-08, 1.8041e-08, 1.8029e-08, 1.8048e-08, 1.8034e-08,\n",
       "            1.8038e-08, 1.8034e-08, 1.8029e-08, 1.8030e-08, 1.8035e-08, 1.8032e-08,\n",
       "            1.8033e-08, 1.8036e-08, 1.8030e-08, 1.8038e-08, 1.8038e-08, 1.8037e-08,\n",
       "            1.8040e-08, 1.8034e-08, 1.8038e-08, 1.8029e-08, 1.8037e-08, 1.8034e-08,\n",
       "            1.8045e-08, 1.8035e-08, 1.8031e-08, 1.8031e-08, 1.8028e-08, 1.8031e-08,\n",
       "            1.8036e-08, 1.8031e-08, 1.8037e-08, 1.8035e-08, 1.8033e-08, 1.8048e-08,\n",
       "            1.8037e-08, 1.8028e-08, 1.8030e-08, 1.8030e-08, 1.8033e-08, 1.8035e-08,\n",
       "            1.8025e-08, 1.8026e-08, 1.8038e-08, 1.8028e-08, 1.8038e-08, 1.8038e-08,\n",
       "            1.8025e-08, 1.8031e-08, 1.8031e-08, 1.8037e-08, 1.8023e-08, 1.8027e-08,\n",
       "            1.8031e-08, 1.8040e-08, 1.8031e-08, 1.8035e-08, 1.8043e-08, 1.8028e-08,\n",
       "            1.8033e-08, 1.8041e-08, 1.8032e-08, 1.8046e-08, 1.8035e-08, 1.8033e-08,\n",
       "            1.8038e-08, 1.8039e-08, 1.8028e-08, 1.8033e-08, 1.8026e-08, 1.8037e-08,\n",
       "            1.8034e-08, 1.8037e-08, 1.8040e-08, 1.8031e-08, 1.8033e-08, 1.8046e-08,\n",
       "            1.8034e-08, 1.8035e-08, 1.8039e-08, 1.8030e-08, 1.8024e-08, 1.8028e-08,\n",
       "            1.8033e-08, 1.8031e-08, 1.8032e-08, 1.8037e-08, 1.8038e-08, 1.8042e-08,\n",
       "            1.8030e-08, 1.8023e-08, 1.8030e-08, 1.8033e-08, 1.8027e-08, 1.8033e-08,\n",
       "            1.8026e-08, 1.8036e-08, 1.8034e-08, 1.8038e-08, 1.8030e-08, 1.8032e-08,\n",
       "            1.8034e-08, 1.8042e-08, 1.8035e-08, 1.8030e-08, 1.8026e-08, 1.8024e-08,\n",
       "            1.8033e-08, 1.8043e-08, 1.8041e-08, 1.8027e-08, 1.8037e-08, 1.8042e-08,\n",
       "            1.8039e-08, 1.8037e-08, 1.8042e-08, 1.8042e-08, 1.8030e-08, 1.8030e-08,\n",
       "            1.8034e-08, 1.8044e-08, 1.8033e-08, 1.8035e-08, 1.8038e-08, 1.8029e-08,\n",
       "            1.8027e-08, 1.8041e-08, 1.8033e-08, 1.8046e-08, 1.8028e-08, 1.8041e-08,\n",
       "            1.8034e-08, 1.8026e-08, 1.8038e-08, 1.8030e-08, 1.8039e-08, 1.8040e-08,\n",
       "            1.8032e-08, 1.8034e-08, 1.8032e-08, 1.8039e-08, 1.8036e-08, 1.8033e-08,\n",
       "            1.8031e-08, 1.8037e-08, 1.8018e-08, 1.8038e-08, 1.8033e-08, 1.8038e-08,\n",
       "            1.8033e-08, 1.8036e-08, 1.8035e-08, 1.8043e-08, 1.8033e-08, 1.8036e-08,\n",
       "            1.8034e-08, 1.8031e-08, 1.8039e-08, 1.8025e-08, 1.8030e-08, 1.8034e-08,\n",
       "            1.8044e-08, 1.8025e-08, 1.8028e-08, 1.8036e-08, 1.8037e-08, 1.8025e-08,\n",
       "            1.8038e-08, 1.8044e-08, 1.8030e-08, 1.8032e-08, 1.8039e-08, 1.8038e-08,\n",
       "            1.8032e-08, 1.8034e-08, 1.8038e-08, 1.8041e-08, 1.8022e-08, 1.8035e-08,\n",
       "            1.8031e-08, 1.8050e-08, 1.8031e-08, 1.8027e-08, 1.8037e-08, 1.8029e-08,\n",
       "            1.8029e-08, 1.8035e-08, 1.8034e-08, 1.8028e-08, 1.8030e-08, 1.8041e-08,\n",
       "            1.8047e-08, 1.8048e-08, 1.8033e-08, 1.8046e-08, 1.8032e-08, 1.8029e-08,\n",
       "            1.8032e-08, 1.8036e-08, 1.8036e-08, 1.8037e-08, 1.8028e-08, 1.8026e-08,\n",
       "            1.8032e-08, 1.8026e-08, 1.8036e-08, 1.8042e-08, 1.8031e-08, 1.8038e-08,\n",
       "            1.8043e-08, 1.8037e-08, 1.8036e-08, 1.8027e-08, 1.8031e-08, 1.8037e-08,\n",
       "            1.8036e-08, 1.8030e-08, 1.8031e-08, 1.8041e-08, 1.8037e-08, 1.8033e-08,\n",
       "            1.8024e-08, 1.8045e-08, 1.8032e-08, 1.8042e-08, 1.8036e-08, 1.8037e-08,\n",
       "            1.8032e-08, 1.8034e-08, 1.8031e-08, 1.8038e-08, 1.8029e-08, 1.8032e-08,\n",
       "            1.8030e-08, 1.8032e-08, 1.8035e-08, 1.8043e-08, 1.8032e-08, 1.8045e-08,\n",
       "            1.8029e-08, 1.8038e-08, 1.8038e-08, 1.8036e-08, 1.8033e-08, 1.8031e-08,\n",
       "            1.8034e-08, 1.8038e-08, 1.8039e-08, 1.8028e-08, 1.8044e-08, 1.8030e-08,\n",
       "            1.8028e-08, 1.8032e-08, 1.8033e-08, 1.8032e-08, 1.8034e-08, 1.8042e-08,\n",
       "            1.8028e-08, 1.8026e-08, 1.8042e-08, 1.8037e-08, 1.8028e-08, 1.8035e-08,\n",
       "            1.8036e-08, 1.8033e-08, 1.8038e-08, 1.8037e-08, 1.8032e-08, 1.8029e-08,\n",
       "            1.8027e-08, 1.8029e-08, 1.8031e-08, 1.8036e-08, 1.8039e-08, 1.8037e-08,\n",
       "            1.8034e-08, 1.8035e-08, 1.8031e-08, 1.8042e-08, 1.8037e-08, 1.8038e-08,\n",
       "            1.8032e-08, 1.8034e-08, 1.8041e-08, 1.8046e-08, 1.8035e-08, 1.8034e-08,\n",
       "            1.8033e-08, 1.8037e-08, 1.8041e-08, 1.8029e-08, 1.8034e-08, 1.8030e-08,\n",
       "            1.8031e-08, 1.8040e-08, 1.8022e-08, 1.8028e-08, 1.8037e-08, 1.8025e-08,\n",
       "            1.8033e-08, 1.8031e-08, 1.8037e-08, 1.8039e-08, 1.8031e-08, 1.8032e-08,\n",
       "            1.8042e-08, 1.8039e-08, 1.8028e-08, 1.8033e-08, 1.8036e-08, 1.8042e-08,\n",
       "            1.8039e-08, 1.8025e-08, 1.8042e-08, 1.8033e-08, 1.8037e-08, 1.8040e-08,\n",
       "            1.8036e-08, 1.8023e-08, 1.8029e-08, 1.8032e-08, 1.8037e-08, 1.8042e-08,\n",
       "            1.8030e-08, 1.8037e-08, 1.8030e-08, 1.8031e-08, 1.8034e-08, 1.8034e-08,\n",
       "            1.8027e-08, 1.8031e-08, 1.8038e-08, 1.8032e-08, 1.8024e-08, 1.8022e-08,\n",
       "            1.8030e-08, 1.8038e-08, 1.8027e-08, 1.8028e-08, 1.8035e-08, 1.8034e-08,\n",
       "            1.8038e-08, 1.8025e-08, 1.8035e-08, 1.8039e-08, 1.8032e-08, 1.8044e-08,\n",
       "            1.8031e-08, 1.8031e-08, 1.8033e-08, 1.8036e-08, 1.8035e-08, 1.8031e-08,\n",
       "            1.8037e-08, 1.8027e-08, 1.8029e-08, 1.8047e-08, 1.8031e-08, 1.8036e-08,\n",
       "            1.8030e-08, 1.8048e-08, 1.8033e-08, 1.8046e-08, 1.8036e-08, 1.8036e-08,\n",
       "            1.8034e-08, 1.8038e-08, 1.8039e-08, 1.8037e-08, 1.8036e-08, 1.8032e-08,\n",
       "            1.8038e-08, 1.8026e-08, 1.8030e-08, 1.8034e-08, 1.8041e-08, 1.8033e-08,\n",
       "            1.8032e-08, 1.8035e-08, 1.8028e-08, 1.8042e-08, 1.8036e-08, 1.8034e-08,\n",
       "            1.8036e-08, 1.8032e-08, 1.8040e-08, 1.8041e-08, 1.8047e-08, 1.8032e-08,\n",
       "            1.8031e-08, 1.8037e-08, 1.8038e-08, 1.8034e-08, 1.8043e-08, 1.8033e-08,\n",
       "            1.8026e-08, 1.8035e-08, 1.8037e-08, 1.8033e-08, 1.8037e-08, 1.8045e-08,\n",
       "            1.8025e-08, 1.8036e-08, 1.8028e-08, 1.8039e-08, 1.8031e-08, 1.8035e-08,\n",
       "            1.8043e-08, 1.8031e-08, 1.8025e-08, 1.8039e-08, 1.8038e-08, 1.8036e-08,\n",
       "            1.8028e-08, 1.8034e-08, 1.8031e-08, 1.8021e-08, 1.8036e-08, 1.8032e-08,\n",
       "            1.8044e-08, 1.8032e-08, 1.8029e-08, 1.8037e-08, 1.8030e-08, 1.8043e-08,\n",
       "            1.8037e-08, 1.8042e-08, 1.8029e-08, 1.8037e-08, 1.8029e-08, 1.8038e-08,\n",
       "            1.8040e-08, 1.8039e-08, 1.8042e-08, 1.8036e-08, 1.8037e-08, 1.8031e-08,\n",
       "            1.8032e-08, 1.8030e-08, 1.8033e-08, 1.8034e-08, 1.8034e-08, 1.8030e-08,\n",
       "            1.8041e-08, 1.8035e-08, 1.8031e-08, 1.8025e-08, 1.8033e-08, 1.8036e-08,\n",
       "            1.8039e-08, 1.8030e-08, 1.8026e-08, 1.8029e-08, 1.8030e-08, 1.8034e-08,\n",
       "            1.8030e-08, 1.8032e-08, 1.8030e-08, 1.8029e-08, 1.8029e-08, 1.8037e-08,\n",
       "            1.8035e-08, 1.8033e-08, 1.8041e-08, 1.8036e-08, 1.8034e-08, 1.8032e-08,\n",
       "            1.8029e-08, 1.8030e-08, 1.8029e-08, 1.8027e-08, 1.8036e-08, 1.8033e-08,\n",
       "            1.8038e-08, 1.8036e-08, 1.8035e-08, 1.8029e-08, 1.8043e-08, 1.8020e-08,\n",
       "            1.8039e-08, 1.8024e-08, 1.8042e-08, 1.8043e-08, 1.8035e-08, 1.8032e-08,\n",
       "            1.8041e-08, 1.8039e-08, 1.8042e-08, 1.8034e-08, 1.8030e-08, 1.8032e-08,\n",
       "            1.8039e-08, 1.8036e-08, 1.8039e-08, 1.8047e-08, 1.8038e-08, 1.8032e-08,\n",
       "            1.8030e-08, 1.8045e-08, 1.8037e-08, 1.8031e-08, 1.8038e-08, 1.8046e-08,\n",
       "            1.8033e-08, 1.8034e-08, 1.8038e-08, 1.8037e-08, 1.8032e-08, 1.8040e-08,\n",
       "            1.8027e-08, 1.8033e-08, 1.8038e-08, 1.8029e-08, 1.8042e-08, 1.8029e-08,\n",
       "            1.8034e-08, 1.8045e-08], device='cuda:0'),\n",
       "    'max_exp_avg_sq': tensor([4.1436e-08, 4.1069e-08, 4.0814e-08, 4.2145e-08, 4.3472e-08, 4.2102e-08,\n",
       "            4.3813e-08, 3.9960e-08, 3.9016e-08, 4.3048e-08, 4.1530e-08, 4.2355e-08,\n",
       "            4.2015e-08, 4.1603e-08, 4.0374e-08, 4.1476e-08, 4.2340e-08, 3.9899e-08,\n",
       "            4.2125e-08, 4.0333e-08, 4.0217e-08, 4.2792e-08, 4.5644e-08, 4.2897e-08,\n",
       "            4.1477e-08, 4.0911e-08, 4.4094e-08, 4.3787e-08, 4.1962e-08, 4.1946e-08,\n",
       "            4.2091e-08, 4.0215e-08, 4.2472e-08, 4.4782e-08, 4.2489e-08, 4.1931e-08,\n",
       "            4.0887e-08, 4.1392e-08, 4.1025e-08, 4.3898e-08, 4.5446e-08, 4.0413e-08,\n",
       "            4.3375e-08, 4.5032e-08, 3.8580e-08, 4.4829e-08, 4.2388e-08, 4.3377e-08,\n",
       "            4.2374e-08, 4.3070e-08, 4.3013e-08, 4.1782e-08, 4.3917e-08, 4.1706e-08,\n",
       "            4.2902e-08, 4.2065e-08, 4.1916e-08, 4.5176e-08, 3.9165e-08, 4.1507e-08,\n",
       "            4.3418e-08, 4.2292e-08, 4.2680e-08, 4.0624e-08, 4.1034e-08, 4.2989e-08,\n",
       "            4.6494e-08, 3.9881e-08, 4.4872e-08, 4.5587e-08, 4.4533e-08, 4.0976e-08,\n",
       "            4.1505e-08, 4.4826e-08, 4.5018e-08, 3.9946e-08, 4.3935e-08, 4.0761e-08,\n",
       "            4.2279e-08, 4.0741e-08, 4.1404e-08, 3.9798e-08, 4.0709e-08, 4.1125e-08,\n",
       "            4.0526e-08, 4.1399e-08, 3.9798e-08, 4.3866e-08, 4.1727e-08, 4.2903e-08,\n",
       "            4.1615e-08, 4.1129e-08, 4.5932e-08, 4.1009e-08, 4.4868e-08, 4.5308e-08,\n",
       "            4.1908e-08, 4.1573e-08, 4.1867e-08, 4.2156e-08, 4.0273e-08, 3.9979e-08,\n",
       "            4.0669e-08, 4.4158e-08, 4.1993e-08, 4.1178e-08, 4.4540e-08, 4.2633e-08,\n",
       "            4.3629e-08, 4.3213e-08, 4.3117e-08, 4.2895e-08, 4.3131e-08, 4.1378e-08,\n",
       "            4.0534e-08, 4.5240e-08, 4.4929e-08, 4.2074e-08, 4.1366e-08, 4.2863e-08,\n",
       "            4.2698e-08, 4.1855e-08, 3.9574e-08, 3.9341e-08, 4.1735e-08, 4.0623e-08,\n",
       "            4.4939e-08, 4.0923e-08, 4.2113e-08, 4.1832e-08, 4.2981e-08, 4.1456e-08,\n",
       "            4.1838e-08, 4.1255e-08, 4.3387e-08, 4.0647e-08, 4.1637e-08, 4.3582e-08,\n",
       "            4.2490e-08, 4.5117e-08, 4.1017e-08, 3.8735e-08, 3.9526e-08, 4.0258e-08,\n",
       "            4.5678e-08, 4.0520e-08, 4.2104e-08, 4.4208e-08, 4.2901e-08, 4.1820e-08,\n",
       "            4.3172e-08, 3.8820e-08, 4.2132e-08, 3.9783e-08, 4.2726e-08, 4.1408e-08,\n",
       "            4.1527e-08, 4.3132e-08, 4.0368e-08, 4.1380e-08, 4.1708e-08, 4.1623e-08,\n",
       "            4.2036e-08, 3.9400e-08, 4.0434e-08, 4.1331e-08, 4.2266e-08, 4.6813e-08,\n",
       "            4.3744e-08, 4.4873e-08, 4.2331e-08, 4.2860e-08, 4.0119e-08, 4.1100e-08,\n",
       "            4.0774e-08, 4.0085e-08, 4.1690e-08, 4.0715e-08, 4.5845e-08, 4.0983e-08,\n",
       "            4.3133e-08, 3.9921e-08, 4.0401e-08, 4.3823e-08, 4.0258e-08, 3.9495e-08,\n",
       "            3.8669e-08, 4.1331e-08, 4.2006e-08, 4.7445e-08, 4.1669e-08, 4.0441e-08,\n",
       "            4.1654e-08, 4.0307e-08, 4.0765e-08, 4.2489e-08, 4.0480e-08, 4.0569e-08,\n",
       "            4.1873e-08, 4.0379e-08, 4.1503e-08, 4.1282e-08, 4.0067e-08, 4.2392e-08,\n",
       "            4.1665e-08, 4.2317e-08, 4.1666e-08, 4.2844e-08, 4.1388e-08, 4.1707e-08,\n",
       "            4.2303e-08, 4.3625e-08, 4.3242e-08, 4.3280e-08, 4.3023e-08, 4.0183e-08,\n",
       "            3.8822e-08, 4.2718e-08, 4.1192e-08, 4.1004e-08, 4.2319e-08, 4.0275e-08,\n",
       "            4.0346e-08, 4.2063e-08, 4.0642e-08, 4.2978e-08, 4.2335e-08, 4.1503e-08,\n",
       "            4.5324e-08, 4.1596e-08, 4.0472e-08, 4.2528e-08, 4.2882e-08, 4.1229e-08,\n",
       "            4.0677e-08, 4.3461e-08, 4.4953e-08, 4.2359e-08, 4.0519e-08, 4.2871e-08,\n",
       "            4.3157e-08, 4.1139e-08, 4.5642e-08, 4.1780e-08, 4.0051e-08, 4.2065e-08,\n",
       "            4.2762e-08, 4.3400e-08, 4.2458e-08, 4.6560e-08, 4.3151e-08, 4.0726e-08,\n",
       "            4.2263e-08, 4.1085e-08, 4.2418e-08, 4.2442e-08, 4.2910e-08, 3.9977e-08,\n",
       "            4.2000e-08, 4.2560e-08, 4.1060e-08, 4.1882e-08, 4.5107e-08, 4.5105e-08,\n",
       "            4.1108e-08, 4.1147e-08, 3.8972e-08, 4.3413e-08, 4.1655e-08, 4.1556e-08,\n",
       "            4.1939e-08, 4.2983e-08, 4.2680e-08, 4.1285e-08, 4.4747e-08, 4.5445e-08,\n",
       "            4.0287e-08, 4.1395e-08, 4.1041e-08, 4.2540e-08, 4.1398e-08, 4.3518e-08,\n",
       "            4.1748e-08, 4.4884e-08, 4.1754e-08, 4.8524e-08, 4.2898e-08, 4.3105e-08,\n",
       "            4.1624e-08, 4.0311e-08, 4.1112e-08, 3.9873e-08, 3.9206e-08, 4.2708e-08,\n",
       "            4.1241e-08, 3.9216e-08, 4.3018e-08, 4.3314e-08, 4.0635e-08, 4.2618e-08,\n",
       "            4.2461e-08, 4.1578e-08, 4.3986e-08, 4.0131e-08, 4.0279e-08, 4.0478e-08,\n",
       "            4.2138e-08, 4.4269e-08, 4.3122e-08, 4.2053e-08, 4.7007e-08, 4.2703e-08,\n",
       "            4.1501e-08, 4.0623e-08, 4.2706e-08, 4.1642e-08, 4.1402e-08, 4.1453e-08,\n",
       "            3.9196e-08, 4.2728e-08, 4.2111e-08, 4.1525e-08, 4.2955e-08, 4.2767e-08,\n",
       "            4.1957e-08, 4.2925e-08, 4.5087e-08, 4.1255e-08, 4.4749e-08, 4.1536e-08,\n",
       "            4.1152e-08, 4.1744e-08, 4.1069e-08, 4.1240e-08, 4.3220e-08, 4.0189e-08,\n",
       "            4.2958e-08, 4.0971e-08, 4.1447e-08, 3.9300e-08, 4.0713e-08, 4.3462e-08,\n",
       "            4.2923e-08, 4.4667e-08, 4.3707e-08, 3.9216e-08, 4.2088e-08, 4.2158e-08,\n",
       "            4.1646e-08, 4.0751e-08, 4.2177e-08, 4.1438e-08, 4.4222e-08, 4.2068e-08,\n",
       "            4.1912e-08, 4.2714e-08, 4.8853e-08, 4.1444e-08, 4.2998e-08, 4.1916e-08,\n",
       "            4.2355e-08, 4.1270e-08, 4.1040e-08, 3.9087e-08, 4.3014e-08, 4.1638e-08,\n",
       "            4.1093e-08, 4.0987e-08, 3.9803e-08, 4.0865e-08, 4.3720e-08, 4.2474e-08,\n",
       "            4.0585e-08, 4.1058e-08, 4.1617e-08, 3.8827e-08, 4.0864e-08, 4.1409e-08,\n",
       "            4.3418e-08, 3.9972e-08, 4.2164e-08, 4.1119e-08, 4.2850e-08, 4.1577e-08,\n",
       "            4.2314e-08, 4.2012e-08, 4.3029e-08, 4.1297e-08, 4.3202e-08, 4.0706e-08,\n",
       "            4.0462e-08, 4.3208e-08, 4.2839e-08, 4.1523e-08, 4.0327e-08, 4.4059e-08,\n",
       "            3.9492e-08, 3.9321e-08, 4.3358e-08, 4.2681e-08, 4.1798e-08, 4.5873e-08,\n",
       "            3.8495e-08, 4.2162e-08, 4.2539e-08, 4.4328e-08, 4.3194e-08, 3.8972e-08,\n",
       "            4.4751e-08, 4.1691e-08, 4.2886e-08, 4.2334e-08, 4.3104e-08, 4.2926e-08,\n",
       "            4.1188e-08, 4.1925e-08, 3.9840e-08, 4.1508e-08, 4.4488e-08, 4.1806e-08,\n",
       "            4.1687e-08, 4.0562e-08, 4.3077e-08, 4.2899e-08, 4.1173e-08, 4.0326e-08,\n",
       "            4.3157e-08, 4.3154e-08, 4.0710e-08, 4.3412e-08, 4.2106e-08, 4.2349e-08,\n",
       "            3.9543e-08, 4.5351e-08, 4.3993e-08, 4.1205e-08, 4.3484e-08, 4.1677e-08,\n",
       "            3.9730e-08, 4.3405e-08, 4.2668e-08, 4.2991e-08, 4.0437e-08, 4.4700e-08,\n",
       "            4.0681e-08, 4.2395e-08, 4.1257e-08, 4.1608e-08, 4.3241e-08, 4.2083e-08,\n",
       "            4.0397e-08, 4.5094e-08, 4.2230e-08, 4.0571e-08, 4.3035e-08, 4.3484e-08,\n",
       "            4.5594e-08, 4.2108e-08, 4.5045e-08, 4.1033e-08, 4.4143e-08, 4.3522e-08,\n",
       "            4.8071e-08, 4.0394e-08, 4.0846e-08, 4.4789e-08, 4.1345e-08, 4.4535e-08,\n",
       "            4.2968e-08, 4.5504e-08, 4.2345e-08, 4.2329e-08, 4.1639e-08, 4.1861e-08,\n",
       "            4.2267e-08, 4.2602e-08, 4.1592e-08, 3.9875e-08, 4.1614e-08, 3.9108e-08,\n",
       "            4.0791e-08, 4.3752e-08, 4.3055e-08, 3.9614e-08, 4.2212e-08, 4.2568e-08,\n",
       "            4.0473e-08, 4.2275e-08, 4.3353e-08, 4.1837e-08, 4.3751e-08, 4.1531e-08,\n",
       "            4.1356e-08, 4.2588e-08, 4.4475e-08, 4.5234e-08, 4.3193e-08, 4.0949e-08,\n",
       "            4.3193e-08, 3.9950e-08, 4.1716e-08, 4.2305e-08, 4.0224e-08, 4.1078e-08,\n",
       "            4.0238e-08, 4.0805e-08, 4.0665e-08, 4.1875e-08, 4.4873e-08, 4.3179e-08,\n",
       "            4.1707e-08, 4.5194e-08, 4.2548e-08, 4.1693e-08, 4.2618e-08, 3.9930e-08,\n",
       "            4.2828e-08, 4.2099e-08, 4.2583e-08, 4.2312e-08, 4.4620e-08, 4.4642e-08,\n",
       "            4.1942e-08, 4.2685e-08, 4.0223e-08, 4.3584e-08, 3.9166e-08, 4.2383e-08,\n",
       "            4.0664e-08, 4.4076e-08, 3.8996e-08, 4.0848e-08, 4.3406e-08, 4.1560e-08,\n",
       "            4.1788e-08, 4.4310e-08, 4.2277e-08, 4.1228e-08, 4.0683e-08, 4.3040e-08,\n",
       "            4.0964e-08, 4.0764e-08, 4.0529e-08, 4.0903e-08, 3.9669e-08, 4.1749e-08,\n",
       "            4.4198e-08, 4.1315e-08, 4.1805e-08, 4.0270e-08, 4.2112e-08, 4.3332e-08,\n",
       "            4.2511e-08, 4.3697e-08, 4.5609e-08, 3.9035e-08, 4.0646e-08, 4.3841e-08,\n",
       "            4.1195e-08, 4.1866e-08, 4.2392e-08, 4.1266e-08, 4.4162e-08, 4.1091e-08,\n",
       "            4.1282e-08, 4.2802e-08, 4.0441e-08, 4.0962e-08, 4.1507e-08, 4.1413e-08,\n",
       "            3.9081e-08, 4.1016e-08, 3.8832e-08, 4.3677e-08, 4.1150e-08, 4.1717e-08,\n",
       "            4.0628e-08, 4.3033e-08, 4.1663e-08, 4.0371e-08, 4.1246e-08, 4.1552e-08,\n",
       "            4.0724e-08, 4.1545e-08, 4.4611e-08, 4.3163e-08, 4.4018e-08, 4.1337e-08,\n",
       "            4.0704e-08, 4.1556e-08, 4.0921e-08, 4.7141e-08, 4.2265e-08, 4.1443e-08,\n",
       "            4.2450e-08, 3.9008e-08, 4.0274e-08, 4.0968e-08, 4.2133e-08, 4.4243e-08,\n",
       "            4.1906e-08, 4.1299e-08, 4.5868e-08, 4.1545e-08, 4.6099e-08, 4.1661e-08,\n",
       "            4.1457e-08, 4.0555e-08, 4.0868e-08, 3.9769e-08, 4.2599e-08, 4.1962e-08,\n",
       "            4.3039e-08, 4.2460e-08, 4.5670e-08, 4.3262e-08, 4.3161e-08, 4.1305e-08,\n",
       "            4.2710e-08, 4.2490e-08, 4.2648e-08, 4.2614e-08, 3.8943e-08, 4.3333e-08,\n",
       "            4.1138e-08, 4.0673e-08, 4.0463e-08, 4.2810e-08, 4.2795e-08, 4.3368e-08,\n",
       "            4.0149e-08, 4.2696e-08, 4.2793e-08, 4.0090e-08, 4.2409e-08, 4.2453e-08,\n",
       "            4.2336e-08, 4.1728e-08, 4.2882e-08, 4.1425e-08, 4.3851e-08, 3.9424e-08,\n",
       "            4.0691e-08, 4.3008e-08, 4.0059e-08, 4.3512e-08, 3.9727e-08, 4.3823e-08,\n",
       "            4.3129e-08, 4.1975e-08, 4.1603e-08, 4.2063e-08, 4.3122e-08, 4.1561e-08,\n",
       "            4.3428e-08, 4.3080e-08, 4.0569e-08, 4.1701e-08, 4.3867e-08, 4.0819e-08,\n",
       "            3.9864e-08, 4.3256e-08, 4.3794e-08, 4.1515e-08, 4.0296e-08, 4.1180e-08,\n",
       "            4.3028e-08, 4.3551e-08, 4.3918e-08, 4.2860e-08, 4.3487e-08, 4.0307e-08,\n",
       "            4.0715e-08, 4.1656e-08, 4.3191e-08, 4.0184e-08, 4.2539e-08, 4.2695e-08,\n",
       "            4.2809e-08, 4.1920e-08, 4.0859e-08, 3.9366e-08, 4.1770e-08, 4.0967e-08,\n",
       "            4.0998e-08, 4.4328e-08, 4.2143e-08, 4.4071e-08, 4.0818e-08, 4.0357e-08,\n",
       "            4.3363e-08, 4.0078e-08, 4.3556e-08, 4.3103e-08, 3.9620e-08, 4.1249e-08,\n",
       "            4.4127e-08, 4.3299e-08, 4.1479e-08, 4.0634e-08, 4.3034e-08, 4.3376e-08,\n",
       "            4.0190e-08, 4.1053e-08, 3.9437e-08, 4.2252e-08, 4.0966e-08, 4.2241e-08,\n",
       "            4.3754e-08, 4.4557e-08, 4.2506e-08, 4.5606e-08, 4.2521e-08, 4.2054e-08,\n",
       "            4.5181e-08, 4.2811e-08, 4.3051e-08, 4.8449e-08, 4.0567e-08, 4.1300e-08,\n",
       "            4.2556e-08, 4.2375e-08, 4.0947e-08, 4.1134e-08, 4.3373e-08, 4.1690e-08,\n",
       "            4.1412e-08, 3.9522e-08, 4.3069e-08, 4.0979e-08, 4.2181e-08, 4.4282e-08,\n",
       "            4.0248e-08, 3.8652e-08, 4.2862e-08, 4.1216e-08, 4.1260e-08, 4.3695e-08,\n",
       "            4.8244e-08, 4.1820e-08, 3.8780e-08, 3.8984e-08, 4.2518e-08, 4.1158e-08,\n",
       "            3.9715e-08, 4.6157e-08, 4.2445e-08, 4.0623e-08, 4.0727e-08, 4.2084e-08,\n",
       "            4.0087e-08, 4.4938e-08, 4.3492e-08, 4.2190e-08, 4.5065e-08, 4.2367e-08,\n",
       "            4.2177e-08, 4.0797e-08, 4.2008e-08, 4.4028e-08, 4.1552e-08, 4.4400e-08,\n",
       "            4.3415e-08, 4.3826e-08, 4.1080e-08, 4.2611e-08, 4.0766e-08, 3.9772e-08,\n",
       "            4.4608e-08, 4.3033e-08, 3.9931e-08, 4.1339e-08, 4.1014e-08, 4.1304e-08,\n",
       "            4.2359e-08, 4.1914e-08, 4.2035e-08, 4.4792e-08, 4.2964e-08, 4.2060e-08,\n",
       "            4.2586e-08, 4.3188e-08, 4.2842e-08, 4.1102e-08, 4.1596e-08, 4.4570e-08,\n",
       "            4.5937e-08, 4.0492e-08, 4.3994e-08, 4.1038e-08, 4.1056e-08, 4.2114e-08,\n",
       "            4.2165e-08, 4.0468e-08, 4.0650e-08, 3.9830e-08, 4.0466e-08, 4.2131e-08,\n",
       "            4.3146e-08, 4.5153e-08, 4.1463e-08, 3.9387e-08, 4.3587e-08, 3.9752e-08,\n",
       "            4.0212e-08, 4.1562e-08, 4.4092e-08, 4.2626e-08, 3.9299e-08, 3.9494e-08,\n",
       "            4.5766e-08, 4.1730e-08, 4.0531e-08, 4.5002e-08, 3.9960e-08, 4.2668e-08,\n",
       "            4.2818e-08, 4.2128e-08, 4.2633e-08, 4.3877e-08, 4.0888e-08, 4.4300e-08,\n",
       "            4.2770e-08, 3.9190e-08, 4.3752e-08, 4.0471e-08, 4.1763e-08, 4.2336e-08,\n",
       "            4.3530e-08, 4.1130e-08, 4.0288e-08, 4.5176e-08, 4.0728e-08, 4.2615e-08,\n",
       "            4.0665e-08, 4.1305e-08, 4.4083e-08, 4.6130e-08, 4.1443e-08, 4.2269e-08,\n",
       "            4.0742e-08, 4.1578e-08, 4.4563e-08, 4.1011e-08, 4.2560e-08, 3.9783e-08,\n",
       "            4.5184e-08, 4.1173e-08, 4.1124e-08, 4.1373e-08, 4.2021e-08, 4.2606e-08,\n",
       "            4.5885e-08, 4.4293e-08, 4.2173e-08, 4.2429e-08, 4.2422e-08, 4.3977e-08,\n",
       "            4.2633e-08, 4.0132e-08, 4.2743e-08, 4.3811e-08, 4.1532e-08, 4.2201e-08,\n",
       "            4.5186e-08, 4.4993e-08, 4.1823e-08, 4.3016e-08, 4.3928e-08, 4.2572e-08,\n",
       "            3.9375e-08, 3.9341e-08, 4.1749e-08, 4.4017e-08, 4.1089e-08, 4.5846e-08,\n",
       "            4.3410e-08, 4.3064e-08, 3.9329e-08, 4.1625e-08, 4.3024e-08, 4.1732e-08,\n",
       "            4.2622e-08, 4.1035e-08, 4.1117e-08, 4.3105e-08, 4.1390e-08, 4.3897e-08,\n",
       "            3.9563e-08, 4.2240e-08, 4.4308e-08, 3.9757e-08, 4.3538e-08, 4.0162e-08,\n",
       "            4.1823e-08, 4.3410e-08, 4.1906e-08, 4.2519e-08, 4.2618e-08, 4.0849e-08,\n",
       "            4.2605e-08, 4.1368e-08, 4.0720e-08, 4.1699e-08, 4.4238e-08, 4.0243e-08,\n",
       "            4.0592e-08, 4.1764e-08, 4.2746e-08, 4.8726e-08, 4.3037e-08, 4.0634e-08,\n",
       "            4.5857e-08, 4.3895e-08, 4.4111e-08, 4.0853e-08, 4.1104e-08, 4.2379e-08,\n",
       "            4.2232e-08, 4.3541e-08, 4.2487e-08, 4.1476e-08, 4.4175e-08, 4.2066e-08,\n",
       "            4.0906e-08, 4.5991e-08, 4.0922e-08, 4.3523e-08, 4.0455e-08, 4.0030e-08,\n",
       "            4.1639e-08, 4.0499e-08, 4.3356e-08, 4.3044e-08, 4.3552e-08, 4.5890e-08,\n",
       "            4.1605e-08, 4.1050e-08, 4.6459e-08, 4.3097e-08, 4.0849e-08, 4.2564e-08,\n",
       "            4.2464e-08, 4.0519e-08, 4.2203e-08, 4.0902e-08, 4.2168e-08, 4.3392e-08,\n",
       "            4.0341e-08, 4.2382e-08, 4.1462e-08, 4.5094e-08, 4.1620e-08, 4.1786e-08,\n",
       "            4.5248e-08, 3.9397e-08, 4.3268e-08, 4.3051e-08, 4.6131e-08, 4.4377e-08,\n",
       "            4.4509e-08, 4.2142e-08, 4.0299e-08, 4.0295e-08, 4.0935e-08, 4.2381e-08,\n",
       "            4.2982e-08, 4.2005e-08, 4.4484e-08, 4.4010e-08, 4.1987e-08, 4.1337e-08,\n",
       "            4.1522e-08, 4.2776e-08, 4.9852e-08, 4.1487e-08, 4.1396e-08, 4.5712e-08,\n",
       "            4.3610e-08, 4.4276e-08, 4.1317e-08, 4.2849e-08, 4.2666e-08, 4.4660e-08,\n",
       "            3.9718e-08, 4.2294e-08, 4.6241e-08, 4.2644e-08, 4.0859e-08, 4.0196e-08,\n",
       "            3.9404e-08, 4.3551e-08], device='cuda:0')}},\n",
       "  'param_groups': [{'lr': 0.0001,\n",
       "    'betas': (0.9, 0.999),\n",
       "    'eps': 1e-08,\n",
       "    'weight_decay': 0.01,\n",
       "    'amsgrad': True,\n",
       "    'foreach': None,\n",
       "    'maximize': False,\n",
       "    'capturable': False,\n",
       "    'differentiable': False,\n",
       "    'fused': None,\n",
       "    'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_state = torch.load(\"/home/boat/pattern/pattern_term_project_2024/boat_weight/deepkillme_2agents/deepkillme_white_it_48000.pt\")\n",
    "white_player.load_model_state(saved_state[\"models\"])\n",
    "saved_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game 0\n",
      "_b_b_b_b\n",
      "b_b_b_b_\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_w_w\n",
      "w_w_w_w_\n",
      "0 turn: black last_moved_piece: None\n",
      "7 legal moves [(4, 8), (5, 8), (5, 9), (6, 9), (6, 10), (7, 10), (7, 11)]\n",
      "black moved 7, 11\n",
      "whitePlayer Memory [{'black': {'men': set(), 'kings': {0}}, 'white': {'men': set(), 'kings': {2}}}, (2, 9), {'black': {'men': set(), 'kings': {0}}, 'white': {'men': set(), 'kings': {9}}}, 1.5]\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_b_._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_w_w\n",
      "w_w_w_w_\n",
      "1 turn: white last_moved_piece: None\n",
      "7 legal moves [(24, 21), (24, 20), (25, 22), (25, 21), (26, 23), (26, 22), (27, 23)]\n",
      "do use q\n",
      "white moved 26, 22\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 2, 3, 4, 5, 6, 11}, 'kings': set()}, 'white': {'men': {24, 25, 26, 27, 28, 29, 30, 31}, 'kings': set()}}, (26, 22), {'black': {'men': {0, 1, 2, 3, 4, 5, 6, 11}, 'kings': set()}, 'white': {'men': {22, 24, 25, 27, 28, 29, 30, 31}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_b_._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_w_w_._w\n",
      "w_w_w_w_\n",
      "2 turn: black last_moved_piece: None\n",
      "8 legal moves [(2, 7), (3, 7), (4, 8), (5, 8), (5, 9), (6, 9), (6, 10), (11, 15)]\n",
      "black moved 6, 10\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 2, 3, 4, 5, 6, 11}, 'kings': set()}, 'white': {'men': {24, 25, 26, 27, 28, 29, 30, 31}, 'kings': set()}}, (26, 22), {'black': {'men': {0, 1, 2, 3, 4, 5, 6, 11}, 'kings': set()}, 'white': {'men': {22, 24, 25, 27, 28, 29, 30, 31}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._._\n",
      "_._._b_b\n",
      "._._._._\n",
      "_._._._.\n",
      "._._w_._\n",
      "_w_w_._w\n",
      "w_w_w_w_\n",
      "3 turn: white last_moved_piece: None\n",
      "8 legal moves [(22, 18), (22, 17), (24, 21), (24, 20), (25, 21), (27, 23), (30, 26), (31, 26)]\n",
      "do use q\n",
      "white moved 22, 18\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 2, 3, 4, 5, 10, 11}, 'kings': set()}, 'white': {'men': {22, 24, 25, 27, 28, 29, 30, 31}, 'kings': set()}}, (22, 18), {'black': {'men': {0, 1, 2, 3, 4, 5, 10, 11}, 'kings': set()}, 'white': {'men': {18, 24, 25, 27, 28, 29, 30, 31}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._._\n",
      "_._._b_b\n",
      "._._._._\n",
      "_._._w_.\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_w_w_\n",
      "4 turn: black last_moved_piece: None\n",
      "10 legal moves [(1, 6), (2, 6), (2, 7), (3, 7), (4, 8), (5, 8), (5, 9), (10, 14), (10, 15), (11, 15)]\n",
      "black moved 10, 14\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 2, 3, 4, 5, 10, 11}, 'kings': set()}, 'white': {'men': {22, 24, 25, 27, 28, 29, 30, 31}, 'kings': set()}}, (22, 18), {'black': {'men': {0, 1, 2, 3, 4, 5, 10, 11}, 'kings': set()}, 'white': {'men': {18, 24, 25, 27, 28, 29, 30, 31}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._._\n",
      "_._._._b\n",
      "._._b_._\n",
      "_._._w_.\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_w_w_\n",
      "5 turn: white last_moved_piece: None\n",
      "1 legal moves [(18, 9)]\n",
      "do use q\n",
      "white moved 18, 9\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 2, 3, 4, 5, 11, 14}, 'kings': set()}, 'white': {'men': {18, 24, 25, 27, 28, 29, 30, 31}, 'kings': set()}}, (18, 9), {'black': {'men': {0, 1, 2, 3, 4, 5, 11}, 'kings': set()}, 'white': {'men': {9, 24, 25, 27, 28, 29, 30, 31}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_b_b\n",
      "b_b_._._\n",
      "_._w_._b\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_w_w_\n",
      "6 turn: black last_moved_piece: None\n",
      "1 legal moves [(5, 14)]\n",
      "black moved 5, 14\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 2, 3, 4, 5, 11, 14}, 'kings': set()}, 'white': {'men': {18, 24, 25, 27, 28, 29, 30, 31}, 'kings': set()}}, (18, 9), {'black': {'men': {0, 1, 2, 3, 4, 5, 11}, 'kings': set()}, 'white': {'men': {9, 24, 25, 27, 28, 29, 30, 31}, 'kings': set()}}, 0.5]\n",
      "\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._b_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_._w\n",
      "w_w_w_w_\n",
      "7 turn: white last_moved_piece: None\n",
      "7 legal moves [(24, 21), (24, 20), (25, 22), (25, 21), (27, 23), (30, 26), (31, 26)]\n",
      "do use q\n",
      "white moved 25, 21\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 2, 3, 4, 11, 14}, 'kings': set()}, 'white': {'men': {24, 25, 27, 28, 29, 30, 31}, 'kings': set()}}, (25, 21), {'black': {'men': {0, 1, 2, 3, 4, 11, 14}, 'kings': set()}, 'white': {'men': {21, 24, 27, 28, 29, 30, 31}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._b_._\n",
      "_._._._.\n",
      "._w_._._\n",
      "_w_._._w\n",
      "w_w_w_w_\n",
      "8 turn: black last_moved_piece: None\n",
      "10 legal moves [(0, 5), (1, 5), (1, 6), (2, 6), (2, 7), (3, 7), (4, 8), (11, 15), (14, 17), (14, 18)]\n",
      "black moved 14, 17\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 2, 3, 4, 11, 14}, 'kings': set()}, 'white': {'men': {24, 25, 27, 28, 29, 30, 31}, 'kings': set()}}, (25, 21), {'black': {'men': {0, 1, 2, 3, 4, 11, 14}, 'kings': set()}, 'white': {'men': {21, 24, 27, 28, 29, 30, 31}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._b_._.\n",
      "._w_._._\n",
      "_w_._._w\n",
      "w_w_w_w_\n",
      "9 turn: white last_moved_piece: None\n",
      "1 legal moves [(21, 14)]\n",
      "do use q\n",
      "white moved 21, 14\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 2, 3, 4, 11, 17}, 'kings': set()}, 'white': {'men': {21, 24, 27, 28, 29, 30, 31}, 'kings': set()}}, (21, 14), {'black': {'men': {0, 1, 2, 3, 4, 11}, 'kings': set()}, 'white': {'men': {14, 24, 27, 28, 29, 30, 31}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_b_b\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._w_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_._._w\n",
      "w_w_w_w_\n",
      "10 turn: black last_moved_piece: None\n",
      "8 legal moves [(0, 5), (1, 5), (1, 6), (2, 6), (2, 7), (3, 7), (4, 8), (11, 15)]\n",
      "black moved 2, 7\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 2, 3, 4, 11, 17}, 'kings': set()}, 'white': {'men': {21, 24, 27, 28, 29, 30, 31}, 'kings': set()}}, (21, 14), {'black': {'men': {0, 1, 2, 3, 4, 11}, 'kings': set()}, 'white': {'men': {14, 24, 27, 28, 29, 30, 31}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._b\n",
      "b_._._b_\n",
      "_._._._b\n",
      "._._w_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_._._w\n",
      "w_w_w_w_\n",
      "11 turn: white last_moved_piece: None\n",
      "9 legal moves [(14, 10), (14, 9), (24, 21), (24, 20), (27, 23), (29, 25), (30, 26), (30, 25), (31, 26)]\n",
      "do use q\n",
      "white moved 31, 26\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 3, 4, 7, 11}, 'kings': set()}, 'white': {'men': {14, 24, 27, 28, 29, 30, 31}, 'kings': set()}}, (31, 26), {'black': {'men': {0, 1, 3, 4, 7, 11}, 'kings': set()}, 'white': {'men': {14, 24, 26, 27, 28, 29, 30}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._b\n",
      "b_._._b_\n",
      "_._._._b\n",
      "._._w_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_._w_w\n",
      "w_w_w_._\n",
      "12 turn: black last_moved_piece: None\n",
      "6 legal moves [(0, 5), (1, 5), (1, 6), (4, 8), (7, 10), (11, 15)]\n",
      "black moved 7, 10\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 3, 4, 7, 11}, 'kings': set()}, 'white': {'men': {14, 24, 27, 28, 29, 30, 31}, 'kings': set()}}, (31, 26), {'black': {'men': {0, 1, 3, 4, 7, 11}, 'kings': set()}, 'white': {'men': {14, 24, 26, 27, 28, 29, 30}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._b\n",
      "b_._._._\n",
      "_._._b_b\n",
      "._._w_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_._w_w\n",
      "w_w_w_._\n",
      "13 turn: white last_moved_piece: None\n",
      "1 legal moves [(14, 7)]\n",
      "do use q\n",
      "white moved 14, 7\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 3, 4, 10, 11}, 'kings': set()}, 'white': {'men': {14, 24, 26, 27, 28, 29, 30}, 'kings': set()}}, (14, 7), {'black': {'men': {0, 1, 3, 4, 11}, 'kings': set()}, 'white': {'men': {7, 24, 26, 27, 28, 29, 30}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._b\n",
      "b_._._w_\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_._w_w\n",
      "w_w_w_._\n",
      "14 turn: black last_moved_piece: None\n",
      "1 legal moves [(3, 10)]\n",
      "black moved 3, 10\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 3, 4, 10, 11}, 'kings': set()}, 'white': {'men': {14, 24, 26, 27, 28, 29, 30}, 'kings': set()}}, (14, 7), {'black': {'men': {0, 1, 3, 4, 11}, 'kings': set()}, 'white': {'men': {7, 24, 26, 27, 28, 29, 30}, 'kings': set()}}, 0.5]\n",
      "\n",
      "_b_b_._.\n",
      "b_._._._\n",
      "_._._b_b\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_._w_w\n",
      "w_w_w_._\n",
      "15 turn: white last_moved_piece: None\n",
      "7 legal moves [(24, 21), (24, 20), (26, 23), (26, 22), (27, 23), (29, 25), (30, 25)]\n",
      "do use q\n",
      "white moved 29, 25\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 4, 10, 11}, 'kings': set()}, 'white': {'men': {24, 26, 27, 28, 29, 30}, 'kings': set()}}, (29, 25), {'black': {'men': {0, 1, 4, 10, 11}, 'kings': set()}, 'white': {'men': {24, 25, 26, 27, 28, 30}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._.\n",
      "b_._._._\n",
      "_._._b_b\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_w_w\n",
      "w_._w_._\n",
      "16 turn: black last_moved_piece: None\n",
      "7 legal moves [(0, 5), (1, 5), (1, 6), (4, 8), (10, 14), (10, 15), (11, 15)]\n",
      "black moved 10, 15\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 4, 10, 11}, 'kings': set()}, 'white': {'men': {24, 26, 27, 28, 29, 30}, 'kings': set()}}, (29, 25), {'black': {'men': {0, 1, 4, 10, 11}, 'kings': set()}, 'white': {'men': {24, 25, 26, 27, 28, 30}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._._b_\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_w_w_w\n",
      "w_._w_._\n",
      "17 turn: white last_moved_piece: None\n",
      "7 legal moves [(24, 21), (24, 20), (25, 22), (25, 21), (26, 23), (26, 22), (27, 23)]\n",
      "do use q\n",
      "white moved 26, 23\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 4, 11, 15}, 'kings': set()}, 'white': {'men': {24, 25, 26, 27, 28, 30}, 'kings': set()}}, (26, 23), {'black': {'men': {0, 1, 4, 11, 15}, 'kings': set()}, 'white': {'men': {23, 24, 25, 27, 28, 30}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._._b_\n",
      "_._._._.\n",
      "._._._w_\n",
      "_w_w_._w\n",
      "w_._w_._\n",
      "18 turn: black last_moved_piece: None\n",
      "6 legal moves [(0, 5), (1, 5), (1, 6), (4, 8), (15, 18), (15, 19)]\n",
      "black moved 15, 19\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 4, 11, 15}, 'kings': set()}, 'white': {'men': {24, 25, 26, 27, 28, 30}, 'kings': set()}}, (26, 23), {'black': {'men': {0, 1, 4, 11, 15}, 'kings': set()}, 'white': {'men': {23, 24, 25, 27, 28, 30}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._._b\n",
      "._._._w_\n",
      "_w_w_._w\n",
      "w_._w_._\n",
      "19 turn: white last_moved_piece: None\n",
      "6 legal moves [(23, 18), (24, 21), (24, 20), (25, 22), (25, 21), (30, 26)]\n",
      "do use q\n",
      "white moved 24, 20\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 4, 11, 19}, 'kings': set()}, 'white': {'men': {23, 24, 25, 27, 28, 30}, 'kings': set()}}, (24, 20), {'black': {'men': {0, 1, 4, 11, 19}, 'kings': set()}, 'white': {'men': {20, 23, 25, 27, 28, 30}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._._b\n",
      "w_._._w_\n",
      "_._w_._w\n",
      "w_._w_._\n",
      "20 turn: black last_moved_piece: None\n",
      "1 legal moves [(19, 26)]\n",
      "black moved 19, 26\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 4, 11, 19}, 'kings': set()}, 'white': {'men': {23, 24, 25, 27, 28, 30}, 'kings': set()}}, (24, 20), {'black': {'men': {0, 1, 4, 11, 19}, 'kings': set()}, 'white': {'men': {20, 23, 25, 27, 28, 30}, 'kings': set()}}, 0.5]\n",
      "\n",
      "_b_b_._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._._._\n",
      "_._w_b_w\n",
      "w_._w_._\n",
      "21 turn: white last_moved_piece: None\n",
      "1 legal moves [(30, 23)]\n",
      "do use q\n",
      "white moved 30, 23\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 4, 11, 26}, 'kings': set()}, 'white': {'men': {20, 25, 27, 28, 30}, 'kings': set()}}, (30, 23), {'black': {'men': {0, 1, 11, 4}, 'kings': set()}, 'white': {'men': {20, 23, 25, 27, 28}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._._w_\n",
      "_._w_._w\n",
      "w_._._._\n",
      "22 turn: black last_moved_piece: None\n",
      "5 legal moves [(0, 5), (1, 5), (1, 6), (4, 8), (11, 15)]\n",
      "black moved 4, 8\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 4, 11, 26}, 'kings': set()}, 'white': {'men': {20, 25, 27, 28, 30}, 'kings': set()}}, (30, 23), {'black': {'men': {0, 1, 11, 4}, 'kings': set()}, 'white': {'men': {20, 23, 25, 27, 28}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_b_._._b\n",
      "._._._._\n",
      "_._._._.\n",
      "w_._._w_\n",
      "_._w_._w\n",
      "w_._._._\n",
      "23 turn: white last_moved_piece: None\n",
      "6 legal moves [(20, 16), (25, 22), (25, 21), (28, 24), (23, 19), (23, 18)]\n",
      "do use q\n",
      "white moved 23, 18\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 11, 8}, 'kings': set()}, 'white': {'men': {20, 23, 25, 27, 28}, 'kings': set()}}, (23, 18), {'black': {'men': {0, 1, 11, 8}, 'kings': set()}, 'white': {'men': {18, 20, 25, 27, 28}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_b_._._b\n",
      "._._._._\n",
      "_._._w_.\n",
      "w_._._._\n",
      "_._w_._w\n",
      "w_._._._\n",
      "24 turn: black last_moved_piece: None\n",
      "7 legal moves [(0, 4), (0, 5), (1, 5), (1, 6), (8, 12), (8, 13), (11, 15)]\n",
      "black moved 8, 12\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 11, 8}, 'kings': set()}, 'white': {'men': {20, 23, 25, 27, 28}, 'kings': set()}}, (23, 18), {'black': {'men': {0, 1, 11, 8}, 'kings': set()}, 'white': {'men': {18, 20, 25, 27, 28}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._._._b\n",
      "b_._._._\n",
      "_._._w_.\n",
      "w_._._._\n",
      "_._w_._w\n",
      "w_._._._\n",
      "25 turn: white last_moved_piece: None\n",
      "7 legal moves [(18, 15), (18, 14), (20, 16), (25, 22), (25, 21), (27, 23), (28, 24)]\n",
      "do use q\n",
      "white moved 20, 16\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 11, 12}, 'kings': set()}, 'white': {'men': {18, 20, 25, 27, 28}, 'kings': set()}}, (20, 16), {'black': {'men': {0, 1, 11, 12}, 'kings': set()}, 'white': {'men': {16, 18, 25, 27, 28}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._._._b\n",
      "b_._._._\n",
      "_w_._w_.\n",
      "._._._._\n",
      "_._w_._w\n",
      "w_._._._\n",
      "26 turn: black last_moved_piece: None\n",
      "1 legal moves [(12, 21)]\n",
      "black moved 12, 21\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 11, 12}, 'kings': set()}, 'white': {'men': {18, 20, 25, 27, 28}, 'kings': set()}}, (20, 16), {'black': {'men': {0, 1, 11, 21}, 'kings': set()}, 'white': {'men': {25, 18, 27, 28}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._w_.\n",
      "._b_._._\n",
      "_._w_._w\n",
      "w_._._._\n",
      "27 turn: black last_moved_piece: 21\n",
      "1 legal moves [(21, 30)]\n",
      "black moved 21, 30\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 11, 12}, 'kings': set()}, 'white': {'men': {18, 20, 25, 27, 28}, 'kings': set()}}, (20, 16), {'black': {'men': {0, 1, 11, 21}, 'kings': set()}, 'white': {'men': {25, 18, 27, 28}, 'kings': set()}}, 0.5]\n",
      "\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._w_.\n",
      "._._._._\n",
      "_._._._w\n",
      "w_._B_._\n",
      "28 turn: white last_moved_piece: None\n",
      "4 legal moves [(18, 15), (18, 14), (27, 23), (28, 24)]\n",
      "do use q\n",
      "white moved 18, 14\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 11}, 'kings': {30}}, 'white': {'men': {18, 27, 28}, 'kings': set()}}, (18, 14), {'black': {'men': {0, 1, 11}, 'kings': {30}}, 'white': {'men': {27, 28, 14}, 'kings': set()}}, 0]\n",
      "\n",
      "_b_b_._.\n",
      "._._._._\n",
      "_._._._b\n",
      "._._w_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._w\n",
      "w_._B_._\n",
      "29 turn: black last_moved_piece: None\n",
      "12 legal moves [(0, 4), (0, 5), (1, 5), (1, 6), (11, 15), (30, 26), (30, 23), (30, 19), (30, 25), (30, 21), (30, 16), (30, 12)]\n",
      "black moved 0, 4\n",
      "whitePlayer Memory [{'black': {'men': {0, 1, 11}, 'kings': {30}}, 'white': {'men': {18, 27, 28}, 'kings': set()}}, (18, 14), {'black': {'men': {0, 1, 11}, 'kings': {30}}, 'white': {'men': {27, 28, 14}, 'kings': set()}}, 0]\n",
      "\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._w_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._w\n",
      "w_._B_._\n",
      "30 turn: white last_moved_piece: None\n",
      "4 legal moves [(14, 10), (14, 9), (27, 23), (28, 24)]\n",
      "do use q\n",
      "white moved 28, 24\n",
      "whitePlayer Memory [{'black': {'men': {1, 11, 4}, 'kings': {30}}, 'white': {'men': {27, 28, 14}, 'kings': set()}}, (28, 24), {'black': {'men': {1, 11, 4}, 'kings': {30}}, 'white': {'men': {24, 27, 14}, 'kings': set()}}, 0]\n",
      "\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._w_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_._._w\n",
      "._._B_._\n",
      "31 turn: black last_moved_piece: None\n",
      "11 legal moves [(1, 5), (1, 6), (4, 8), (11, 15), (30, 26), (30, 23), (30, 19), (30, 25), (30, 21), (30, 16), (30, 12)]\n",
      "black moved 30, 12\n",
      "whitePlayer Memory [{'black': {'men': {1, 11, 4}, 'kings': {30}}, 'white': {'men': {27, 28, 14}, 'kings': set()}}, (28, 24), {'black': {'men': {1, 11, 4}, 'kings': {30}}, 'white': {'men': {24, 27, 14}, 'kings': set()}}, 0]\n",
      "\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "B_._w_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_w_._._w\n",
      "._._._._\n",
      "32 turn: white last_moved_piece: None\n",
      "5 legal moves [(14, 10), (14, 9), (27, 23), (24, 21), (24, 20)]\n",
      "do use q\n",
      "white moved 24, 21\n",
      "whitePlayer Memory [{'black': {'men': {1, 11, 4}, 'kings': {12}}, 'white': {'men': {24, 27, 14}, 'kings': set()}}, (24, 21), {'black': {'men': {1, 11, 4}, 'kings': {12}}, 'white': {'men': {27, 21, 14}, 'kings': set()}}, 0]\n",
      "\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "B_._w_._\n",
      "_._._._.\n",
      "._w_._._\n",
      "_._._._w\n",
      "._._._._\n",
      "33 turn: black last_moved_piece: None\n",
      "1 legal moves [(12, 25)]\n",
      "black moved 12, 25\n",
      "whitePlayer Memory [{'black': {'men': {1, 11, 4}, 'kings': {12}}, 'white': {'men': {24, 27, 14}, 'kings': set()}}, (24, 21), {'black': {'men': {1, 11, 4}, 'kings': {12}}, 'white': {'men': {27, 21, 14}, 'kings': set()}}, 0.5]\n",
      "\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_._._._b\n",
      "._._w_._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._B_._w\n",
      "._._._._\n",
      "34 turn: white last_moved_piece: None\n",
      "3 legal moves [(14, 10), (14, 9), (27, 23)]\n",
      "do use q\n",
      "white moved 14, 9\n",
      "whitePlayer Memory [{'black': {'men': {1, 11, 4}, 'kings': {25}}, 'white': {'men': {27, 14}, 'kings': set()}}, (14, 9), {'black': {'men': {1, 11, 4}, 'kings': {25}}, 'white': {'men': {9, 27}, 'kings': set()}}, 0]\n",
      "\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_._w_._b\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._B_._w\n",
      "._._._._\n",
      "35 turn: black last_moved_piece: None\n",
      "12 legal moves [(1, 5), (1, 6), (4, 8), (11, 15), (25, 29), (25, 30), (25, 22), (25, 18), (25, 15), (25, 21), (25, 16), (25, 12)]\n",
      "black moved 25, 12\n",
      "whitePlayer Memory [{'black': {'men': {1, 11, 4}, 'kings': {25}}, 'white': {'men': {27, 14}, 'kings': set()}}, (14, 9), {'black': {'men': {1, 11, 4}, 'kings': {25}}, 'white': {'men': {9, 27}, 'kings': set()}}, 0]\n",
      "\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_._w_._b\n",
      "B_._._._\n",
      "_._._._.\n",
      "._._._._\n",
      "_._._._w\n",
      "._._._._\n",
      "36 turn: white last_moved_piece: None\n",
      "3 legal moves [(9, 6), (9, 5), (27, 23)]\n",
      "do use q\n",
      "white moved 27, 23\n",
      "whitePlayer Memory [{'black': {'men': {1, 11, 4}, 'kings': {12}}, 'white': {'men': {9, 27}, 'kings': set()}}, (27, 23), {'black': {'men': {1, 11, 4}, 'kings': {12}}, 'white': {'men': {9, 23}, 'kings': set()}}, 0]\n",
      "\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_._w_._b\n",
      "B_._._._\n",
      "_._._._.\n",
      "._._._w_\n",
      "_._._._.\n",
      "._._._._\n",
      "37 turn: black last_moved_piece: None\n",
      "10 legal moves [(1, 5), (1, 6), (4, 8), (11, 15), (12, 16), (12, 21), (12, 25), (12, 30), (12, 8), (12, 5)]\n",
      "black moved 12, 30\n",
      "whitePlayer Memory [{'black': {'men': {1, 11, 4}, 'kings': {12}}, 'white': {'men': {9, 27}, 'kings': set()}}, (27, 23), {'black': {'men': {1, 11, 4}, 'kings': {12}}, 'white': {'men': {9, 23}, 'kings': set()}}, 0]\n",
      "\n",
      "_._b_._.\n",
      "b_._._._\n",
      "_._w_._b\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._w_\n",
      "_._._._.\n",
      "._._B_._\n",
      "38 turn: white last_moved_piece: None\n",
      "4 legal moves [(9, 6), (9, 5), (23, 19), (23, 18)]\n",
      "do use q\n",
      "white moved 9, 6\n",
      "whitePlayer Memory [{'black': {'men': {1, 11, 4}, 'kings': {30}}, 'white': {'men': {9, 23}, 'kings': set()}}, (9, 6), {'black': {'men': {1, 11, 4}, 'kings': {30}}, 'white': {'men': {6, 23}, 'kings': set()}}, 0]\n",
      "\n",
      "_._b_._.\n",
      "b_._w_._\n",
      "_._._._b\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._w_\n",
      "_._._._.\n",
      "._._B_._\n",
      "39 turn: black last_moved_piece: None\n",
      "2 legal moves [(1, 10), (30, 19)]\n",
      "black moved 1, 10\n",
      "whitePlayer Memory [{'black': {'men': {1, 11, 4}, 'kings': {30}}, 'white': {'men': {9, 23}, 'kings': set()}}, (9, 6), {'black': {'men': {1, 11, 4}, 'kings': {30}}, 'white': {'men': {6, 23}, 'kings': set()}}, 0.5]\n",
      "\n",
      "_._._._.\n",
      "b_._._._\n",
      "_._._b_b\n",
      "._._._._\n",
      "_._._._.\n",
      "._._._w_\n",
      "_._._._.\n",
      "._._B_._\n",
      "40 turn: white last_moved_piece: None\n",
      "2 legal moves [(23, 19), (23, 18)]\n",
      "do use q\n",
      "white moved 23, 19\n",
      "whitePlayer Memory [{'black': {'men': {10, 11, 4}, 'kings': {30}}, 'white': {'men': {23}, 'kings': set()}}, (23, 19), {'black': {'men': {10, 11, 4}, 'kings': {30}}, 'white': {'men': {19}, 'kings': set()}}, 0]\n",
      "\n",
      "_._._._.\n",
      "b_._._._\n",
      "_._._b_b\n",
      "._._._._\n",
      "_._._._w\n",
      "._._._._\n",
      "_._._._.\n",
      "._._B_._\n",
      "41 turn: black last_moved_piece: None\n",
      "10 legal moves [(4, 8), (10, 14), (10, 15), (11, 15), (30, 26), (30, 23), (30, 25), (30, 21), (30, 16), (30, 12)]\n",
      "black moved 11, 15\n",
      "whitePlayer Memory [{'black': {'men': {10, 11, 4}, 'kings': {30}}, 'white': {'men': {23}, 'kings': set()}}, (23, 19), {'black': {'men': {10, 11, 4}, 'kings': {30}}, 'white': {'men': {19}, 'kings': set()}}, 1]\n",
      "\n",
      "black player wins\n",
      "total legal moves 230 avg branching factor 5.476190476190476\n",
      "black player evaluated 131 positions in 0.03s (avg 4354.90 positions/s) effective branching factor 5.95\n",
      "black player pruned dict_items([])\n",
      "\n",
      "round : 1/1 result lose, explore_rate 0.1\n",
      "black win 1 draw 0 loss 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# A few matches against a random player\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "max_game_len = 100\n",
    "n_matches = 1\n",
    "n_wins, n_draws, n_losses = 0, 0, 0\n",
    "is_show_game = True\n",
    "explore_rate=0.1\n",
    "rand = 1203\n",
    "\n",
    "def rollout_order_gen_random(x):\n",
    "    random.shuffle(x)\n",
    "    return x\n",
    "    \n",
    "for i in tqdm(range(n_matches)):\n",
    "    if is_show_game:\n",
    "        print('game', i)\n",
    "    ch = Checkers()\n",
    "    black_player = MinimaxPlayer(\n",
    "        'black',\n",
    "        # value_func=partial(first_order_adv, 'black', 200, 100, 20, 0),\n",
    "        value_func=partial(first_order_adv, 'black', 86.0315, 54.568, 87.21072, 25.85066),        \n",
    "        # The provided legal moves might be ordered differently\n",
    "        rollout_order_gen=rollout_order_gen_random,\n",
    "        search_depth=1,\n",
    "        seed=i+rand)\n",
    "\n",
    "    #modify this function to put our RL model as white\n",
    "    winner = play_a_game(ch, black_player.next_move, white_player.next_move, max_game_len,is_show_detail = is_show_game, white_player=white_player)\n",
    "\n",
    "    if is_show_game:\n",
    "        print('black player evaluated %i positions in %.2fs (avg %.2f positions/s) effective branching factor %.2f' % (black_player.n_evaluated_positions, black_player.evaluation_dt, black_player.n_evaluated_positions / black_player.evaluation_dt, (black_player.n_evaluated_positions / black_player.ply) ** (1 / black_player.search_depth)))\n",
    "        print('black player pruned', black_player.prunes.items())\n",
    "        print()\n",
    "        \n",
    "    result:RESULT_TYPE\n",
    "    if winner == 'black':\n",
    "        n_wins += 1\n",
    "        result = RESULT_TYPE.LOSE\n",
    "    elif winner is None:\n",
    "        n_draws += 1\n",
    "        result = RESULT_TYPE.DRAW\n",
    "    else:\n",
    "        n_losses += 1\n",
    "        result = RESULT_TYPE.WIN\n",
    "\n",
    "    print(f\"round : {i+1}/{n_matches} result {result.value}, explore_rate {explore_rate}\")\n",
    "\n",
    "print('black win', n_wins, 'draw', n_draws, 'loss', n_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'black': {'men': {0, 1, 2, 3, 4, 11, 14, 21}, 'kings': set()},\n",
       "   'white': {'men': {22, 23, 24, 26, 28, 29, 31}, 'kings': set()}},\n",
       "  (24, 17),\n",
       "  None,\n",
       "  -150]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in white_player.memory if not i[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
